<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://davidzhao1015.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://davidzhao1015.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-07T04:48:54+00:00</updated><id>https://davidzhao1015.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Worked Example for Reproducible Power Analysis in Python</title><link href="https://davidzhao1015.github.io/blog/2026/power-analysis-notebook/" rel="alternate" type="text/html" title="Worked Example for Reproducible Power Analysis in Python"/><published>2026-01-05T00:00:00+00:00</published><updated>2026-01-05T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2026/power-analysis-notebook</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2026/power-analysis-notebook/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/demo-power-analysis-evident.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="power-analysis;"/><category term="Jupyter;"/><category term="Python;"/><category term="Evident"/><summary type="html"><![CDATA[A step-by-step guide to conducting power analysis using Jupyter notebook]]></summary></entry><entry><title type="html">Power Analysis as a Workflow for Microbiome Data</title><link href="https://davidzhao1015.github.io/blog/2026/power-analysis/" rel="alternate" type="text/html" title="Power Analysis as a Workflow for Microbiome Data"/><published>2026-01-03T00:00:00+00:00</published><updated>2026-01-03T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2026/power-analysis</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2026/power-analysis/"><![CDATA[<h2 id="background--motivation-why-reproducible-power-analysis-matters">Background &amp; Motivation: Why reproducible power analysis matters</h2> <p>In microbiome research, whether in human cohorts or animal trials, determining the appropriate sample size is essential. Studies that are too small lack the statistical power to detect meaningful biological effects, while studies that are unnecessarily large risk wasting resources and raising ethical concerns.</p> <p>Despite this, power analysis is frequently misapplied in microbiome studies.</p> <p>As both a data analyst and an academic journal reviewer, I often encounter microbiome studies that are underpowered. Importantly, these studies are rarely weakened because authors lack statistical knowledge. More often, they fall short because the assumptions underlying the power analysis are unclear, inconsistently applied, or difficult to reproduce. Common reviewer concerns include effect sizes chosen without justification, limited exploration of alternative assumptions, and power calculations that cannot be revisited when study design choices are questioned.</p> <p>These challenges are amplified by the high variability and sparsity characteristic of microbiome data, whether derived from marker-gene or shotgun sequencing. For example, a trial designed to detect differences in gut microbial diversity between treatment and control groups may fail, not because the analysis is incorrect, but because the sample size is insufficient to capture the underlying biological signal given the chosen assumptions.</p> <p>In my experience, studies that navigate peer review successfully tend to treat power analysis not as a one-off calculation, but as a structured and revisitable analytical workflow. Assumptions are made explicit, alternatives can be explored, and results can be updated transparently when reviewers ask “what if?”. This perspective motivates the framework outlined in this article and sets the stage for its executable implementation.</p> <h2 id="scope--purpose-what-this-workflow-covers">Scope &amp; Purpose: What this workflow covers</h2> <p>This article introduces the core biostatistical concepts behind power analysis and outlines a high-level analytical workflow.</p> <p>While this conceptual foundation is often sufficient to understand <em>what</em> decisions are required, applying power analysis in real studies typically involves iterating across assumptions, adapting to specific experimental designs, and selecting appropriate software tools.</p> <p>These practical considerations are beyond the scope of this introductory post, but they follow directly from the workflow described here and motivate its executable implementation.</p> <h2 id="core-concepts-informing-the-workflow">Core concepts informing the workflow</h2> <p>Before diving into sample size calculation and power analysis, it is important to revisit a few fundamental biostatistical concepts. As a microbiologist turned data analyst, my goal here is to explain these ideas as simply and accurately as possible.</p> <ul> <li>Type I Error (α error): When comparing an experimental group to a control group, we test whether an observed difference is real or due to chance. If our test statistic exceeds a critical threshold (based on the chosen significance level, α), we reject the null hypothesis. However, there is always a probability - commonly set at 5% - that we reject the null hypothesis even when it is actually true. This false-positive conclusion is called a Type I error.</li> <li>Type II Error (β error): A Type II error occurs when we fail to reject the null hypothesis even though a true difference exists between groups. In other words, it represents a missed detection. The probability of making this error is denoted by β.</li> <li>Power (1 – β): Statistical power is the probability of correctly detecting a true difference if it exists. A study with higher power is more likely to identify meaningful effects, while a low-powered study risks overlooking them.</li> <li>Significance Level (α): The significance level is the threshold probability of observing extreme results under the null hypothesis. It defines the cut-off for deciding whether an observed difference is statistically significant.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PowerAnalysis_FIg1_PowerOfTest.png" sizes="95vw"/> <img src="/assets/img/PowerAnalysis_FIg1_PowerOfTest.png" class="img-fluid rounded z-depth-1" width="700" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1. Illustration of statistical power (source: Wikipedia)</figcaption> </figure> </div> <ul> <li>Effect Size: Effect size quantifies the magnitude of difference in a biological or clinical outcome—for example, changes in alpha-diversity, relative abundance of taxa, or other microbiome-related metrics. The appropriate measure of effect size depends on the type of outcome data (continuous, categorical, or count-based). Researchers often obtain plausible effect size estimates from pilot studies or prior literature. Importantly, the smaller the effect size one aims to detect, the larger the sample size and power required to ensure reliable results.</li> <li>Population variance: The population variance quantifies spread of the outcome variable, which is an essential component in calculating critical values and then power. While it is often unknown, one can infer population variance from sample variance reported in pilot studies or literature.</li> <li>Power curve: A graphical tool that shows how statistical power changes with sample size. The upward-sloping curve illustrates that larger sample sizes generally lead to higher statistical power.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PowerAnalysis_Fig2_Sample_Sizes_Effect_on_Power.png" sizes="95vw"/> <img src="/assets/img/PowerAnalysis_Fig2_Sample_Sizes_Effect_on_Power.png" class="img-fluid rounded z-depth-1" width="700" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2. Illustration of power curve (source: Wikipedia)</figcaption> </figure> </div> <p>Together, these concepts form the foundation for sample size determination and power analysis, helping researchers balance the risks of false positives, false negatives, and practical feasibility when designing experiments.</p> <h2 id="workflow-for-reproducible-power-analysis">Workflow for reproducible power analysis</h2> <h3 id="step-1-define-the-research-question">Step 1. Define the research question</h3> <p>Begin by clearly articulating the objective of your study. The following guiding questions can help refine the research question:</p> <ul> <li>What outcome do you aim to measure or detect?</li> <li>Does the analysis involve comparing outcomes between two or more groups, or modeling associations using regression?</li> </ul> <p>Example: Detecting a difference in Shannon diversity between cases and controls.</p> <h3 id="step-2-choose-the-primary-outcome">Step 2. Choose the primary outcome</h3> <p>Identify the type of primary outcome, as this will influence both the statistical test and the power calculation approach.</p> <p>Common outcome types include:</p> <ul> <li>Continuous</li> <li>Proportion</li> <li>Binary</li> <li>Multivariate distance-based measures</li> </ul> <h3 id="step-3-specify-the-statistical-test">Step 3. Specify the statistical test</h3> <p>Select the statistical test that aligns with the research question and outcome types.</p> <ul> <li>The chosen test determines the statistical distribution used in the power calculation.</li> <li>Common examples include t-test, ANOVA, regression models, PERMANOVA, χ² tests, and others.</li> </ul> <h3 id="step-4-specify-effect-size">Step 4. Specify effect size</h3> <p>Define the <em>minimum scientifically meaningful difference</em> you want to detect.</p> <p>Effect size can be informed by:</p> <ul> <li>Pilot data</li> <li>Previous literature</li> <li>Clinician or domain-expert input</li> <li>Biological relevance (not purely statistical considerations)</li> </ul> <p>Common effect size metrics include:</p> <ul> <li>Cohen’s d (difference in means)</li> <li>R² in PERMANOVA</li> <li>Odds ratio</li> <li>hazard ratio</li> <li>Fold change in abundance</li> </ul> <h3 id="step-5-specify-design-parameters">Step 5. Specify design parameters</h3> <p>Set the key parameters that govern hypothesis testing:</p> <ul> <li>α (significance level), typically 0.05</li> <li>Power, commonly 0.80 or 0.90</li> <li>One-sided versus two-sided tests</li> <li>Multiple testing correction, if applicable</li> </ul> <h3 id="step-6-perform-the-power-or-sample-size-calculation">Step 6. Perform the power or sample size calculation</h3> <p>Estimate the required sample size using:</p> <ul> <li>The specified effect size</li> <li>The selected statistical test</li> <li>Chosen α and power levels</li> <li>The corresponding test-specific distribution (e.g., <em>t</em>, <em>Z</em>, χ², <em>F</em>, noncentral <em>F</em>)</li> </ul> <p>This can be done analytically using closed-form formulas or via simulation when assumptions are complex.</p> <h3 id="step-7-interpret-the-results">Step 7. Interpret the results</h3> <p>Finally, interpret the power analysis results in the context of study feasibility, scientific relevance, and potential limitations, and assess whether adjustments to the design are necessary.</p> <h2 id="iteration-pain-point--why-a-template-helps">Iteration pain point &amp; Why a template helps</h2> <p>In practice, researchers rarely run power analysis just once. Effect sizes are uncertain, reviewers request alternative assumptions, and multiple outcomes must be evaluated. Repeating these steps manually, especially across different tests or designs, quickly becomes time-consuming and error-prone.</p> <h2 id="summary-and-practical-next-steps-with-notebook-template">Summary and practical next steps (with Notebook template)</h2> <p>The workflow above defines <em>what</em> decisions are required for power analysis in microbiome studies.</p> <p>In practice, these steps are rarely executed once. Researchers often:</p> <ul> <li>iterate across multiple outcomes,</li> <li>explore a range of plausible effect sizes,</li> <li>respond to reviewer requests for alternative assumptions,</li> <li>and document results in a reproducible way.</li> </ul> <p>To support this process, I implemented the same workflow in a Python <a href="https://davidzhao1015.github.io/blog/2026/power-analysis-notebook/">notebook template</a>, where each step maps directly to code cells and visual outputs.</p>]]></content><author><name></name></author><category term="power-analysis,"/><category term="sample-size,"/><category term="microbiome-data"/><summary type="html"><![CDATA[A practical workflow for study design, reviewer-ready assumptions, and reproducible power analysis]]></summary></entry><entry><title type="html">How to Reconstruct a Kaplan–Meier Curve from a Published Life Table: A Step-by-Step Beginner’s Guide Using Python</title><link href="https://davidzhao1015.github.io/blog/2025/lifetable-km-curve/" rel="alternate" type="text/html" title="How to Reconstruct a Kaplan–Meier Curve from a Published Life Table: A Step-by-Step Beginner’s Guide Using Python"/><published>2025-12-01T00:00:00+00:00</published><updated>2025-12-01T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/lifetable-km-curve</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/lifetable-km-curve/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In rare-disease research, meta-analyses and systematic reviews often rely on published literature rather than raw individual patient data (IPD). For many analysts new to the field, it’s not immediately obvious that <strong>survival curves can still be reconstructed, even when IPD is unavailable</strong>.</p> <p>This tutorial demonstrates <strong>how to rebuild a Kaplan-Meier (KM) survival curve from a published life table</strong>, step by step. Once the survival function is reconstructed, you can recover meaningful time-to-event insights such as the <strong>median time to symptom onset</strong>, which is especially valuable for rare diseases where sample sizes are small and reporting formats vary.</p> <h3 id="what-you-will-learn">What you will learn</h3> <p>By the end of this tutorial, you will understand:</p> <ul> <li>How to convert a published life table into an estimated Kaplan–Meier survival curve</li> <li>How to approximate the number of events in each interval</li> <li>How to generate simulated (“pseudo”) individual-level data</li> <li>How to fit a KM model in Python using the <strong>lifelines</strong> package</li> <li>How to visualize the survival function and extract median event times</li> </ul> <h3 id="prerequisites">Prerequisites</h3> <ul> <li>Basic familiarity with Python (e.g., using pandas)</li> <li>No prior survival-analysis knowledge required <br/><br/></li> </ul> <h2 id="what-is-a-life-table-and-why-does-it-matter">What Is a Life Table, and Why Does It Matter?</h2> <p>Before jumping into code, it’s worth understanding what a life table represents.</p> <p>A <strong>life table</strong> reports the number of individuals who remain event-free at specific follow-up times. Even though it does not contain individual observations, it still retains sufficient structure to approximate a survival pattern.</p> <p>Reconstructing a KM curve from a life table is particularly useful when:</p> <ul> <li>Only aggregate results are published (common in <strong>retrospective cohorts</strong>)</li> <li>Raw KM curves are missing or provided only as images</li> <li>Conducting <strong>meta-analyses for rare diseases</strong>, where IPD is rarely available</li> <li>You need a quantitative estimate of survival metrics (median, percentiles, etc.)</li> </ul> <p>With a bit of algebra and simulation, we can recover a reasonable approximation of the original survival function.<br/><br/></p> <h2 id="workflow-overview">Workflow Overview</h2> <p>To help beginners visualize the full process, here is the conceptual workflow for reconstructing a Kaplan–Meier curve from a life table:</p> <ol> <li> <p>Import Python libraries: Load packages for data manipulation (pandas), modeling (lifelines), and visualization (matplotlib).</p> </li> <li> <p>Convert the life-table counts into a DataFrame: Enter (or load) the event-free counts at each follow-up time.</p> </li> <li> <p>Calculate the number of events per interval: Events are approximated using the drop in survival counts between time points.</p> </li> <li> <p>Expand the data into “pseudo” individual-level records: Each row becomes a simulated patient, allowing us to fit a survival model.</p> </li> <li> <p>Fit a Kaplan–Meier model using lifelines.KaplanMeierFitter: Use the simulated data to estimate the survival function.</p> </li> <li> <p>Visualize the KM curve: Plot the reconstructed survival curve to inspect its shape and behavior.</p> </li> <li> <p>Extract key survival metrics: Compute median time to event, percentiles, and other summary statistics.</p> </li> </ol> <p>This workflow provides a practical way to derive time-to-event insights when access to raw patient-level data is limited.<br/><br/></p> <h2 id="implementation">Implementation</h2> <p>Next, I will walk you through the Python code step by step, briefly highlighting the key outputs and how to interpret them.</p> <p>Steps 3 and 4 are the most important for simulating the Kaplan–Meier function and curve, and are worth particular attention during review.<br/><br/></p> <p><strong>Step 1: Import Python libraries</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">lifelines</span> <span class="c1"># Survival analysis library
</span><span class="kn">from</span> <span class="n">lifelines</span> <span class="kn">import</span> <span class="n">KaplanMeierFitter</span> <span class="c1"># Fit Kaplan-Meier model
</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div> <p><br/></p> <p><strong>Step 2. Convert the life-table counts into a DataFrame</strong></p> <p>The life table was extracted from a research paper describing the onset of interstitial lung disease (ILD) in a cohort. ILD is a symptom in which lung tissue becomes damaged and cannot fully expand or fill with air.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create lifetable DataFrame from given data
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">year</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">symptoms_free</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">745</span><span class="p">,</span> <span class="mi">592</span><span class="p">,</span> <span class="mi">382</span><span class="p">,</span> <span class="mi">262</span><span class="p">,</span> <span class="mi">171</span><span class="p">,</span> <span class="mi">97</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th>year</th> <th>symptoms_free</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>745</td> </tr> <tr> <td>1</td> <td>592</td> </tr> <tr> <td>2</td> <td>382</td> </tr> <tr> <td>3</td> <td>262</td> </tr> <tr> <td>4</td> <td>171</td> </tr> <tr> <td>5</td> <td>97</td> </tr> </tbody> </table> <p>The life table reports the number of patients who remained free of the symptom of interest over a follow-up period of up to five years.</p> <p>It provides the number of patients surviving without ILD from year 1 through year 5, as well as the number at risk at year 0, corresponding to the index date of the retrospective cohort.<br/><br/></p> <p><strong>Step 3: Calculate the number of events per interval</strong></p> <p>Next, we need to back-calculate the number of patients who developed the symptom during each time interval.</p> <p>We assume that no patients were censored and that each individual has a binary status, either with or without the symptom. Under this assumption, the number of events in a given year can be calculated as the difference between the number of symptom-free patients in the previous year and the number of symptom-free patients in the current year.</p> <p>For example, the number of events in year 1 equals the number of symptom-free patients at year 0 minus the number of symptom-free patients at year 1.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute events per interval (from symptoms_free counts)
</span><span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">events</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">symptoms_free</span><span class="sh">"</span><span class="p">].</span><span class="nf">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">symptoms_free</span><span class="sh">"</span><span class="p">]</span>
<span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">events</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># no event at time 0
</span><span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th>year</th> <th>symptoms_free</th> <th>events</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>745</td> <td>0</td> </tr> <tr> <td>1</td> <td>592</td> <td>153</td> </tr> <tr> <td>2</td> <td>382</td> <td>210</td> </tr> <tr> <td>3</td> <td>262</td> <td>120</td> </tr> <tr> <td>4</td> <td>171</td> <td>91</td> </tr> <tr> <td>5</td> <td>97</td> <td>74</td> </tr> </tbody> </table> <p><strong>4. Expand the data into “pseudo” individual-level records</strong></p> <p>Next, we recreate an individual-level dataset that includes each patient’s observed time and symptom-free status, with one row per individual. In this dataset, a value of 1 indicates that the patient developed the symptom, while 0 indicates that the patient remained symptom-free.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create expanded individual-level dataset for Kaplan-Meier fitting
</span><span class="n">records</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
    <span class="n">year</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">year</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">events</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">events</span><span class="sh">"</span><span class="p">])</span>
    <span class="c1"># event cases
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">events</span><span class="p">):</span>
        <span class="n">records</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">:</span> <span class="n">year</span><span class="p">,</span> <span class="sh">"</span><span class="s">event</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="sh">"</span><span class="s">symptoms_free</span><span class="sh">"</span><span class="p">])):</span>
    <span class="n">records</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="sh">"</span><span class="s">event</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>

<span class="n">df_long</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">records</span><span class="p">)</span>
</code></pre></div></div> <p>We then verify that the reconstructed dataset contains the correct number of patients at risk (n = 745).<br/><br/></p> <p><strong>Step 5: Fit a Kaplan–Meier model using lifelines.KaplanMeierFitter</strong></p> <p>We will fit a Kaplan–Meier function using the reconstructed dataset, applying the KaplanMeierFitter function from the lifelines module.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit Kaplan-Meier model
</span><span class="n">km</span> <span class="o">=</span> <span class="nc">KaplanMeierFitter</span><span class="p">()</span>
<span class="n">km</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">durations</span><span class="o">=</span><span class="n">df_long</span><span class="p">[</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">],</span> <span class="n">event_observed</span><span class="o">=</span><span class="n">df_long</span><span class="p">[</span><span class="sh">"</span><span class="s">event</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div> <p><br/></p> <p><strong>Step 6: Visualize the KM curve: Plot the reconstructed survival curve to inspect its shape and behavior</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot survival function
</span><span class="n">km</span><span class="p">.</span><span class="nf">plot_survival_function</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Kaplan-Meier Symptom Free Curve</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time (years)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Symptom Free Probability</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lifetable_km.png" sizes="95vw"/> <img src="/assets/img/lifetable_km.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure.</figcaption> </figure> </div> <p>The survival curve illustrates the Kaplan–Meier function from the index year through the end of the follow-up period in the retrospective cohort study.</p> <p>Each downward step within a time interval reflects the assumption that patients in that interval have a uniform probability of remaining symptom-free.<br/></p> <p><strong>Step 7: Extract key survival metrics</strong></p> <p>We are able to obtain the median time to symptom onset directly from the Kaplan–Meier function. This is useful for understanding the overall pattern of symptom development in the cohort.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Print median survival time
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Median Symptom Free Time:</span><span class="sh">"</span><span class="p">,</span> <span class="n">km</span><span class="p">.</span><span class="n">median_survival_time_</span><span class="p">)</span>
</code></pre></div></div> <p><br/><br/></p> <h2 id="method-limitations">Method Limitations</h2> <p>While reconstructing a Kaplan–Meier curve from a life table is useful, the approach has several inherent constraints:</p> <ul> <li>Assumes no censoring: Life tables do not distinguish between events and loss to follow-up. This method treats all declines as events, which may inflate event rates if censoring occurred.</li> <li>Assumes uniform timing of events: Events are distributed evenly across each interval, even though real-world data may show clustering that cannot be recovered without individual-level data.</li> <li>Not true individual-level data: The expanded dataset matches counts but does not reflect actual patient trajectories. Results should be interpreted as approximations.</li> <li>Limited by table granularity: Annual life-table data can only produce annual-level survival patterns. Finer intervals (e.g., monthly) would improve accuracy but are rarely available in published studies.<br/><br/></li> </ul> <h2 id="final-takeaways">Final Takeaways</h2> <p>Reconstructing a Kaplan–Meier curve from a life table is far simpler than it initially appears.</p> <p>With only a few rows of published counts and basic Python code, you can approximate a full survival curve and extract meaningful time-to-event insights.</p> <p>Here are the key lessons from this guide:</p> <ul> <li>Life tables contain enough information to rebuild an approximate survival function.</li> <li>By computing interval events and expanding them into pseudo–individual-level records, we can fit a KM model without IPD.</li> <li>Tools like <code class="language-plaintext highlighter-rouge">lifelines</code> make it easy to visualize survival curves and extract median survival time or other summary statistics.</li> <li>This workflow is especially valuable in systematic reviews, rare disease epidemiology, and health economic modeling, where KM curves are needed but individual-level data are rarely provided.</li> </ul> <p>Overall, this method gives researchers a pragmatic way to extract more value from published evidence, and enables a deeper understanding of survival patterns even when the original dataset is inaccessible.<br/><br/></p> <h2 id="download-the-full-notebook-and-try-it-yourself">Download the Full Notebook and Try It Yourself</h2> <p>If you’d like to explore the complete code, visualizations, and step-by-step logic, you can download the full notebook from my <a href="https://github.com/davidzhao1015/fit-km-model-from-lifetable/blob/300ea9d688650fb227d70e60e74094f34e8bfb9c/fit-km-model-from-lifetable.ipynb">GitHub repository</a>.</p> <p>The notebook includes:</p> <ul> <li>all Python code used in this tutorial</li> <li>expanded comments for beginners</li> <li>the reconstructed KM curve</li> </ul> <p>Feel free to adapt the notebook for your own meta-analysis, rare disease research, or health economic modeling projects.</p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="survival-analysis,"/><category term="evidence-synthesis,"/><category term="python,"/><category term="kaplan-meier,"/><category term="life-table"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">From Practice to Framework: Reusable PSA Tools for Cost-Effectiveness and Epidemiology Modelling</title><link href="https://davidzhao1015.github.io/blog/2025/psa-tools/" rel="alternate" type="text/html" title="From Practice to Framework: Reusable PSA Tools for Cost-Effectiveness and Epidemiology Modelling"/><published>2025-11-27T00:00:00+00:00</published><updated>2025-11-27T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/psa-tools</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/psa-tools/"><![CDATA[<h2 id="background">Background</h2> <p>This article is intended to share practical learning and hands-on experience with probabilistic sensitivity analysis (PSA), drawn from real-world health-economic work in rare-disease therapy.</p> <p>It provides a walkthrough of a reusable PSA architecture, illustrated with use cases from RGX-202, and introduces a probability-distribution inventory with a quick-selection guide for choosing appropriate distributions in PSA. Although the examples come from one working project, the framework is broadly applicable to future cost-effectiveness and epidemiology modelling projects that require sensitivity analysis.</p> <p>The article focuses on the applied aspects of PSA, how to use statistical probability distributions, Excel’s built-in formulas, and backend VBA to implement simulations. It intentionally does not cover the theoretical foundations of statistical distributions or general Excel functionality, which are beyond its intended scope.<br/><br/></p> <h2 id="conceptual-workflow-for-implementing-uncertainty-simulation-in-psa">Conceptual workflow for implementing uncertainty simulation in PSA</h2> <h3 id="problem-and-solution">Problem and Solution</h3> <p>The primary goal of PSA is to <strong>quantify how uncertainty in model inputs affects the outputs</strong> of a cost-effectiveness (CE) model. PSA uses statistical simulation, typically Monte Carlo simulation, to generate thousands of model runs, each drawing inputs from predefined, plausible uncertainty ranges.</p> <p>From my learning and real-world project experience, two steps are often the most challenging when implementing PSA in Excel:</p> <ol> <li><strong>building a robust PSA architecture</strong> (in-cell formulas and VBA), and</li> <li><strong>selecting appropriate probability distributions</strong> for each input parameter.</li> </ol> <p>To address these challenges, I’ve developed a set of <strong>reusable PSA tools</strong> designed to make these steps easier and more consistent across projects.<br/><br/></p> <h3 id="conceptual-workflow">Conceptual Workflow</h3> <p>The high-level workflow for PSA includes:</p> <ol> <li>Select an appropriate probability distribution</li> <li>Retrieve deterministic inputs and standard deviations from references</li> <li>Parameterize the selected distribution</li> <li>Generate random samples</li> <li>Run the simulation</li> </ol> <p>The following sections walk through the architecture and each step in more detail.<br/><br/></p> <h2 id="introduction-to-two-reusable-tools">Introduction to two reusable tools</h2> <h3 id="first-tool-psa-architecture-in-excel">First Tool: PSA Architecture in Excel</h3> <p>The screenshot below illustrates the main PSA architecture in Excel, with four framed sections corresponding to the first four steps of the PSA workflow.</p> <p>I’ve also uploaded the Excel workbook containing this PSA setup for anyone interested in reviewing or applying it. Please refer to the <em>“Generate_uncertainty”</em> sheet for the full implementation.</p> <p>In brief, the <strong>visible</strong> portion of the PSA architecture includes four components:</p> <ol> <li>Selection of probability-distribution types</li> <li>Entry of deterministic inputs and standard deviations</li> <li>Parameterization of the chosen distribution</li> <li>Setup for generating random samples</li> </ol> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig1_psa_architecture_snapshot.png" sizes="95vw"/> <img src="/assets/img/fig1_psa_architecture_snapshot.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1. PSA Architecture Snapshot</figcaption> </figure> </div> <p>The hidden portion contains reusable in-cell formulas that calculate distribution parameters and generate uncertainty-adjusted random inputs. These formulas support consistent and transparent PSA implementation across projects.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig2_psa_architecture_snapshot_formula.png" sizes="95vw"/> <img src="/assets/img/fig2_psa_architecture_snapshot_formula.png" class="img-fluid rounded z-depth-1" width="1000" height="800" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2. Example of embedded in-cell formulas within the PSA architecture</figcaption> </figure> </div> <h3 id="the-second-tool-probability-distribution-inventory">The Second Tool: Probability-Distribution Inventory</h3> <p>Choosing an appropriate probability distribution for each model input is the <strong>first and most critical step</strong> in PSA. This selection requires both an understanding of statistical distributions and domain knowledge in health economics or epidemiology.</p> <p>The inventory provides the key information needed to make informed decisions, including:</p> <ul> <li>the <strong>support</strong> (range) of each distribution,</li> <li><strong>parameterization</strong> details,</li> <li><strong>example use cases</strong>, and</li> <li>links to <strong>external references</strong> for further reading.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig3_prob_dist_inventory.png" sizes="95vw"/> <img src="/assets/img/fig3_prob_dist_inventory.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3. Screenshot of the inventory of probability distributions commonly used in HE and Epidemiology</figcaption> </figure> </div> <p>The rightmost column also includes <strong>practical tips</strong> to support distribution selection in real-world modelling.</p> <p>A later section of this article will explain how to use this inventory more effectively.<br/><br/></p> <h2 id="end-to-end-walkthrough-of-the-excel-based-psa-architecture">End-to-end walkthrough of the Excel-based PSA architecture</h2> <p>As illustrated in Figure 1, this section walks through the four key PSA steps implemented in Excel, followed by an overview of the simulation-validation process and the dashboard used to visually inspect the results.<br/><br/></p> <h3 id="steps-corresponding-to-the-four-framed-sections">Steps (corresponding to the four framed sections)</h3> <ol> <li><strong>Select an appropriate probability distribution</strong> <ul> <li>Choose a suitable distribution for the parameter of interest. Refer to the <em>Inventory</em> sheet for guidance.</li> <li>This choice is critical because it determines the required parameterization.</li> <li>For demonstration, the sheet highlights three commonly used distributions: <strong>Beta, Gamma, and Lognormal</strong>.</li> </ul> </li> <li><strong>Enter deterministic inputs, standard deviation, and optional epsilon</strong> <ul> <li>Input the deterministic value (e.g., the mean) and its standard deviation, typically sourced from literature such as ICER (2019).</li> <li>An optional <strong>epsilon</strong> (e.g., 0.0001) can be used when the standard deviation is reported as zero; it is not needed when a non-zero SD is available.</li> </ul> </li> <li><strong>Parameterize the selected distribution</strong> <ul> <li>The model automatically calculates the required distribution parameters (e.g., Alpha, Beta, Mu, Sigma) using built-in formulas.</li> <li>Optional upper and lower limits may be entered when reported in the literature.</li> <li>Parameterization uses the previously entered mean, standard deviation, and, if applicable, epsilon.</li> </ul> </li> <li><strong>Generate random samples</strong></li> </ol> <p>Column J contains formulas that generate random samples based on the chosen distribution and its parameterization.<br/><br/></p> <h3 id="running-the-simulation">Running the simulation</h3> <p>To test the setup:</p> <ul> <li>Go to the <strong>Validate_simulation</strong> sheet</li> <li>Click <strong>Run simulation</strong></li> <li>Clear previous simulation results if needed</li> </ul> <p>The buttons are linked to established backend VBA.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig4_validation_simulation_psa.png" sizes="95vw"/> <img src="/assets/img/fig4_validation_simulation_psa.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4. Validation and simulation sheet screenshot</figcaption> </figure> </div> <p>Briefly, the Live row displays probability outputs for the current iteration (referencing the previous sheet), while the rows below it store results from all 1,000 iterations. Two user-friendly buttons on the right allow users to run or clear the backend VBA simulations.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig5_dashboard_psa.png" sizes="95vw"/> <img src="/assets/img/fig5_dashboard_psa.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 5. Partial screenshot of the simulation dashboard</figcaption> </figure> </div> <p>The Dashboard supports visual inspection of the simulation outputs. The histogram displays the distribution of randomly sampled values for the selected probability distribution, while the framed summary table on the right compares the mean and standard deviation of the sampled data against the deterministic inputs.<br/><br/></p> <h3 id="how-to-use-the-probability-distribution-inventory">How to use the probability-distribution inventory</h3> <p>As shown in Figure 3, users can begin with <strong>column “Tips for distribution selection”</strong> to quickly narrow down candidate probability distributions that align with the characteristics of the HE or Epi parameter of interest. From there, users can review essential properties of each candidate distribution, such as whether its support matches the actual range of the model input, and whether relevant use cases exist. The <strong>parameterization section</strong> and <strong>linked Wikipedia references</strong> provide additional guidance for implementing the distribution statistically.<br/><br/></p> <h2 id="conclusion">Conclusion</h2> <p>The two tools introduced here are designed to support users in implementing PSA in Excel through a reusable architecture, clear documentation, VBA-powered simulations, and intuitive visual inspection tools.</p> <p>The PSA architecture can be integrated into nearly any CE modelling project with only minor adjustments. The VBA scripts are also adaptable to other projects with minimal modifications.</p> <p>The probability-distribution inventory, along with its practical selection guide, helps users choose appropriate distributions for each model input with confidence.<br/><br/></p> <h2 id="links-to-the-excel-file-on-github">Links to the Excel file on Github</h2> <p>If you’re interested in exploring or applying these tools, you can download the macro-enabled Excel workbook, <strong>“PSA_Implementation_Tools_2025.11.24_DZ.xlsm,”</strong> from my GitHub repository (<a href="https://github.com/davidzhao1015/psa-tools-excel/blob/main/PSA_Implementation_Tools_2025.11.24_DZ.xlsm">here</a>).</p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="sensitivity-analysis,"/><category term="epi,"/><category term="health-economy,"/><category term="excel,"/><category term="vba"/><summary type="html"><![CDATA[Background]]></summary></entry><entry><title type="html">Parametric Survival Fit &amp;amp; PSA for Age-at-Onset in Autoimmune Disease: Generalized Gamma Workflow</title><link href="https://davidzhao1015.github.io/blog/2025/sensitivity-analysis-survival-model/" rel="alternate" type="text/html" title="Parametric Survival Fit &amp;amp; PSA for Age-at-Onset in Autoimmune Disease: Generalized Gamma Workflow"/><published>2025-11-15T00:00:00+00:00</published><updated>2025-11-15T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/sensitivity-analysis-survival-model</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/sensitivity-analysis-survival-model/"><![CDATA[<p>In rare disease epidemiology, researchers often face a familiar challenge: most published studies do not provide individual-level patient data. Instead, they report only high-level summary statistics such as quartiles, means, and total sample size. Yet for health economic modeling, burden of disease estimation, or natural history modeling, we still need to understand the full age-at-onset distribution, including its shape, skew, and uncertainty.</p> <p>This article presents a practical workflow for reconstructing that distribution using parametric survival models, with a focus on the generalized gamma distribution. We will also show how to incorporate parameter uncertainty using probabilistic sensitivity analysis (PSA) and how to compute key outputs such as diagnosis probabilities within specific age bands.</p> <p>This workflow is adapted from a full analysis <a href="sen-analysis-Gamma-MVN-Cholesky.ipynb">notebook</a>, and is intended to be immediately useful for epidemiologists, biostatisticians, and health economists working with sparse or aggregate-only datasets.<br/><br/></p> <h2 id="1-why-parametric-survival-modeling-matters">1. Why Parametric Survival Modeling Matters</h2> <p>When individual-level data are unavailable, modeling decisions become difficult:</p> <ul> <li>How do we estimate onset age distributions when we only know quartiles?</li> <li>How can downstream models quantify uncertainty?</li> <li>Can we still estimate age-specific probabilities, like onset before 12 or after 18?</li> </ul> <p>The answer is <em>yes</em>. Well-chosen parametric models—fit using a quantile-matching method—allow us to approximate the underlying distribution closely enough for epidemiological and health economic purposes.</p> <p>The approach also supports probabilistic uncertainty methods widely used in cost-effectiveness modeling.<br/><br/></p> <h2 id="2-summary-data-and-python-setup">2. Summary Data and Python Setup</h2> <h3 id="21-summary-statistics-used">2.1 Summary Statistics Used</h3> <p>The analysis uses the following literature-reported values.</p> <table> <thead> <tr> <th>Statistic</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Min</td> <td>19</td> </tr> <tr> <td>Q1</td> <td>61</td> </tr> <tr> <td>Median</td> <td>66</td> </tr> <tr> <td>Q3</td> <td>72</td> </tr> <tr> <td>Max</td> <td>88</td> </tr> <tr> <td>Mean</td> <td>67</td> </tr> <tr> <td>Sample Size</td> <td>111</td> </tr> </tbody> </table> <p><br/>These values provide a robust representation of the distribution’s central tendency and spread.<br/><br/></p> <h3 id="22-software-environment">2.2 Software Environment</h3> <p>The full workflow uses:</p> <ul> <li><strong>NumPy</strong> for numerical operations</li> <li><strong>pandas</strong> for data structuring</li> <li><strong>SciPy</strong> for probability distributions</li> <li><strong>statsmodels</strong> for numerical Hessian estimation</li> <li><strong>matplotlib</strong> for visualization</li> </ul> <p>All version details are recorded to support reproducibility.<br/><br/></p> <h2 id="3-fitting-parametric-models-from-summary-data">3. Fitting Parametric Models from Summary Data</h2> <h3 id="31-why-use-parametric-distributions">3.1 Why Use Parametric Distributions?</h3> <p>Parametric survival distributions help us:</p> <ul> <li>Interpolate and extrapolate beyond observed quartiles</li> <li>Obtain smooth CDFs and PDFs</li> <li>Compute probabilities within arbitrary age bands</li> <li>Implement Monte Carlo–based uncertainty quantification</li> </ul> <p>This is particularly valuable in rare disease research, where datasets are often small.<br/><br/></p> <h3 id="32-quantile-matching-reconstructing-the-distribution">3.2 Quantile Matching: Reconstructing the Distribution</h3> <p>Because raw data are missing, we instead match:</p> \[(Q1, Median, Q3)_\text{model} \approx (Q1, Median, Q3)_\text{observed}\] <p>We define an objective function that penalizes deviations between model-predicted and observed quartiles, and we optimize the model’s parameters to minimize this squared error.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q1</span><span class="p">,</span> <span class="n">median</span><span class="p">,</span> <span class="n">q3</span> <span class="o">=</span> <span class="mi">61</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">72</span>
<span class="n">empirical_q</span> <span class="o">=</span> <span class="p">[</span><span class="n">q1</span><span class="p">,</span> <span class="n">median</span><span class="p">,</span> <span class="n">q3</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">gengamma_objective</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">scale</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="nf">gengamma</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">theo_q</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">ppf</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">theo_q</span><span class="p">)</span><span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">empirical_q</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span>
</code></pre></div></div> <p>This allows us to recover a plausible distribution consistent with the literature.<br/><br/></p> <h3 id="33-why-generalized-gamma">3.3 Why Generalized Gamma?</h3> <p>While lognormal and Weibull are widely used, the <strong>generalized gamma</strong> provides:</p> <ul> <li>high flexibility in skewness</li> <li>adjustable tail behavior</li> <li>ability to mimic many other survival distributions</li> </ul> <p>After fitting the model via quantile matching, we obtain parameter estimates for shape (a), power (c), and scale (s). These values define the final onset-age distribution.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">initial_guess_gengamma</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">]</span>
<span class="n">bounds_gengamma</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">0.01</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="bp">None</span><span class="p">)]</span>

<span class="n">result_gengamma</span> <span class="o">=</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">gengamma_objective</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">initial_guess_gengamma</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds_gengamma</span><span class="p">)</span>

<span class="n">a_fit_gengamma</span><span class="p">,</span> <span class="n">c_fit_gengamma</span><span class="p">,</span> <span class="n">scale_fit_gengamma</span> <span class="o">=</span> <span class="n">result_gengamma</span><span class="p">.</span><span class="n">x</span>
</code></pre></div></div> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>a</td> <td>26.6110</td> </tr> <tr> <td>c</td> <td>1.5835</td> </tr> <tr> <td>scale</td> <td>8.4092</td> </tr> </tbody> </table> <p><br/><br/></p> <h3 id="34-diagnostic-checks">3.4 Diagnostic Checks</h3> <p>To validate the fit, we:</p> <ul> <li>simulate onset ages using the fitted parameters</li> <li>plot a histogram compared to the reported median</li> <li>overlay the fitted CDF against empirical quartile points</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/survival_model_diagnostic_check.png" sizes="95vw"/> <img src="/assets/img/survival_model_diagnostic_check.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This confirms that the generalized gamma model aligns well with both the median and quartile anchors.<br/><br/></p> <h2 id="4-adding-uncertainty-probabilistic-sensitivity-analysis-psa">4. Adding Uncertainty: Probabilistic Sensitivity Analysis (PSA)</h2> <h3 id="41-why-psa">4.1 Why PSA?</h3> <p>Health economic and epidemiological models require not only point estimates but also credible intervals, especially when clinical evidence is sparse.</p> <p>Parameter uncertainty in the fitted model must therefore be propagated to all downstream quantities.</p> <p>The overarching goal is to generate randomly sampled, correlated parameter sets that reflect the plausible variance and covariance structure of the original fitted models for PSA.</p> <p>The conceptual procedures are as follows,</p> <ol> <li>Identify the model parameters</li> <li>Build the variance–covariance matrix</li> <li>Calculate the Cholesky factor</li> <li>Generate random noise using MVN</li> <li>Apply the Cholesky factor to preserve correlations</li> <li>Compute the final random parameter draws</li> <li>Use the sampled parameters in the survival model<br/></li> </ol> <h3 id="42-building-the-variance-covariance-matrix">4.2 Building the Variance-Covariance Matrix</h3> <p>We approximate the Hessian numerically at the parameter optimum. Inverting the Hessian yields the variance–covariance matrix of the parameter estimates.</p> <p>This matrix captures both individual parameter uncertainty and correlations among parameters.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">a_fit</span><span class="p">,</span> <span class="n">c_fit</span><span class="p">,</span> <span class="n">scale_fit</span><span class="p">])</span>
<span class="n">H</span> <span class="o">=</span> <span class="nf">approx_hess</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">gengamma_objective</span><span class="p">)</span>
<span class="n">vcov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: right"> </th> <th style="text-align: right">a variance/covariance</th> <th style="text-align: right">c variance/covariance</th> <th style="text-align: right">scale variance/covariance</th> </tr> </thead> <tbody> <tr> <td style="text-align: right">a</td> <td style="text-align: right">595.13</td> <td style="text-align: right">-17.76</td> <td style="text-align: right">-315.06</td> </tr> <tr> <td style="text-align: right">c</td> <td style="text-align: right">-17.76</td> <td style="text-align: right">0.55</td> <td style="text-align: right">9.63</td> </tr> <tr> <td style="text-align: right">scale</td> <td style="text-align: right">-315.06</td> <td style="text-align: right">9.63</td> <td style="text-align: right">169.34</td> </tr> </tbody> </table> <p><br/> This is a 3×3 variance-covariance matrix for the 3 fitted parameters of the generalized gamma distribution.</p> <p>Diagonal entries = variances of each parameter.</p> <p>Example:</p> \[\mathrm{Var}(a) = 595.13, \quad \mathrm{Var}(c) = 0.551, \quad \mathrm{Var}(\text{scale}) = 169.34\] <p>Off-diagonal entries = covariances between parameters.</p> <p>Example:</p> \[\mathrm{Cov}(a, \text{scale}) = -315.06\] <p><br/></p> <h3 id="43-multivariate-normal-sampling">4.3 Multivariate Normal Sampling</h3> <p>Using Cholesky decomposition, we sample thousands of parameter sets from a multivariate normal distribution:</p> \[\theta^{(i)} \sim \text{MVN}(\hat{\theta},\, \Sigma)\] <p>This forms the backbone of the PSA.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">cholesky</span><span class="p">(</span><span class="n">vcov</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">standard_normal</span><span class="p">((</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">theta_draws</span> <span class="o">=</span> <span class="n">theta_hat</span> <span class="o">+</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">L</span><span class="p">.</span><span class="n">T</span>
</code></pre></div></div> <p>Even though the fitted parameters \((\mu, \log\sigma, Q)\) are point estimates, each has uncertainty, and they are correlated. For example, if log-sigma increases slightly, Q might decrease slightly to keep the model a good fit.</p> <p>To reflect this, we sample parameters from a multivariate normal distribution centered at the MLE estimates with covariance equal to the Hessian-derived covariance matrix.</p> <p>Here’s what the code does:<br/></p> <ol> <li> <p>standard_normal((5000,3)) → generates 5,000 draws of independent noise, one for each parameter.</p> </li> <li> <p>L = cholesky(vcov) → finds a matrix L such that \(LL^\top = \text{covariance}\).<br/> This matrix converts independent noise into correlated noise with the correct variability.</p> </li> <li> <p>Z @ L.T → transforms the independent noise into noise that matches the covariance structure of the parameters.</p> </li> <li> <p>theta_hat + … → shifts the draws so they are centered at the fitted parameters.</p> </li> </ol> <p>The resulting theta_draws are 5,000 simulated sets of generalized gamma parameters that realistically reflect estimation uncertainty. These are used to visualize uncertainty in survival curves, median survival, RMST, or other model outputs.<br/><br/></p> <h2 id="5-estimating-age-band-probabilities">5. Estimating Age-Band Probabilities</h2> <p>For many models, we need to know the probability that onset occurs within:</p> <ul> <li><strong>0–12 years</strong></li> <li><strong>12–18 years</strong></li> <li><strong>18+ years</strong></li> </ul> <p>Using the CDF from each Monte Carlo draw, we compute:</p> \[P(a \leq \text{onset} &lt; b) = F(b) - F(a)\] <p>This avoids slow individual-level simulation and supports rapid PSA.</p> <p>A summary table (across ~5,000 draws) includes:</p> <ul> <li>mean probability</li> <li>standard deviation</li> <li>median</li> <li>95% uncertainty interval (2.5–97.5th percentile)</li> </ul> <table> <thead> <tr> <th>Age Band</th> <th>Mean</th> <th>SD</th> <th>Median</th> <th>CI Lower (2.5%)</th> <th>CI Upper (97.5%)</th> </tr> </thead> <tbody> <tr> <td>0–12</td> <td>2.67%</td> <td>14.00%</td> <td>0.00%</td> <td>0.00%</td> <td>44.47%</td> </tr> <tr> <td>12–18</td> <td>1.95%</td> <td>9.41%</td> <td>0.00%</td> <td>0.00%</td> <td>23.81%</td> </tr> <tr> <td>18+</td> <td>95.38%</td> <td>18.45%</td> <td>100.00%</td> <td>9.54%</td> <td>100.00%</td> </tr> </tbody> </table> <p><br/>The results show:</p> <ul> <li>~95% of onsets occur after age 18</li> <li>0–12 and 12–18 have wide uncertainty due to very low event frequency</li> </ul> <p>This output can be plugged directly into disease models.<br/><br/></p> <h2 id="6-visualizing-uncertainty-cdf-ensembles">6. Visualizing Uncertainty: CDF Ensembles</h2> <p>One of the most insightful plots overlays:</p> <ul> <li><strong>Light gray curves</strong>: thousands of CDFs generated from sampled parameters</li> <li><strong>A blue line</strong>: mean CDF across all Monte Carlo draws</li> <li><strong>Red points</strong>: empirical quartile anchors</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/psa_diag_check.png" sizes="95vw"/> <img src="/assets/img/psa_diag_check.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This reveals:</p> <ul> <li>Strong alignment near the median</li> <li>Greatest uncertainty in the tails</li> <li>Minimal probability mass before ~20 years</li> <li>A steep probability rise between 50–70 years</li> </ul> <p>The ensemble plot illustrates not just the best-fit curve, but the <em>range of plausible distributions</em> given uncertainty in the literature.<br/><br/></p> <h2 id="7-ensuring-reproducibility">7. Ensuring Reproducibility</h2> <p>Reproducibility is essential for transparent modeling, peer review, and regulatory or client submissions.</p> <p>At the end of the analysis, we record:</p> <ul> <li>Python version</li> <li>OS</li> <li>NumPy, pandas, SciPy, statsmodels, matplotlib versions <br/><br/></li> </ul> <h2 id="8-conclusion">8. Conclusion</h2> <p>Even when individual-level data are unavailable, we can still reconstruct a meaningful, validated age-at-onset distribution using parametric survival models.</p> <p>The generalized gamma distribution offers excellent flexibility, and the combination of:</p> <ul> <li>Quantile matching</li> <li>Diagnostic validation</li> <li>Probabilistic sensitivity analysis (PSA)</li> </ul> <p>This approach fits naturally into broader health economic and epidemiological frameworks, enabling:</p> <ul> <li>Estimation of onset-age probabilities</li> <li>Sensitivity and uncertainty analyses</li> <li>Population-level burden and cost projections</li> <li>Forecasting models informed by realistic evidence intervals</li> </ul>]]></content><author><name></name></author><category term="bioinformatics"/><category term="sensitivity-analysis,"/><category term="survival-model,"/><category term="epi,"/><category term="health-economy"/><summary type="html"><![CDATA[In rare disease epidemiology, researchers often face a familiar challenge: most published studies do not provide individual-level patient data. Instead, they report only high-level summary statistics such as quartiles, means, and total sample size. Yet for health economic modeling, burden of disease estimation, or natural history modeling, we still need to understand the full age-at-onset distribution, including its shape, skew, and uncertainty.]]></summary></entry><entry><title type="html">Turning Messy Symptom Text into Structured Insights</title><link href="https://davidzhao1015.github.io/blog/2025/map-hpo-symptom/" rel="alternate" type="text/html" title="Turning Messy Symptom Text into Structured Insights"/><published>2025-09-19T00:00:00+00:00</published><updated>2025-09-19T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/map-hpo-symptom</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/map-hpo-symptom/"><![CDATA[<h2 id="the-challenge">The Challenge</h2> <p>In rare disease research, clinical symptoms are often reported in free text, words like “droopy eyelid”, “muscle weakness”, or “difficulty feeding.” While these descriptions are clinically meaningful, their lack of standardization makes it nearly impossible to merge, compare, or analyze across studies.</p> <p>This inconsistency is a major barrier for literature reviews, meta-analyses, and evidence synthesis. Without a common language, symptoms that mean the same thing may be counted differently, and downstream models risk being biased or incomplete.<br/><br/></p> <h2 id="a-structured-solution-with-hpo">A Structured Solution with HPO</h2> <p>The Human Phenotype Ontology (HPO) provides a standardized vocabulary for phenotypic abnormalities, widely used in genomics, clinical genetics, and rare disease studies. If free-text symptoms can be reliably mapped to HPO terms, researchers gain:</p> <ul> <li>Consistency: every symptom linked to a unique ID.</li> <li>Context: ontology hierarchy shows how terms relate.</li> <li>Richness: access to synonyms, definitions, and metadata.</li> </ul> <p>To tackle this, I built a Python pipeline that takes reported symptoms as input and outputs a structured table of standardized HPO terms, IDs, and lineage paths.<br/><br/></p> <h2 id="how-the-workflow-works">How the Workflow Works</h2> <p>The pipeline combines API lookups, fuzzy string matching, and ontology metadata into a step-by-step process:</p> <ol> <li>Search the HPO API with the reported symptom to retrieve candidate terms.</li> <li>Fuzzy match validation: compute similarity between the input text and candidate term names.</li> <li>Synonym and definition lookup from the official HPO ontology file.</li> <li>Ontology lineage extraction: trace each matched term back to the root concept, showing context.</li> <li>Decision rule: accept if the fuzzy score is ≥80, or if the input matches an official synonym.</li> </ol> <p>This hybrid approach ensures both precision and recall, handling common synonyms and spelling variations gracefully.<br/><br/></p> <h2 id="a-peek-under-the-hood">A Peek Under the Hood</h2> <p>The implementation uses a few key Python tools:</p> <ul> <li><code class="language-plaintext highlighter-rouge">rapidfuzz</code> → fast, permissive fuzzy string matching library.</li> <li><code class="language-plaintext highlighter-rouge">obonet</code> → parses ontology files like hp.obo into a traversable network.</li> <li><code class="language-plaintext highlighter-rouge">functools.lru_cache</code> → caches ontology queries for speed.</li> <li><code class="language-plaintext highlighter-rouge">requests</code> → queries the JAX HPO API with retries for robustness.</li> </ul> <p>Together, they form a lightweight but powerful pipeline that scales across hundreds of symptoms.<br/><br/></p> <h2 id="demonstration">Demonstration</h2> <p>Here’s an example with three symptoms:</p> <table> <thead> <tr> <th>reported_symptom</th> <th>hpo_term</th> <th>hpo_id</th> <th>fuzzy_score</th> <th>status</th> </tr> </thead> <tbody> <tr> <td>ptosis</td> <td>Ptosis</td> <td>HP:0000508</td> <td>83.3</td> <td>matched</td> </tr> <tr> <td>weak suck</td> <td>Weak cry</td> <td>HP:0001612</td> <td>58.8</td> <td>not matched</td> </tr> <tr> <td>exercise intolerance</td> <td>Exercise intolerance</td> <td>HP:0003546</td> <td>95.0</td> <td>matched</td> </tr> </tbody> </table> <ul> <li>“ptosis” maps cleanly.</li> <li>“weak suck” is ambiguous (flagged as not matched).</li> <li>“exercise intolerance” matches perfectly.</li> </ul> <p>This table is the end product: a structured, machine-readable mapping that can feed into larger data workflows.<br/><br/></p> <h2 id="applications-in-rare-disease-research">Applications in Rare Disease Research</h2> <p>This pipeline is directly applicable to:</p> <ul> <li>Systematic reviews &amp; evidence synthesis → harmonize symptom data across dozens of studies.</li> <li>Registry curation → align patient-reported outcomes with controlled vocabularies.</li> <li>Epidemiology &amp; modeling → enable symptom-based stratification, subgroup analysis, or disease trajectory modeling.</li> </ul> <p>By reducing noise from free text, it supports more reliable cross-study comparisons and integration.<br/><br/></p> <h2 id="limitations--future-work">Limitations &amp; Future Work</h2> <p>Like any tool, there’s room for improvement:</p> <ul> <li>API availability and response speed.</li> <li>Lineage extraction currently follows only the first parent (simplified).</li> <li>Fuzzy thresholds may need tuning per dataset.</li> </ul> <p>Future directions include:</p> <ul> <li>Integrating semantic embeddings for deeper understanding of symptom phrases.</li> <li>Linking to other vocabularies (MeSH, UMLS) for interoperability.<br/><br/></li> </ul> <h2 id="conclusion">Conclusion</h2> <p>This pipeline addresses a real-world pain point in rare disease research: the challenge of inconsistent symptom reporting. By turning free text into structured HPO terms, it creates a reproducible foundation for evidence synthesis, registry development, and computational modeling.</p> <p>And the best part? Its design is generalizable, the same workflow could be extended to other areas of biomedical text mining, from electronic health records to clinical trial reports.<br/><br/></p> <h2 id="try-it-yourself">Try It Yourself</h2> <p>The full notebook and code are available on <a href="https://github.com/davidzhao1015/map-hpo-symptom/blob/main/HPO-symptom-standardize.ipynb">Github</a>.</p> <p>This article is meant for the rare disease research community. But on my portfolio site, I’ll showcase the technical implementation in detail for potential collaborators.</p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="epi,"/><category term="meta-analysis,"/><category term="ontology,"/><category term="rare-disease"/><summary type="html"><![CDATA[The Challenge]]></summary></entry><entry><title type="html">Visualizing Microbiome Taxonomy with Metacoder in R: A Step-by-Step Guide</title><link href="https://davidzhao1015.github.io/blog/2025/tax-heattree-r/" rel="alternate" type="text/html" title="Visualizing Microbiome Taxonomy with Metacoder in R: A Step-by-Step Guide"/><published>2025-08-23T00:00:00+00:00</published><updated>2025-08-23T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/tax-heattree-r</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/tax-heattree-r/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In microbiome research, stacked bar charts are a go-to method for showing the abundance of different taxa. But there’s a catch — they don’t really show the hierarchical relationships within the taxonomy data.</p> <p>That’s where the metacoder R package comes in. Published in <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5340466/">PLoS ONE in 2017</a> and available on CRAN, metacoder makes it easier to explore and visualize taxonomy in a way that reflects its natural hierarchy, while also mapping data across different taxonomic levels.</p> <p>In this post, I’ll show you how to turn your own data into clean, publication-ready plots using metacoder. The package includes helpful <a href="https://grunwaldlab.github.io/metacoder_documentation/example.html">tutorials</a>, but they don’t always cover every scenario — like the one I’ll walk you through here.</p> <p>I’ll also share tips and lessons learned from my experience as an intermediate R user and microbiome researcher, so you can save time and avoid common pitfalls.<br/><br/></p> <h2 id="example">Example</h2> <p>For this example, I used a dataset curated from the <a href="https://pubmed.ncbi.nlm.nih.gov/35412130/">review</a> by Dr. Micheal Gaenzle on key microbiomes in food fermentation. The input data includes the full taxonomy lineages for more than 30 genera commonly involved in food fermentation.</p> <p>One particularly interesting table in the review maps these 30+ bacterial genera to 115 types of fermented foods from around the world. That inspired me to create a family-focused heat tree to visualize the biodiversity of bacteria involved in fermentation.</p> <p>Gathering detailed metadata for a perfectly accurate tree takes time, so for now, I worked with aggregated data — specifically, the proportion of fermented food types linked to each genus. While metacoder can integrate numeric data into heat trees, I found that it generated a misleading legend in this case. To keep the visualization clear, I’ve chosen not to display numeric values on the plot. Instead, I’ll describe the distribution of key bacterial families in the text alongside the visualization.</p> <p>The example input data can be download <a href="https://github.com/davidzhao1015/taxonomy-lineage-viz/blob/main/tax_abund_data.csv">here</a>.</p> <p>With that context set, let’s walk through the steps to prepare the data and generate the heat tree in R, so you can try it with your own dataset.<br/><br/></p> <h2 id="programmatic-workflow">Programmatic workflow</h2> <p>The overall workflow for this example is straightforward and involves three main steps:</p> <ol> <li>Read the taxonomy input data</li> <li>Parse the data into a taxmap object that is compatible with metacoder</li> <li>Generate and customize the heat tree visualization</li> </ol> <p>In the next sections, I’ll walk through each step, showing the code and explaining how you can adapt it to your own dataset.<br/><br/></p> <h2 id="implementation-in-r">Implementation in R</h2> <p><strong>1. Load and inspect your taxonomy data</strong></p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load required library</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">metacoder</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">

</span><span class="c1"># Read in the taxonomy input file</span><span class="w">
</span><span class="n">tax_abund</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"tax_abund_data.csv"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; head(tax_abund)
  Species_label Fermented.foods1 normalized_prop        Kingdom         Phylum
1         spp_1               16       0.1391304 Pseudomonadati Pseudomonadota
2        spp_10              101       0.8782609      Bacillati      Bacillota
3        spp_11              101       0.8782609      Bacillati      Bacillota
4        spp_12              101       0.8782609      Bacillati      Bacillota
5        spp_13              101       0.8782609      Bacillati      Bacillota
6        spp_14              101       0.8782609      Bacillati      Bacillota
                Class           Order           Family                   Genus
1 Alphaproteobacteria Acetobacterales Acetobacteraceae             Acetobacter
2             Bacilli Lactobacillales Lactobacillaceae    Companilactobacillus
3             Bacilli Lactobacillales Lactobacillaceae Schleiferilactobacillus
4             Bacilli Lactobacillales Lactobacillaceae       Ligilactobacillus
5             Bacilli Lactobacillales Lactobacillaceae     Lactiplantibacillus
6             Bacilli Lactobacillales Lactobacillaceae      Loigolactobacillus
  Species
1      NA
2      NA
3      NA
4      NA
5      NA
6      NA
</code></pre></div></div> <p>This file contains the full taxonomy lineages for approximately 30 genera mentioned in Dr. Gaenzle’s review.</p> <ul> <li><strong>Rows</strong>: Each row represents one genus.</li> <li><strong>Columns</strong>: Include the full taxonomy path (Kingdom → Phylum → Class → Order → Family → Genus), along with aggregated counts and proportions of food types containing that genus.</li> <li><strong>Species column</strong>: Values are set to NA where species-level data is not available.<br/><br/></li> </ul> <p><strong>2. Parse the data into a <code class="language-plaintext highlighter-rouge">taxmap</code> object</strong></p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">obj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">parse_tax_data</span><span class="p">(</span><span class="n">tax_abund</span><span class="p">,</span><span class="w"> </span><span class="n">class_cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="o">:</span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="n">named_by_rank</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; print(obj)
&lt;Taxmap&gt;
  66 taxa: ab. Pseudomonadati, ac. Bacillati ... co. Lactobacillus
  66 edges: NA-&gt;ab, NA-&gt;ac, ab-&gt;ad ... bd-&gt;cm, be-&gt;cn, at-&gt;co
  2 data sets:
    tax_data:
      # A tibble: 36 × 11
        taxon_id Species_label Fermented.foods1 normalized_prop
        &lt;chr&gt;    &lt;chr&gt;                    &lt;int&gt;           &lt;dbl&gt;
      1 bf       spp_1                       16           0.139
      2 bg       spp_10                     101           0.878
      3 bh       spp_11                     101           0.878
      # ℹ 33 more rows
      # ℹ 7 more variables: Kingdom &lt;chr&gt;, Phylum &lt;chr&gt;, Class &lt;chr&gt;,
      #   Order &lt;chr&gt;, Family &lt;chr&gt;, Genus &lt;chr&gt;, Species &lt;lgl&gt;
      # ℹ Use `print(n = ...)` to see more rows
    tax_abund:
      # A tibble: 66 × 2
        taxon_id normalized_prop
        &lt;chr&gt;              &lt;dbl&gt;
      1 ab                 0.835
      2 ac                20.7  
      3 ad                 0.835
      # ℹ 63 more rows
      # ℹ Use `print(n = ...)` to see more rows
  0 functions:
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">parse_tax_data()</code> transforms your table into a taxmap object that powers the heat tree visualization.</p> <ul> <li>The <code class="language-plaintext highlighter-rouge">class_cols</code> argument points to the column that contains the taxonomy path.</li> <li>Setting <code class="language-plaintext highlighter-rouge">named_by_rank</code> = TRUE ensures the function recognizes each taxonomic rank correctly.</li> </ul> <p><strong>3. Generate the heat tree</strong></p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">

</span><span class="n">ht_plot_abund</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">heat_tree</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span><span class="w">
    </span><span class="n">node_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="o">$</span><span class="n">taxon_names</span><span class="p">(),</span><span class="w">
    </span><span class="n">node_color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="o">$</span><span class="n">n_obs</span><span class="p">(),</span><span class="w">
    </span><span class="n">node_color_range</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"purple"</span><span class="p">,</span><span class="w"> </span><span class="s2">"yellow"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w">
    </span><span class="n">initial_layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reingold-tilford"</span><span class="p">,</span><span class="w">
    </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"davidson-harel"</span><span class="p">,</span><span class="w">
    </span><span class="n">node_color_axis_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Number of genera \nwithin the taxa"</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="n">ht_plot_abund</span><span class="w">
</span></code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/heat_tree_plot.png" sizes="95vw"/> <img src="/assets/img/heat_tree_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This initial plot focuses on showing the hierarchical relationships.</p> <ul> <li>Because the numeric proportions generated a misleading legend, no quantitative data are mapped here.</li> <li>This keeps the visualization clean while still clearly showing the relationships between families and genera involved in fermentation.<br/><br/></li> </ul> <h2 id="bonus">Bonus</h2> <h3 id="optional-add-quantitative-data-with-caution">Optional: Add quantitative data (with caution)</h3> <p>After you’ve parsed your taxonomy into a taxmap object, you can compute per-taxon values (e.g., the proportion of fermented food types per genus) and attach them to the object for plotting.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate per-taxon abundance (here using a column called "normalized_prop")</span><span class="w">
</span><span class="c1"># This creates obj$data$tax_abund with one value per taxon</span><span class="w">
</span><span class="n">obj</span><span class="o">$</span><span class="n">data</span><span class="o">$</span><span class="n">tax_abund</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">calc_taxon_abund</span><span class="p">(</span><span class="w">
	</span><span class="n">obj</span><span class="p">,</span><span class="w"> 
	</span><span class="s2">"tax_data"</span><span class="p">,</span><span class="w"> 
	</span><span class="n">cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"normalized_prop"</span><span class="p">)</span><span class="w">

</span><span class="c1"># Draw a heat tree using the computed values</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">

</span><span class="n">ht_plot_abund2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">heat_tree</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span><span class="w">
    </span><span class="n">node_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="o">$</span><span class="n">taxon_names</span><span class="p">(),</span><span class="w"> </span><span class="c1"># Show taxon names</span><span class="w">
    </span><span class="n">node_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="o">$</span><span class="n">data</span><span class="o">$</span><span class="n">tax_abund</span><span class="o">$</span><span class="n">normalized_prop</span><span class="p">,</span><span class="w"> </span><span class="c1"># Size by proportions</span><span class="w">
    </span><span class="n">node_color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="o">$</span><span class="n">data</span><span class="o">$</span><span class="n">tax_abund</span><span class="o">$</span><span class="n">normalized_prop</span><span class="p">,</span><span class="w"> </span><span class="c1"># Color by proportions</span><span class="w">
    </span><span class="n">node_color_range</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"purple"</span><span class="p">,</span><span class="w"> </span><span class="s2">"yellow"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w"> </span><span class="c1"># Color palette</span><span class="w">
    </span><span class="n">initial_layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reingold-tilford"</span><span class="p">,</span><span class="w">
    </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"davidson-harel"</span><span class="p">,</span><span class="w">
    </span><span class="n">node_color_axis_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Prop in Fermented Foods"</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="n">ht_plot_abund2</span><span class="w">
</span></code></pre></div></div> <p><strong>Why the caution?</strong></p> <p>Mapping numbers to node color/size can be powerful, but the legend and scaling can be misleading if your values are tightly clustered, all zeros, or contain many missing values. In my case, the legend confused readers, so I left numeric mappings out of the final figure and explained key patterns in the text instead.</p> <h3 id="optional-focus-at-the-family-level">Optional: Focus at the family level</h3> <p>You can subset the taxonomy to simplify the figure or highlight a specific level (e.g., <strong>family</strong>) and then plot. This often reveals clearer biological patterns.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Keep families (and their super/subtaxa as you prefer) and color by count of child taxa</span><span class="w">
</span><span class="c1"># The example below hides genus nodes to emphasize family-level structure.</span><span class="w">

</span><span class="n">ht_plot_abund3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">obj</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> 
    </span><span class="n">filter_taxa</span><span class="p">(</span><span class="n">taxon_ranks</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s2">"Genus"</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="c1"># Drop genus nodes for a cleaner family-level view</span><span class="w">
    </span><span class="n">heat_tree</span><span class="p">(</span><span class="n">node_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">taxon_names</span><span class="p">,</span><span class="w"> 
    </span><span class="n">node_color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n_obs</span><span class="p">,</span><span class="w">
    </span><span class="n">node_color_range</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"purple"</span><span class="p">,</span><span class="w"> </span><span class="s2">"yellow"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w">
    </span><span class="n">initial_layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reingold-tilford"</span><span class="p">,</span><span class="w">
    </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"davidson-harel"</span><span class="p">,</span><span class="w">
    </span><span class="n">node_color_axis_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Number of \ngenera within"</span><span class="p">)</span><span class="w">

</span><span class="n">ht_plot_abund3</span><span class="w">
</span></code></pre></div></div> <p><br/></p> <h2 id="data-interpretation">Data Interpretation</h2> <p>The heat tree highlights several key insights about the diversity of bacteria involved in food fermentation:</p> <ul> <li>Core Fermenters – Families like <em>Lactobacillaceae</em>, <em>Leuconostocaceae</em>, <em>Streptococcaceae</em>, <em>Enterococcaceae</em>, and <em>Carnobacteriaceae</em> form the backbone of dairy, cereal, and vegetable fermentations.</li> <li>Initiators – <em>Enterobacteriaceae</em> and <em>Erwiniaceae</em> often kick-start spontaneous fermentations in vegetables, cereals, tubers, coffee, and cocoa.</li> <li>Niche Specialists – <em>Acetobacteraceae</em>, <em>Bacillaceae</em>, and <em>Propionibacteriaceae</em> are critical in vinegar, natto, soy/ fish sauces, and Swiss cheese fermentations.</li> <li>Surface &amp; Meat Fermenters – Families like <em>Staphylococcaceae</em>, <em>Micrococcaceae</em>, and <em>Brevibacteriaceae</em> play key roles in ripening, aroma development, and safety in meat and cheese fermentations.</li> <li>Minor but Emerging Players – <em>Eggerthellaceae</em> show a secondary but notable presence in vegetable fermentations such as sauerkraut and kimchi.</li> </ul> <p>These insights not only showcase the rich biodiversity of fermenting microbes but also highlight their specialized roles in shaping flavors, textures, and safety across different foods. For researchers, educators, or fermentation enthusiasts, such visualizations can guide strain selection, recipe development, and deeper exploration into the microbial ecosystems that make our favorite fermented products possible.<br/><br/></p> <h2 id="personal-tips">Personal tips</h2> <p>Here are a few tips from my experience working with metacoder.</p> <p><strong>Start with the examples</strong>: The package tutorials and help docs include plenty of sample datasets. Taking time to explore these will make it much easier to prepare your own input data correctly.</p> <p><strong>Legend adjustments are limited</strong>: While metacoder offers a lot of flexibility in customizing your plots, the position of the legend doesn’t seem to be adjustable. Plan your layout with that limitation in mind.</p> <p><strong>Interpret node sizes and colors carefully</strong>: These elements are proportional to your quantitative data, such as OTU counts or other biological variables. Always double-check your legend to avoid over- or under-interpreting the results.</p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="taxonomy,"/><category term="microbiome,"/><category term="visualization"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Retrieving Full Taxonomy Lineage from NCBI with Python</title><link href="https://davidzhao1015.github.io/blog/2025/retrieve-tax-lineage-API/" rel="alternate" type="text/html" title="Retrieving Full Taxonomy Lineage from NCBI with Python"/><published>2025-08-09T00:00:00+00:00</published><updated>2025-08-09T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/retrieve-tax-lineage-API</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/retrieve-tax-lineage-API/"><![CDATA[<h2 id="problem">Problem</h2> <p>In microbiome data analysis, taxonomy assignments—derived from marker genes or whole genomes—are essential for understanding microbial ecosystems when paired with abundance data. These assignments are not limited to single taxon names but are structured as hierarchical lineages across multiple taxonomic ranks, offering richer biological insights. However, researchers often only have partial taxonomy data (e.g., names at a single rank), which limits interpretability and analytical depth.<br/><br/></p> <h2 id="solution">Solution</h2> <p>To unlock the full potential of taxonomy-based insights, incomplete taxon names can be mapped to their complete hierarchical lineages by retrieving standardized taxonomy data from the NCBI database. This approach enriches the dataset and supports more robust biological interpretation.<br/><br/></p> <h2 id="background">Background</h2> <p>The <strong>NCBI Taxonomy</strong> is the official nomenclature and classification resource for the <strong>International Nucleotide Sequence Database Collaboration (INSDC)</strong>, which includes GenBank, EMBL, and DDBJ. It provides curated organism names and an approximately phylogenetic classification for all source organisms represented in INSDC records, serving as a framework for linking resources within NCBI and to external taxon-specific information. Names are selected by NCBI curators based on published taxonomic data and expert opinion, ensuring a single current name is assigned to each taxon. The database focuses on nomenclature and systematics rather than detailed taxon descriptions.<br/><br/></p> <h2 id="solution-implementation">Solution implementation</h2> <p>To retrieve full taxonomy lineages, one can use either the NCBI Taxonomy web browser or a programmatic method. The programmatic approach is highly efficient and advantageous, allowing automation of repetitive retrieval for multiple taxa at once. The NCBI E-utilities Python API enables access to the web server directly from Python scripts, integrating seamlessly with other steps in a microbiome data analysis pipeline.</p> <p>This section includes two parts:</p> <ol> <li>A reusable function to download and parse the full taxonomy lineage for a given taxon at a single rank.</li> <li>An example demonstrating how to apply this function to multiple taxa, using a food fermentation microbiome dataset as a case study.<br/><br/></li> </ol> <h3 id="reusable-script">Reusable script</h3> <p>The customizable function below retrieves the full lineage using the Bio.Entrez.esearch(), Bio.Entrez.efetch(), and Bio.Entrez.read() functions from the <strong>Bio.Entrez</strong> submodule in the Biopython package.</p> <p>The input is a taxon string at a given rank, and the output is a list of taxa from higher to lower ranks. You can use this function as is or adapt it for your specific needs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">Bio</span> <span class="kn">import</span> <span class="n">Entrez</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">os</span>


<span class="k">def</span> <span class="nf">get_taxonomy_lineage</span><span class="p">(</span><span class="n">taxon_name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">,</span> <span class="n">email</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Fetches complete taxonomy lineage based on the known taxonomic level (the highest resolution available) from the NCBI taxonomy database.

    Args:
        rank (str): The taxonomic rank to search for (default is </span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="s">). </span><span class="sh">"</span><span class="s">Family</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">Order</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">Class</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">Phylum</span><span class="sh">"</span><span class="s">, or </span><span class="sh">"</span><span class="s">Kingdom</span><span class="sh">"</span><span class="s"> can also be used.
        taxon_name (str): The name of the taxon to search for. If None, the function will not perform a search.
        email (str): Your email address for NCBI Entrez. This is required to track usage and for contact in case of issues.

    Returns:
        list: A list containing the lineage of the taxon, or None if not found.
        The lineage includes domain, kingdom (or clade), phylum, class, order, and family.
        If the taxon is not found, it returns None.
        If the rank is invalid, it returns None with an error message.
        If the taxon_name is None, it returns None with an error message.
    </span><span class="sh">"""</span>
    <span class="c1">#--- Set the email for NCBI Entrez ---
</span>    <span class="c1"># This is required by NCBI to track usage and for contact in case of issues
</span>    <span class="c1"># Replace with your email address
</span>    <span class="n">Entrez</span><span class="p">.</span><span class="n">email</span> <span class="o">=</span> <span class="n">email</span> <span class="ow">or</span> <span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">NCBI_EMAIL</span><span class="sh">"</span><span class="p">)</span> 


    <span class="c1">#--- Input validation ---
</span>    <span class="c1"># Check if taxon_name is provided
</span>    <span class="k">if</span> <span class="n">taxon_name</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Taxon name is None. Please provide a valid taxon name.</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="c1"># Check if rank is valid
</span>    <span class="k">if</span> <span class="n">rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Family</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Order</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Class</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Phylum</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Kingdom</span><span class="sh">"</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Invalid rank: </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">. Please use one of the following ranks: Genus, Family, Order, Class, Phylum, Kingdom.</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>


    <span class="c1">#--- Search for the genus in the NCBI taxonomy database ---
</span>    <span class="c1"># The search term is formatted to include the genus name followed by "[Genus]" to specify the search field
</span>    <span class="c1"># This ensures that the search is limited to the genus level in the taxonomy database
</span>    <span class="n">handle</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">esearch</span><span class="p">(</span><span class="n">db</span><span class="o">=</span><span class="sh">"</span><span class="s">taxonomy</span><span class="sh">"</span><span class="p">,</span> <span class="n">term</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">taxon_name</span><span class="si">}</span><span class="s">[</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">]</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">record</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">read</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span> <span class="c1"># Read the search results
</span>    <span class="n">handle</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span> <span class="c1"># Close the handle to free resources
</span>

    <span class="c1">#--- Check if any IDs were found for the genus ---
</span>    <span class="c1"># If no IDs are found, print a message and return None
</span>    <span class="c1"># This is important to handle cases where the genus does not exist in the database
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">record</span><span class="p">[</span><span class="sh">"</span><span class="s">IdList</span><span class="sh">"</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">No taxonomy ID found for </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">taxon_name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> 
        <span class="k">return</span> <span class="bp">None</span>


    <span class="c1">#--- Fetch the taxonomy record using the first ID found ---
</span>    <span class="c1"># The first ID in the IdList is used to fetch the complete taxonomy record
</span>    <span class="c1"># This is because the search may return multiple IDs, but we are interested in the first one
</span>    <span class="c1"># The efetch function retrieves the record in XML format for easier parsing
</span>    <span class="c1"># The record contains detailed information about the taxonomy, including lineage
</span>    <span class="c1"># The lineage includes domain, kingdom (or clade), phylum, class, order, and family
</span>    <span class="n">taxid</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="sh">"</span><span class="s">IdList</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Get the first taxonomy ID from the search results
</span>    <span class="n">handle</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">efetch</span><span class="p">(</span><span class="n">db</span><span class="o">=</span><span class="sh">"</span><span class="s">taxonomy</span><span class="sh">"</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">taxid</span><span class="p">,</span> <span class="n">retmode</span><span class="o">=</span><span class="sh">"</span><span class="s">xml</span><span class="sh">"</span><span class="p">)</span> 
    <span class="n">records</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">read</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span> <span class="c1"># Read the fetched record
</span>    <span class="n">handle</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span> <span class="c1"># Close the handle to free resources
</span>
    <span class="c1">#--- Extract the lineage from the fetched record ---
</span>    <span class="c1"># The lineage is extracted from the first record in the list of records returned by efetch
</span>    <span class="c1"># The lineage is a string that includes the complete taxonomy hierarchy for the genus
</span>    <span class="c1"># It is formatted as "domain; kingdom; phylum; class; order; family"
</span>    <span class="n">lineage</span> <span class="o">=</span> <span class="n">records</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">Lineage</span><span class="sh">"</span><span class="p">]</span> <span class="c1"># Extract the lineage from the record, including domain, kingdom (or clade), phylum, class, order, and family
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">lineage</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">lineage_list</span> <span class="o">=</span> <span class="n">lineage</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">; </span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Split the lineage into a list
</span>    
    <span class="k">return</span> <span class="n">lineage_list</span>
</code></pre></div></div> <p>Here’s a quick example to give you an idea of the output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">genus</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Lactobacillus</span><span class="sh">"</span>
<span class="n">lineage</span> <span class="o">=</span> <span class="nf">get_taxonomy_lineage</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">,</span> <span class="n">taxon_name</span><span class="o">=</span><span class="n">genus</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Lineage for </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">lineage</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Lineage for Lactobacillus: ['cellular organisms', 'Bacteria', 'Bacillati', 'Bacillota', 'Bacilli', 'Lactobacillales', 'Lactobacillaceae']
</code></pre></div></div> <h3 id="example">Example</h3> <p>The example demonstrates applying the function to multiple bacterial taxa using loop iteration in Python, maximizing the efficiency of the Python API. You can adapt the code by replacing it with your own input data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define a list of target genera: Fermenting bacteria
</span><span class="n">genera</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Acetobacter</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Gluconacetobacter</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lentibacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Brevibacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Erwinia</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Enterobacter</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Pantoea</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Kosakonia</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Companilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Schleiferilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Ligilactobacillus</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Lactiplantibacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Loigolactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Paucilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Limosilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Fructilactobacillus</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Acetilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Secundilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lentilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Carnobacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Weissella</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Oenococcus</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Enterococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Tetragenococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Streptococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lactococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Pediococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Periweissella</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Leuconostoc</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Marinilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Alkalibacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Eggerthella</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Propionibacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Staphylococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Kocuria</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize an empty DataFrame to store all lineages
</span><span class="n">df_all_lineages</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">()</span>

<span class="c1"># Loop through each genus and get the taxonomy lineage
</span><span class="k">for</span> <span class="n">genus</span> <span class="ow">in</span> <span class="n">genera</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Processing genus: </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">lineage</span> <span class="o">=</span> <span class="nf">get_taxonomy_lineage</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">,</span> <span class="n">taxon_name</span><span class="o">=</span><span class="n">genus</span><span class="p">)</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>  <span class="c1"># Sleep to avoid hitting NCBI's rate limits ~3 requests per second without API key
</span>    
    <span class="c1"># Skip if lineage is None or does not belong to Bacteria
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">lineage</span> <span class="ow">or</span> <span class="n">lineage</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">Bacteria</span><span class="sh">"</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Skipping genus: </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="s"> (No lineage or not Bacteria)</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">continue</span>

    <span class="c1"># Create a DataFrame for the current genus
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="n">df_genus_lineage</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">([</span><span class="n">lineage</span><span class="p">[</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">Domain</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Kingdom</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Phylum</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Class</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Order</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Family</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">df_genus_lineage</span><span class="p">[</span><span class="sh">'</span><span class="s">Genus</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">genus</span>
        
        <span class="c1"># Append to the main DataFrame
</span>        <span class="n">df_all_lineages</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df_all_lineages</span><span class="p">,</span> <span class="n">df_genus_lineage</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error processing genus </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Print summary of successful retrievals
</span><span class="n">genus_successful</span> <span class="o">=</span> <span class="n">df_all_lineages</span><span class="p">[</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">genus_successful</span><span class="p">)</span><span class="si">}</span><span class="s"> out of </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">genera</span><span class="p">)</span><span class="si">}</span><span class="s"> genera were successfully retrieved.</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Save the DataFrame to a CSV file
</span><span class="n">output_file</span> <span class="o">=</span> <span class="sh">"</span><span class="s">taxonomy_lineages.csv</span><span class="sh">"</span>
<span class="n">df_all_lineages</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Lineages saved to </span><span class="si">{</span><span class="n">output_file</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Display the first few rows of the DataFrame
</span><span class="n">df_all_lineages</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div> <p>The first rows of the output table is as below.</p> <table> <thead> <tr> <th> </th> <th>Domain</th> <th>Kingdom</th> <th>Phylum</th> <th>Class</th> <th>Order</th> <th>Family</th> <th>Genus</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>Bacteria</td> <td>Pseudomonadati</td> <td>Pseudomonadota</td> <td>Alphaproteobacteria</td> <td>Acetobacterales</td> <td>Acetobacteraceae</td> <td>Acetobacter</td> </tr> <tr> <td>1</td> <td>Bacteria</td> <td>Pseudomonadati</td> <td>Pseudomonadota</td> <td>Alphaproteobacteria</td> <td>Acetobacterales</td> <td>Acetobacteraceae</td> <td>Gluconacetobacter</td> </tr> <tr> <td>2</td> <td>Bacteria</td> <td>Bacillati</td> <td>Bacillota</td> <td>Bacilli</td> <td>Bacillales</td> <td>Bacillaceae</td> <td>Lentibacillus</td> </tr> <tr> <td>3</td> <td>Bacteria</td> <td>Bacillati</td> <td>Actinomycetota</td> <td>Actinomycetes</td> <td>Micrococcales</td> <td>Brevibacteriaceae</td> <td>Brevibacterium</td> </tr> <tr> <td>4</td> <td>Bacteria</td> <td>Pseudomonadati</td> <td>Pseudomonadota</td> <td>Gammaproteobacteria</td> <td>Enterobacterales</td> <td>Erwiniaceae</td> <td>Erwinia</td> </tr> </tbody> </table> <p><br/></p> <h2 id="conclusion">Conclusion</h2> <p>Taxonomic lineage information provides far more value than a single taxon name, especially in microbiome data analysis, where biological context is key. By programmatically retrieving complete lineages from NCBI’s Taxonomy database, researchers can enrich partial datasets, standardize taxonomic ranks, and gain deeper ecological insights without manual lookups.</p> <p>This workflow demonstrates how a few lines of Python, combined with the Bio.Entrez API, can turn scattered names into structured, multi-rank taxonomy data. Whether you are refining microbial community analyses or preparing results for publication, automating taxonomy retrieval not only saves time but also ensures reproducibility and accuracy.</p> <p>If you often work with incomplete taxonomy data, integrating such an automated retrieval step into your pipeline is a small effort with a high return in both interpretability and research efficiency.<br/><br/></p> <h3 id="reference">Reference</h3> <p><a href="https://www.ncbi.nlm.nih.gov/books/NBK53758/">NCBI help</a><br/> <a href="https://biopython.org/docs/latest/Tutorial/chapter_entrez.html?utm_source=chatgpt.com">Bio.Entrez</a></p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="taxonomy,"/><category term="microbiome,"/><category term="biopython,"/><category term="ncbi-api"/><summary type="html"><![CDATA[Problem]]></summary></entry><entry><title type="html">How to Reconstruct Disease Onset Age Distributions Using Summary Statistics: A Rare Disease Use Case in Python</title><link href="https://davidzhao1015.github.io/blog/2025/reconstruct-age-distribution/" rel="alternate" type="text/html" title="How to Reconstruct Disease Onset Age Distributions Using Summary Statistics: A Rare Disease Use Case in Python"/><published>2025-07-09T00:00:00+00:00</published><updated>2025-07-09T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/reconstruct-age-distribution</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/reconstruct-age-distribution/"><![CDATA[<p>In rare disease research, one of the most frustrating barriers is the lack of individual-level data. While disease onset age is critical for modeling epidemiological burden and designing trials, most published studies only report summary statistics like medians or quartiles.</p> <p>But what if you could reconstruct a full onset age distribution from just these summary stats?</p> <p>In this post, I’ll walk you through how to simulate granular age-at-onset profiles using Python. We’ll use anti-GABABR autoimmune encephalitis (AIE) as a real-world example and fit three candidate distributions—log-normal, Weibull, and generalized gamma—using a technique called quantile matching.</p> <p>You’ll learn how to:</p> <ul> <li>Fit distributions to summary statistics using constrained optimization</li> <li>Simulate individual-level onset ages</li> <li>Compare model fit across distributions</li> <li>Estimate age-band proportions with bootstrap confidence intervals</li> </ul> <p>Whether you’re working on a burden-of-disease model, HTA submission, or just exploring rare disease analytics, this post will give you a reproducible template to start from.</p> <p>The reusable Python notebook is available at <a href="https://github.com/davidzhao1015/rebuild-age-distribution/blob/main/onset-age-dist-construction-AIE.ipynb">here</a></p> <hr/> <h2 id="step-1-background-and-input-data">Step 1: Background and Input Data</h2> <p>Our example comes from <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11087025/">Lamblin et al. (2024)</a>, who reported onset age statistics for anti-GABABR AIE:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">median</span> <span class="o">=</span> <span class="mi">66</span>
<span class="n">q1</span> <span class="o">=</span> <span class="mi">61</span> 
<span class="n">q3</span> <span class="o">=</span> <span class="mi">72</span>
<span class="nb">min</span> <span class="o">=</span> <span class="mi">19</span>
<span class="nb">max</span> <span class="o">=</span> <span class="mi">88</span>
<span class="n">mean</span> <span class="o">=</span> <span class="mi">67</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">111</span>
</code></pre></div></div> <p>For our modeling, we’ll focus on the three quantiles (Q1, median, Q3).</p> <hr/> <h2 id="step-2-quantile-matching-to-fit-distributions">Step 2: Quantile Matching to Fit Distributions</h2> <p>We’ll use constrained optimization to find the best-fitting parameters such that theoretical quantiles match the reported ones.</p> <p>Here’s the objective function for a log-normal distribution:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">lognorm</span>
<span class="kn">from</span> <span class="n">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Empirical quantiles
</span><span class="n">empirical_q</span> <span class="o">=</span> <span class="p">[</span><span class="mi">61</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">72</span><span class="p">]</span>  

<span class="c1"># Objective function: Minimize squared differences between model and empirical quantiles
</span><span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">if</span> <span class="n">sigma</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="nf">lognorm</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span>
    <span class="n">theo_q</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">ppf</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">theo_q</span><span class="p">)</span> <span class="o">-</span> <span class="n">empirical_q</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Initial guess for mu and sigma
</span><span class="n">initial_guess</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">66</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">]</span>

<span class="c1"># Optimize
</span><span class="n">result</span> <span class="o">=</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">initial_guess</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">5</span><span class="p">)])</span>

<span class="n">mu_fit</span><span class="p">,</span> <span class="n">sigma_fit</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">x</span>
</code></pre></div></div> <p>Fitted parameters:</p> <ul> <li>μ (log-scale mean): 4.192</li> <li>σ (log-scale SD): 0.123</li> </ul> <p>This defines the objective function for optimization:</p> <ul> <li>Takes parameters mu and sigma as input</li> <li>Returns infinity if sigma is non-positive (constraint)</li> <li>Calculates theoretical quantiles using the log-normal distribution with given parameters</li> <li>Returns the sum of squared differences between theoretical and empirical quantiles (this is what we want to minimize)</li> </ul> <p>We repeat this process for <code class="language-plaintext highlighter-rouge">Weibull</code> and <code class="language-plaintext highlighter-rouge">generalized gamma</code> as well.</p> <hr/> <h2 id="step-3-visualizing-the-simulated-distribution">Step 3: Visualizing the Simulated Distribution</h2> <p>Once we’ve fitted the distribution, we simulate 10,000 onset ages and plot:</p> <ul> <li>Left: Histogram of simulated ages</li> <li>Right: CDF overlay comparing empirical and model quantiles</li> </ul> <p>Example output for log-normal:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lognormal_dist_cdf.png" sizes="95vw"/> <img src="/assets/img/lognormal_dist_cdf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This dual-panel visualization helps assess how well the simulated distribution captures the shape and tails of the reported data.</p> <hr/> <h2 id="step-4-comparing-goodness-of-fit">Step 4: Comparing Goodness-of-Fit</h2> <p>We assess fit using the sum of squared differences between simulated and empirical quantiles:</p> <table> <thead> <tr> <th><strong>Distribution</strong></th> <th><strong>Sum of Squared Errors</strong></th> </tr> </thead> <tbody> <tr> <td>Log-normal</td> <td>0.0916</td> </tr> <tr> <td>Weibull</td> <td>0.5238</td> </tr> <tr> <td>Generalized Gamma</td> <td>0.0772 (best fit)</td> </tr> </tbody> </table> <p>Unsurprisingly, the generalized gamma distribution—known for its flexibility—performed best.</p> <hr/> <h2 id="step-5-estimating-age-band-proportions-with-bootstrap-ci">Step 5: Estimating Age-Band Proportions with Bootstrap CI</h2> <p>Let’s say we want to estimate the proportion of patients in three age bands:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">-</span> <span class="o">&lt;</span><span class="mi">12</span> <span class="n">years</span>
<span class="o">-</span> <span class="mi">12</span><span class="err">–</span><span class="mi">17</span> <span class="n">years</span>
<span class="o">-</span> <span class="err">≥</span><span class="mi">18</span> <span class="n">years</span>
</code></pre></div></div> <p>We use bootstrapping (1,000 iterations) to estimate proportions and their 95% confidence intervals:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">age_bands</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">18</span><span class="p">),</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">100</span><span class="p">)]</span>
<span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">gengamma</span>

<span class="c1"># Using best-fit generalized gamma parameters
</span><span class="n">bootstrap_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
    <span class="n">sim_ages</span> <span class="o">=</span> <span class="nf">gengamma</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mf">26.611</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">1.583</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">8.409</span><span class="p">).</span><span class="nf">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">proportions</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">sim_ages</span> <span class="o">&gt;=</span> <span class="n">low</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">sim_ages</span> <span class="o">&lt;</span> <span class="n">high</span><span class="p">))</span> <span class="nf">for </span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span> <span class="ow">in</span> <span class="n">age_bands</span><span class="p">]</span>
    <span class="n">bootstrap_results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">proportions</span><span class="p">)</span>
</code></pre></div></div> <p>Resulting summary:</p> <table> <thead> <tr> <th><strong>Age Band</strong></th> <th><strong>Mean Proportion</strong></th> <th><strong>95% CI Lower</strong></th> <th><strong>95% CI Upper</strong></th> </tr> </thead> <tbody> <tr> <td>0–12</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> </tr> <tr> <td>12–18</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> </tr> <tr> <td>18+</td> <td>0.9999</td> <td>0.9997</td> <td>1.0000</td> </tr> </tbody> </table> <p>Visualization:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/age_band_barplot.png" sizes="95vw"/> <img src="/assets/img/age_band_barplot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This clearly shows the distribution is concentrated in adult-onset.</p> <hr/> <h2 id="conclusion-and-reuse">Conclusion and Reuse</h2> <p>This method allows you to simulate realistic onset age distributions using only summary statistics. It’s particularly valuable in rare disease modeling, where raw data are scarce but precise modeling inputs are needed.</p> <p>You can adapt this notebook by:</p> <ul> <li>Swapping in new quantiles from another disease or study</li> <li>Testing additional distribution families (e.g., skew-normal)</li> <li>Extending to model time to diagnosis or treatment delay</li> </ul> <p>Whether you’re building HTA models or conducting evidence synthesis, this is a scalable, data-light, and transparent tool to add to your workflow.</p>]]></content><author><name></name></author><category term="statistics"/><category term="age-at-onset,"/><category term="distribution,"/><category term="rare-disease,"/><category term="python"/><summary type="html"><![CDATA[In rare disease research, one of the most frustrating barriers is the lack of individual-level data. While disease onset age is critical for modeling epidemiological burden and designing trials, most published studies only report summary statistics like medians or quartiles.]]></summary></entry><entry><title type="html">A Beginner’s Hands-On Guide to Meta-Analysis and Confidence Intervals</title><link href="https://davidzhao1015.github.io/blog/2025/meta-analysis-in-R/" rel="alternate" type="text/html" title="A Beginner’s Hands-On Guide to Meta-Analysis and Confidence Intervals"/><published>2025-06-16T00:00:00+00:00</published><updated>2025-06-16T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/meta-analysis-in-R</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/meta-analysis-in-R/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post serves as a <strong>hands-on tutorial for beginners</strong> who are learning how to conduct <strong>meta-analysis</strong> in the context of <strong>microbiome research</strong>. Whether you’re synthesizing findings from multiple studies or comparing gut microbiome diversity across populations, this guide is designed to help you get started with clarity and confidence.</p> <h3 id="what-youll-gain-from-this-tutorial">What You’ll Gain from This Tutorial</h3> <ul> <li><strong>Clear, minimal explanations</strong> of key concepts in fixed-effect and random-effects meta-analysis—no prior advanced statistics required</li> <li><strong>Reusable R code snippets</strong> using the well-documented meta package, making it easy to apply the methods to your own dataset</li> <li><strong>Practical guidance</strong> on choosing appropriate models, weighting strategies, and confidence interval methods, tailored to common challenges in microbiome data</li> </ul> <p>Whether you’re a graduate student, postdoc, or early-career researcher, this tutorial will help you bridge the gap between statistical theory and real-world implementation—so you can focus on the biological insights that matter.</p> <hr/> <h2 id="what-you-need-to-know-first">What You Need to Know First</h2> <h3 id="1-weighting-parametric-vs-non-parametric-approaches">1. Weighting: Parametric vs. Non-parametric Approaches</h3> <p>The goal of meta-analysis is to synthesize findings across studies to estimate a single pooled effect size (e.g., odds ratio, risk difference, mean difference).</p> <p>Since not all studies are equally informative, each study is assigned a <strong>weight</strong> that determines its contribution to the pooled estimate.</p> <p>Two common approaches to weighting:</p> <ul> <li><strong>Sample size–based weighting</strong> (non-parametric): Larger studies receive more weight based solely on their sample sizes. This approach is simpler but does not account for the variability in effect size estimates.</li> <li><strong>Inverse-variance weighting</strong> (parametric): Weights are assigned as the inverse of the variance of each study’s effect size. This method prioritizes more precise studies and is the basis for most modern meta-analytic models.</li> </ul> <h3 id="2-fixed-vs-random-effects-models-and-their-assumptions">2. Fixed vs. Random Effects Models and Their Assumptions</h3> <p>Meta-analysis models differ in how they conceptualize the true effect:</p> <ul> <li><strong>Fixed-effect model</strong> assumes all studies estimate the same true effect size. Differences between studies arise only due to sampling error.</li> <li><strong>Random-effects model</strong> assumes that each study estimates a different (yet related) true effect size. Differences reflect both <strong>within-study variance</strong> (sampling error) and <strong>between-study heterogeneity</strong> (true variation in effects).</li> </ul> <p>The fixed-effect model is appropriate when studies are highly similar (homogeneous), while the random-effects model is more flexible and widely used when heterogeneity exists.</p> <h3 id="3-between-study-heterogeneity-τ">3. Between-study Heterogeneity (τ²)</h3> <p><strong>Between-study heterogeneity</strong> reflects real differences in effect sizes due to factors like study population, intervention, or setting.</p> <p>In a random-effects model, this variability is quantified by <strong>τ² (tau-squared)</strong>.</p> <p>Several estimators are available to compute τ², including:</p> <ul> <li><strong>DerSimonian–Laird (DL)</strong> – the most commonly used.</li> <li><strong>Restricted Maximum Likelihood (REML)</strong> – more accurate in small samples.</li> <li><strong>Paule–Mandel</strong>, <strong>Empirical Bayes</strong>, etc.</li> </ul> <h3 id="4-95-confidence-interval-of-the-pooled-effect">4. 95% Confidence Interval of the Pooled Effect</h3> <p>The <strong>95% confidence interval (CI)</strong> around the pooled effect size represents the uncertainty of the estimate — that is, the range in which the true effect is expected to lie 95% of the time.</p> <p>For <strong>fixed-effect models</strong>, the CI is narrower because only sampling error is considered.</p> <p>For <strong>random-effects models</strong>, the CI is wider due to the inclusion of between-study heterogeneity (τ²).</p> <p>Methods to compute CIs include:</p> <ul> <li><strong>Normal approximation (Wald-type)</strong> – common default.</li> <li><strong>Knapp–Hartung adjustment</strong> – improves coverage in random-effects models, especially with few studies.</li> </ul> <hr/> <h2 id="step-by-step-guide">Step-by-Step Guide</h2> <p>This section introduces two common approaches to synthesizing study results in meta-analysis: the <strong>fixed-effect model</strong>and the <strong>random-effects model</strong>. A guiding question is: <em>Do we assume that all studies estimate a single true effect size?</em> If yes, use a fixed-effect model; if not, or if there is heterogeneity, use a random-effects model.</p> <h3 id="fixed-effect-model">Fixed-Effect Model</h3> <p><strong>Step 1: Calculate Effect Sizes for Individual Studies</strong></p> <p>At the first place, we calculate the effect size for each study included in the meta-analysis. Effect size quantifies the strength of a difference or relationship between variables. Common types include mean difference, standardized mean difference, odds ratio, and correlation.</p> <hr/> <p><strong>Step 2: Assign Weights</strong></p> <p>To account for how much each study contributes to the overall effect estimate, we assign <strong>weights</strong> to each study.</p> <p>There are two common approaches:</p> <p><strong>a) Sample Size–Based Weighting</strong></p> <p>Each study is weighted according to its sample size:</p> \[w_i = n_i\] <p>where \(n_i\) is the sample size of study \(i\).</p> <p>We may also use normalized weights or w̃_i:</p> \[w̃_i = n_i / ∑n_i\] <p>This approach is simple while it does not account for differences in measurement precision across studies.</p> <p><strong>b) Variance-based Weighting</strong></p> <p>A more statistically rigorous method is to weight each study inversely proportional to the <strong>variance</strong> of its effect size estimate:</p> \[w_i = 1 / v_i\] <p>where \(v_i\) is the estimated variance of the effect size \(x_i\) from study \(i\).</p> <ul> <li>This approach gives <strong>greater weight to more precise studies</strong> (those with smaller variances).</li> <li>It is the standard method used in most fixed-effect meta-analysis.</li> <li>In practice, the variance v_i is often derived from standard errors, confidence intervals, or reported summary statistics.</li> </ul> <table> <thead> <tr> <th><strong>Effect Size Type</strong></th> <th><strong>Variance Formula (</strong>\(v_i\)<strong>)</strong></th> </tr> </thead> <tbody> <tr> <td>Mean Difference (2 groups)</td> <td>\(v_i = (SD₁² / n₁) + (SD₂² / n₂)\)</td> </tr> <tr> <td>Standardized Mean Difference (SMD)</td> <td>\(v_i = (n₁ + n₂)/(n₁ × n₂) + d²/(2 × (n₁ + n₂))\)</td> </tr> <tr> <td>Single Group Mean</td> <td>\(v_i = SD² / n\)</td> </tr> </tbody> </table> <hr/> <p><strong>Step 3: Calculate the Weighted Mean Effect Size</strong></p> <p>The overall pooled effect size under the fixed-effect model is computed as:</p> \[μ* = ∑(w_i × x_i) / ∑w_i\] <p>where x_i is the effect size estimate from study i, and w_i is its assigned weight.</p> <hr/> <p><strong>Step 4: Estimate the Variance of the Weighted Mean</strong></p> <p>Assuming a fixed-effect model and using inverse-variance weights or sample-size weights, the variance of the pooled effect is estimated as:</p> \[Var(μ*) = 1 / ∑w_i\] <hr/> <p><strong>Step 5: Calculate the Standard Error</strong></p> <p>The standard error (SE) of the pooled effect size is the square root of its variance:</p> \[SE = √Var(μ*)\] <hr/> <p><strong>Step 6: Compute the 95% Confidence Interval</strong></p> <p>Using the normal (Z) distribution, the 95% confidence interval for the pooled effect size is:</p> \[μ* ± 1.96 × SE\] <p>Here, the critical value, Z-score 1.96 leaves 97.5% of the standard normal distribution (mean = 0, SD = 1) to the left of it.</p> <hr/> <h3 id="random-effects-models">Random Effects Models</h3> <p>Random-effects models account for the possibility that true effect sizes differ across studies, acknowledging heterogeneity beyond sampling error.</p> <p><strong>Step 1: Test for Heterogeneity</strong></p> <p>We calculate the <strong>Q statistic</strong> to test for heterogeneity among studies:</p> \[Q = ∑ w_i^FE × (x_i − μ*_FE)²\] <p>where w_i^FE is fixed-effect weights, μ*_FE is fixed-effect pooled mean</p> <p>A large Q indicates substantial between-study heterogeneity—meaning real differences in effect sizes may exist.</p> <p>The <a href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/heterogeneity.html#i-squared">Higgins &amp; Thompson’s I²</a>, derived from Q, is an alternative metric that quantifies the percentage of variation due to heterogeneity rather than chance.</p> <hr/> <p><strong>Step 2: Estimate the Between-Study Variance (τ²)</strong></p> <p>We now estimate how much true effects differ between studies - this variability is called between-study variance, or <strong>τ².</strong></p> <p>Several estimators are available:</p> <ul> <li><strong>DL (DerSimonian and Laird)</strong> - a widely used default method</li> <li><strong>ML (Maximum Likelihood)</strong></li> <li><strong>REML (Restricted Maximum Likelihood)</strong></li> </ul> <p>These methods differ in how they estimate uncertainty, especially in small samples.</p> <hr/> <p><strong>Step 3: Determine Random-Effects Weights</strong></p> <p>Each study is now weighted based on both within-study variance and between-study variance:</p> \[w_i = 1 / (v_i + τ²)\] <p>where v_i: within-study variance, and τ²: between-study variance</p> <p>This formula reflects a study with less precision and higher heterogeneity dataset gets less weight.</p> <hr/> <p><strong>Step 4: Calculate the Random-Effects Weighted Mean</strong></p> <p>We then pool the study effect size using a weighted average:</p> \[μ* = ∑ (w_i × x_i) / ∑ w_i\] <p>This gives a more generalizable average effect across varying study contexts.</p> <hr/> <p><strong>Step 5: Calculate the Standard Error</strong></p> <p>The uncertainty around the pooled effect size is estimated by:</p> \[SE = √(1 / ∑ w_i)\] <p>This SE will be larger than under the fixed-effect model due to added uncertainty from heterogeneity.</p> <hr/> <p><strong>Step 6: Choose a Confidence Interval Method</strong></p> <p>There are multiple options for calculating the 95% confidence interval around the pooled estimate in random-effects models:</p> <p><strong>a) z-distribution Method</strong></p> <ul> <li>Assumes a normal distribution</li> <li>Does <strong>not</strong> adjust for uncertainty in estimating τ²</li> </ul> \[CI = μ* ± 1.96 × SE\] <hr/> <p><strong>b) t-distribution Method</strong></p> <ul> <li>Uses the t-distribution with k−1 degrees of freedom</li> <li>Accounts for small-sample (n &lt; 30) uncertainty better than the z-method</li> </ul> \[CI = μ* ± t_(k−1, 1−α/2) × SE\] <hr/> <p><strong>c) Hartung-Knapp (HK) Method</strong></p> <p>This method gives wider but more reliable intervals, especially when the number of studies is small or heterogeneity is high.</p> <p>First, calculate the improved variance estimate:</p> \[Var_w(μ*) = ∑ w_i × (x_i − μ*)² / [(k − 1) × ∑ w_i]\] <p>Then, construct the confidence interval:</p> \[CI = μ* ± t_(k−1, 1−α/2) × √Var_w(μ*)\] <hr/> <h2 id="flowchart-overview">Flowchart Overview</h2> <p>The flowchart below visually summarizes the <strong>critical steps</strong> involved in conducting a meta-analysis using either a <strong>fixed-effect</strong> or a <strong>random-effects model</strong>. It is designed to help readers quickly grasp the overall workflow and key distinctions between the two approaches.</p> <ul> <li><strong>Decision points</strong> appear at the top, guiding model selection based on assumptions about study heterogeneity.</li> <li><strong>Color coding</strong> is used to clearly distinguish the <strong>fixed-effect pathway</strong> from the <strong>random-effects pathway</strong>.</li> <li><strong>Dashed lines</strong> highlight key <strong>outcome variables</strong> at each stage (e.g., pooled effect size, standard error, confidence intervals).</li> <li>Steps flow logically from calculating individual effect sizes to pooling, estimating uncertainty, and reporting the final results.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/meta_analysis_workflow_decision_tree.png" sizes="95vw"/> <img src="/assets/img/meta_analysis_workflow_decision_tree.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This diagram complements the written walkthrough by providing a high-level visual guide to the computational and statistical logic behind each model type.</p> <h2 id="easy-r-steps-for-meta-analysis">Easy R Steps for Meta-Analysis</h2> <h3 id="example-dataset">Example Dataset</h3> <p>To make the code examples more practical and relevant to microbiome researchers, this post uses a real dataset from a <strong>published meta-analysis</strong> on the impact of <strong>exclusive breastfeeding</strong> on <strong>infant gut microbiome diversity</strong>.</p> <p>The data were drawn from <strong>high-quality, peer-reviewed studies</strong>, including contributions from respected researchers such as <strong>Dr. Louise Kuhn</strong> and <strong>Dr. Anita Kozyrskyj</strong>, who kindly shared accessible summary statistics and metadata. Dr. Kozyrskyj was also my supervisor during my postdoctoral fellowship, and her support made it possible to reproduce this meta-analysis for educational purposes.</p> <p>The dataset below includes:</p> <ul> <li>Study reference</li> <li>Sample size</li> <li>Effect size (diversity difference)</li> <li>Standard error of the effect size</li> </ul> <p>Notably, the <strong>Subramanian et al. study</strong> (conducted in Bangladesh) had the <strong>largest sample size</strong> and therefore the <strong>highest precision</strong> (i.e., lowest standard error), contributing significantly to the meta-analysis.</p> <table> <thead> <tr> <th>Study</th> <th>Sample Size</th> <th>Diversity Diff.</th> <th>SE</th> </tr> </thead> <tbody> <tr> <td>Subramanian et al., 2014 (Bangladesh)</td> <td>322</td> <td>0.26</td> <td>0.0718</td> </tr> <tr> <td>Azad et al., 2015 (Canada)</td> <td>167</td> <td>0.33</td> <td>0.1583</td> </tr> <tr> <td>Bender et al., 2016 (Haiti)</td> <td>48</td> <td>-0.11</td> <td>0.3474</td> </tr> <tr> <td>Wood et al., 2018 (South Africa)</td> <td>143</td> <td>0.31</td> <td>0.2235</td> </tr> <tr> <td>Pannaraj et al., 2017 (USA(CA/FL))</td> <td>230</td> <td>0.37</td> <td>0.1492</td> </tr> <tr> <td>Sordillo et al., 2017 (USA(CA/MA/MO))</td> <td>220</td> <td>0.77</td> <td>0.1971</td> </tr> <tr> <td>Thompson et al., 2015 (USA(NC))</td> <td>21</td> <td>0.3</td> <td>0.4239</td> </tr> </tbody> </table> <hr/> <h3 id="computational-tools">Computational Tools</h3> <p>To support reproducibility and hands-on learning, this use case provides <strong>re-usable R code snippets</strong> that implement two essential steps in a typical microbiome meta-analysis:</p> <ol> <li><strong>Fixed-effect and random-effects models</strong> using inverse-variance weighting</li> <li><strong>Forest plot visualization</strong> of individual and pooled study results</li> </ol> <p>After a brief comparison of available tools, the <strong>R ecosystem</strong> was chosen over Python due to its <strong>more mature, stable, and flexible support for meta-analysis</strong>, especially in microbiome and clinical research contexts.</p> <p>The analysis was implemented using the <strong>meta R package</strong>, which is:</p> <ul> <li>Well-documented and actively maintained</li> <li>Supported by detailed tutorials and vignettes</li> <li>Beginner-friendly, yet robust enough for advanced use</li> </ul> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Perform fixed-effect and random-effect models on diversity difference using inverse variance </span><span class="w">
</span><span class="n">weightmicrobiome_sdd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">meta</span><span class="o">::</span><span class="n">metagen</span><span class="p">(</span><span class="n">TE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">diversity_diff</span><span class="p">,</span><span class="w"> </span><span class="c1"># Study effect size                                </span><span class="w">
                                    </span><span class="n">seTE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">se</span><span class="p">,</span><span class="w"> </span><span class="c1"># Standard error of effect sizes                               </span><span class="w">
                                    </span><span class="n">studlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">study</span><span class="p">,</span><span class="w"> </span><span class="c1"># Study labels                               </span><span class="w">
                                    </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">microbiome_df</span><span class="p">,</span><span class="w"> </span><span class="c1"># Data frame containing statistical information</span><span class="w">
                                    </span><span class="n">common</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="c1"># Conduct fixed-effect model meta-analysis</span><span class="w">
                                    </span><span class="n">random</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="c1"># Conduct random-effect model meta-analysis                               </span><span class="w">
                                    </span><span class="n">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="c1"># Not print prediction interval                               </span><span class="w">
                                    </span><span class="n">method.I2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Q"</span><span class="p">,</span><span class="w"> </span><span class="c1"># Method used to estimate heterogeneity statistics I^2                               </span><span class="w">
                                    </span><span class="n">method.tau</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"DL"</span><span class="p">,</span><span class="w"> </span><span class="c1"># DerSimonian-Laird estimator                               </span><span class="w">
                                    </span><span class="n">method.tau.ci</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"J"</span><span class="p">,</span><span class="w"> </span><span class="c1"># Method by Jackson (2013)                               </span><span class="w">
                                    </span><span class="n">method.random.ci</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"HK"</span><span class="w"> </span><span class="c1"># Method by Hartung and Knapp (2001a/b)                               </span><span class="w">
                                    </span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <h4 id="understanding-key-arguments-in-metametagen">Understanding key arguments in <code class="language-plaintext highlighter-rouge">meta::metagen()</code></h4> <p>To support beginners who may have only basic familiarity with meta-analysis, each function in this post includes <strong>annotated explanations of its core arguments</strong>. In particular, the R function meta::metagen()—used to compute fixed- and random-effects models—contains several <strong>powerful arguments</strong> that greatly increase its flexibility.</p> <p>Among these, four arguments stand out as especially important:</p> <ul> <li><strong>method.I2</strong> – Controls how the <strong>I² statistic</strong> (a measure of heterogeneity) is calculated.</li> <li><strong>method.tau</strong> – Specifies the method for estimating the <strong>between-study variance (τ²)</strong>, such as DerSimonian-Laird, REML, or ML.</li> <li><strong>method.tau.ci</strong> – Determines how the <strong>confidence interval for τ²</strong> is calculated.</li> <li><a href="http://method.random.ci/"><strong>method.random.ci</strong></a> – Selects the method used to calculate the <strong>confidence interval around the random-effects pooled estimate</strong> (e.g., classical vs. Hartung-Knapp).</li> </ul> <p>These arguments offer <strong>flexibility and customization</strong>, especially when tailoring the analysis to match study heterogeneity or small-sample concerns. However, <strong>intentional use requires a solid understanding</strong> of their conceptual foundations.</p> <p>The <strong>earlier sections of this post</strong> walk through these core concepts to help you build the minimal understanding needed to make informed choices.</p> <p>For a deeper dive and full list of options, consult the <a href="https://cran.r-project.org/web/packages/meta/meta.pdf">official documentation of the meta package</a>. Exploring these arguments will help you fully leverage the power of metagen() in your own microbiome meta-analyses.</p> <hr/> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">summary</span><span class="p">(</span><span class="n">microbiome_sdd</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">                          </span><span class="m">95</span><span class="o">%-CI           %</span><span class="n">W</span><span class="p">(</span><span class="n">common</span><span class="p">)</span><span class="w">  </span><span class="o">%W(random)
Subramanian et al., 2014 (Bangladesh)   0.2600 [ 0.1193; 0.4007]    57.3       40.7
Azad et al., 2015 (Canada)              0.3300 [ 0.0197; 0.6403]    11.8       15.7
Bender et al., 2016 (Haiti)            -0.1100 [-0.7909; 0.5709]     2.4        4.0
Wood et al., 2018 (South Africa)        0.3100 [-0.1281; 0.7481]     5.9        8.9
Pannaraj et al., 2017 (USA(CA/FL))      0.3700 [ 0.0776; 0.6624]    13.3       17.1
Sordillo et al., 2017 (USA(CA/MA/MO))   0.7700 [ 0.3837; 1.1563]     7.6       11.0
Thompson et al., 2015 (USA(NC))         0.3000 [-0.5308; 1.1308]     1.6        2.7

Number of studies: k = 7

                         95%-</span><span class="n">CI</span><span class="w">                   </span><span class="n">z</span><span class="o">|</span><span class="n">t</span><span class="w">     </span><span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="w">
</span><span class="n">Common</span><span class="w"> </span><span class="n">effect</span><span class="w"> </span><span class="n">model</span><span class="o">:</span><span class="w">     </span><span class="m">0.3162</span><span class="w"> </span><span class="p">[</span><span class="m">0.2097</span><span class="p">;</span><span class="w"> </span><span class="m">0.4227</span><span class="p">]</span><span class="w">   </span><span class="m">5.82</span><span class="w">   </span><span class="o">&lt;</span><span class="w"> </span><span class="m">0.0001</span><span class="w">
</span><span class="n">Random</span><span class="w"> </span><span class="n">effects</span><span class="w"> </span><span class="n">model</span><span class="o">:</span><span class="w">    </span><span class="m">0.3367</span><span class="w"> </span><span class="p">[</span><span class="m">0.1602</span><span class="p">;</span><span class="w"> </span><span class="m">0.5132</span><span class="p">]</span><span class="w">   </span><span class="m">4.67</span><span class="w">     </span><span class="m">0.0034</span><span class="w">

</span><span class="n">Quantifying</span><span class="w"> </span><span class="n">heterogeneity</span><span class="w"> </span><span class="p">(</span><span class="n">with</span><span class="w"> </span><span class="m">95</span><span class="o">%-CIs):
tau^2 = 0.0073 [0.0000; 0.2032]; tau = 0.0855 [0.0000; 0.4508]
I^2 = 20.6%</span><span class="w"> </span><span class="p">[</span><span class="m">0.0</span><span class="o">%; 64.0%</span><span class="p">];</span><span class="w"> </span><span class="n">H</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1.12</span><span class="w"> </span><span class="p">[</span><span class="m">1.00</span><span class="p">;</span><span class="w"> </span><span class="m">1.67</span><span class="p">]</span><span class="w">

</span><span class="n">Test</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">heterogeneity</span><span class="o">:</span><span class="w">
</span><span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">7.56</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.2723</span><span class="w">

</span><span class="n">Details</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">meta</span><span class="o">-</span><span class="n">analysis</span><span class="w"> </span><span class="n">methods</span><span class="o">:</span><span class="w">
</span><span class="o">-</span><span class="w"> </span><span class="n">Inverse</span><span class="w"> </span><span class="n">variance</span><span class="w"> </span><span class="n">method</span><span class="w">
</span><span class="o">-</span><span class="w"> </span><span class="n">DerSimonian</span><span class="o">-</span><span class="n">Laird</span><span class="w"> </span><span class="n">estimator</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">tau</span><span class="o">^</span><span class="m">2</span><span class="w">
</span><span class="o">-</span><span class="w"> </span><span class="n">Jackson</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">CI</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">tau</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">tau</span><span class="w">
</span><span class="o">-</span><span class="w"> </span><span class="n">I</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="n">calculation</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Q</span><span class="w">
</span><span class="o">-</span><span class="w"> </span><span class="n">Hartung</span><span class="o">-</span><span class="n">Knapp</span><span class="w"> </span><span class="n">adjustment</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">random</span><span class="w"> </span><span class="n">effects</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="p">(</span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <h3 id="draw-forest-plot">Draw forest plot</h3> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">png</span><span class="p">(</span><span class="n">file</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"forestplot.png"</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1200</span><span class="p">)</span><span class="w">

</span><span class="n">meta</span><span class="o">::</span><span class="n">forest</span><span class="p">(</span><span class="n">microbiome_sdd</span><span class="p">,</span><span class="w">             
            </span><span class="n">sortvar</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TE</span><span class="p">,</span><span class="w"> </span><span class="c1"># Sort study by effect sizes in increasing order             </span><span class="w">
            </span><span class="n">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="c1"># Not show prediction interval             </span><span class="w">
            </span><span class="n">leftcols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"study"</span><span class="p">,</span><span class="w"> </span><span class="s2">"sample_size"</span><span class="p">,</span><span class="w"> </span><span class="s2">"diversity_diff"</span><span class="p">,</span><span class="w"> </span><span class="s2">"se"</span><span class="p">),</span><span class="w">             
            </span><span class="n">leftlabs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"Study"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Sample Size"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Diversity Diff."</span><span class="p">,</span><span class="w"> </span><span class="s2">"SE"</span><span class="p">),</span><span class="w">             
            </span><span class="n">print.tau2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span><span class="n">dev.off</span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/forestplot.png" sizes="95vw"/> <img src="/assets/img/forestplot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="interpret-the-forest-plot">Interpret the Forest Plot</h4> <p>This forest plot compares the results of seven studies evaluating <strong>gut microbiome diversity difference (DD)</strong> between groups. It shows both individual study results and pooled estimates under fixed-effect and random-effects models.</p> <p><strong>1. Study Data (Left Side)</strong></p> <ul> <li>Each row corresponds to a single study included in the meta-analysis.</li> <li><strong>DD</strong> stands for <strong>diversity difference</strong>, which is the effect size used in this analysis.</li> <li><strong>SE</strong> stands for <strong>standard error</strong>, which reflects the precision of the effect size estimate. Smaller SE means higher precision.</li> </ul> <p><strong>2. Confidence Intervals and Visual Elements (Middle Area)</strong></p> <ul> <li><strong>Horizontal lines</strong> show the <strong>95% confidence interval (CI)</strong> for each study’s effect size.</li> <li><strong>Squares</strong> represent the <strong>point estimate</strong> of each study. Larger squares indicate <strong>greater weight</strong> in the analysis.</li> <li>The <strong>vertical solid line</strong> at 0 is the <strong>line of no effect</strong>—values to the right suggest a positive effect.</li> <li><strong>Diamond shapes</strong> summarize the <strong>pooled effect estimates</strong>: <ul> <li>The gray diamond: fixed-effect (common-effect) model</li> <li>The white diamond: random-effects model</li> </ul> </li> <li>The <strong>red horizontal bar</strong> under the diamond is the <strong>prediction interval</strong>, indicating the likely range of effect sizes in <strong>future studies</strong>.</li> </ul> <p><strong>3. Result Summary (Right Side)</strong></p> <ul> <li>The <strong>95% CI</strong> for each study shows the uncertainty around its effect estimate.</li> <li>The columns <strong>Weight (common)</strong> and <strong>Weight (random)</strong> show how much each study contributes to the respective model. <ul> <li>In the <strong>fixed-effect model</strong>, Study 1 carries <strong>57.3%</strong> of the weight due to its very low SE (high precision).</li> <li>In the <strong>random-effects model</strong>, weights are more evenly distributed because the model also accounts for <strong>between-study heterogeneity</strong> (τ²).</li> </ul> </li> </ul> <p><strong>4. Heterogeneity (Bottom Statistics)</strong></p> <ul> <li><strong>I² = 20.6%</strong>: Indicates <strong>low to moderate heterogeneity</strong>—i.e., some variation in effect sizes across studies, but not extreme.</li> <li><strong>p = 0.2723</strong>: The test for heterogeneity is <strong>not statistically significant</strong>, meaning we <strong>cannot reject</strong> the idea that all studies may share one common effect size.</li> </ul> <p><strong>Summary Interpretation</strong></p> <ul> <li>The pooled effect sizes from the <strong>fixed-effect model (0.32)</strong> and the <strong>random-effects model (0.33)</strong> are very similar, which suggests that the result is <strong>robust and stable</strong>.</li> <li>Since mild heterogeneity is present, the <strong>random-effects model is more appropriate</strong> for generalization.</li> <li>The <strong>prediction interval [0.09, 0.57]</strong> suggests that <strong>future studies</strong> are still likely to show a <strong>positive effect</strong>, though the effect size may vary in strength.</li> </ul> <hr/> <h2 id="practical-recommendations">Practical Recommendations</h2> <p><strong>Fixed-Effects Models:</strong> Use the standard <em>z-</em>distribution method with size-weighted means when the number of studies is large.</p> <p><strong>Random-Effects Models:</strong> Prefer the <strong>weighted variance confidence interval, such as HK,</strong> method for better coverage and less sensitive to which estimator used for τ².</p> <p><strong>Sample Size Weighting</strong> is appropriate when:</p> <ul> <li>Variance estimates are unavailable or unreliable</li> <li>Interpretability is a priority</li> <li>Study precisions are comparable</li> </ul> <p><strong>Best Practice:</strong> Report both <strong>parametric</strong> and <strong>nonparametric</strong> weighted results to test the robustness of conclusions.</p> <hr/> <h2 id="references">References</h2> <ul> <li>Perplexity research <a href="https://www.perplexity.ai/search/introduce-step-to-step-guide-t-RFYI0pQZRvmm.9bK664ABg">report</a></li> <li>Doing meta-analysis in R <a href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/">Chapter 3-6</a></li> </ul>]]></content><author><name></name></author><category term="statistics"/><category term="meta-analysis,"/><category term="R,"/><category term="microbiome"/><summary type="html"><![CDATA[Introduction This post serves as a hands-on tutorial for beginners who are learning how to conduct meta-analysis in the context of microbiome research. Whether you’re synthesizing findings from multiple studies or comparing gut microbiome diversity across populations, this guide is designed to help you get started with clarity and confidence.]]></summary></entry></feed>