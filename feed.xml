<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://davidzhao1015.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://davidzhao1015.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-09T23:30:44+00:00</updated><id>https://davidzhao1015.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">How to Retrieve full taxonomy lineage based on a certain rank from NCBI taxonomy database using Python API</title><link href="https://davidzhao1015.github.io/blog/2025/retrieve-tax-lineage-API/" rel="alternate" type="text/html" title="How to Retrieve full taxonomy lineage based on a certain rank from NCBI taxonomy database using Python API"/><published>2025-08-09T00:00:00+00:00</published><updated>2025-08-09T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/retrieve-tax-lineage-API</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/retrieve-tax-lineage-API/"><![CDATA[<h2 id="problemaim">Problem/Aim</h2> <p>In microbiome data analysis, taxonomy assignments—derived from marker genes or whole genomes—are essential for understanding microbial ecosystems when paired with abundance data. These assignments are not limited to single taxon names but are structured as hierarchical lineages across multiple taxonomic ranks, offering richer biological insights. However, researchers often only have partial taxonomy data (e.g., names at a single rank), which limits interpretability and analytical depth.</p> <h2 id="solution">Solution</h2> <p>To unlock the full potential of taxonomy-based insights, incomplete taxon names can be mapped to their complete hierarchical lineages by retrieving standardized taxonomy data from the NCBI database. This approach enriches the dataset and supports more robust biological interpretation.</p> <h2 id="basic-concepts">Basic concepts</h2> <p>Bacteria taxonomy lineage</p> <p>NCBI taxonomy database</p> <p>NCBI E-utilities API</p> <h2 id="solution-implementation"><strong>Solution Implementation</strong></h2> <p>To retrieve full taxonomy lineages, one can use either the NCBI Taxonomy web browser or a programmatic method. The programmatic approach is highly efficient and advantageous, allowing automation of repetitive retrieval for multiple taxa at once. The NCBI E-utilities Python API enables access to the web server directly from Python scripts, integrating seamlessly with other steps in a microbiome data analysis pipeline.</p> <p>This section includes two parts:</p> <ol> <li>A reusable function to download and parse the full taxonomy lineage for a given taxon at a single rank.</li> <li>An example demonstrating how to apply this function to multiple taxa, using a food fermentation microbiome dataset as a case study.</li> </ol> <h3 id="reusable-script"><strong>Reusable Script</strong></h3> <p>The customizable function below retrieves the full lineage using the Bio.Entrez.esearch(), Bio.Entrez.efetch(), and Bio.Entrez.read() functions from the <strong>Bio.Entrez</strong> submodule in the Biopython package.</p> <p>The input is a taxon string at a given rank, and the output is a list of taxa from higher to lower ranks. You can use this function as is or adapt it for your specific needs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">Bio</span> <span class="kn">import</span> <span class="n">Entrez</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">os</span>


<span class="k">def</span> <span class="nf">get_taxonomy_lineage</span><span class="p">(</span><span class="n">taxon_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">rank</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span> <span class="o">|</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Fetches complete taxonomy lineage based on the known taxonomic level (the highest resolution available) from the NCBI taxonomy database.

    Args:
        rank (str): The taxonomic rank to search for (default is </span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="s">). </span><span class="sh">"</span><span class="s">Family</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">Order</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">Class</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">Phylum</span><span class="sh">"</span><span class="s">, or </span><span class="sh">"</span><span class="s">Kingdom</span><span class="sh">"</span><span class="s"> can also be used.
        taxon_name (str): The name of the taxon to search for. If None, the function will not perform a search.

    Returns:
        list: A list containing the lineage of the taxon, or None if not found.
        The lineage includes domain, kingdom (or clade), phylum, class, order, and family.
        If the taxon is not found, it returns None.
        If the rank is invalid, it returns None with an error message.
        If the taxon_name is None, it returns None with an error message.
    </span><span class="sh">"""</span>
    
    <span class="c1">#--- Input validation ---
</span>    <span class="c1"># Check if taxon_name is provided
</span>    <span class="k">if</span> <span class="n">taxon_name</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Taxon name is None. Please provide a valid taxon name.</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="c1"># Check if rank is valid
</span>    <span class="k">if</span> <span class="n">rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Family</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Order</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Class</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Phylum</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Kingdom</span><span class="sh">"</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Invalid rank: </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">. Please use one of the following ranks: Genus, Family, Order, Class, Phylum, Kingdom.</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>


    <span class="c1">#--- Set the email for NCBI Entrez ---
</span>    <span class="c1"># This is required by NCBI to track usage and for contact in case of issues
</span>    <span class="c1"># Replace with your email address
</span>    <span class="n">Entrez</span><span class="p">.</span><span class="n">email</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">NCBI_EMAIL</span><span class="sh">"</span><span class="p">)</span>


    <span class="c1">#--- Search for the genus in the NCBI taxonomy database ---
</span>    <span class="c1"># The search term is formatted to include the genus name followed by "[Genus]" to specify the search field
</span>    <span class="c1"># This ensures that the search is limited to the genus level in the taxonomy database
</span>    <span class="n">handle</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">esearch</span><span class="p">(</span><span class="n">db</span><span class="o">=</span><span class="sh">"</span><span class="s">taxonomy</span><span class="sh">"</span><span class="p">,</span> <span class="n">term</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">taxon_name</span><span class="si">}</span><span class="s">[</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">]</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">record</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">read</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span> <span class="c1"># Read the search results
</span>    <span class="n">handle</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span> <span class="c1"># Close the handle to free resources
</span>

    <span class="c1">#--- Check if any IDs were found for the genus ---
</span>    <span class="c1"># If no IDs are found, print a message and return None
</span>    <span class="c1"># This is important to handle cases where the genus does not exist in the database
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">record</span><span class="p">[</span><span class="sh">"</span><span class="s">IdList</span><span class="sh">"</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">No taxonomy ID found for </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">taxon_name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> 
        <span class="k">return</span> <span class="bp">None</span>


    <span class="c1">#--- Fetch the taxonomy record using the first ID found ---
</span>    <span class="c1"># The first ID in the IdList is used to fetch the complete taxonomy record
</span>    <span class="c1"># This is because the search may return multiple IDs, but we are interested in the first one
</span>    <span class="c1"># The efetch function retrieves the record in XML format for easier parsing
</span>    <span class="c1"># The record contains detailed information about the taxonomy, including lineage
</span>    <span class="c1"># The lineage includes domain, kingdom (or clade), phylum, class, order, and family
</span>    <span class="n">taxid</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="sh">"</span><span class="s">IdList</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Get the first taxonomy ID from the search results
</span>    <span class="n">handle</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">efetch</span><span class="p">(</span><span class="n">db</span><span class="o">=</span><span class="sh">"</span><span class="s">taxonomy</span><span class="sh">"</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">taxid</span><span class="p">,</span> <span class="n">retmode</span><span class="o">=</span><span class="sh">"</span><span class="s">xml</span><span class="sh">"</span><span class="p">)</span> 
    <span class="n">records</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">read</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span> <span class="c1"># Read the fetched record
</span>    <span class="n">handle</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span> <span class="c1"># Close the handle to free resources
</span>
    <span class="c1">#--- Extract the lineage from the fetched record ---
</span>    <span class="c1"># The lineage is extracted from the first record in the list of records returned by efetch
</span>    <span class="c1"># The lineage is a string that includes the complete taxonomy hierarchy for the genus
</span>    <span class="c1"># It is formatted as "domain; kingdom; phylum; class; order; family"
</span>    <span class="n">lineage</span> <span class="o">=</span> <span class="n">records</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">Lineage</span><span class="sh">"</span><span class="p">]</span> <span class="c1"># Extract the lineage from the record, including domain, kingdom (or clade), phylum, class, order, and family
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">lineage</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">lineage_list</span> <span class="o">=</span> <span class="n">lineage</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">; </span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Split the lineage into a list
</span>    
    <span class="k">return</span> <span class="n">lineage_list</span>
</code></pre></div></div> <h3 id="example"><strong>Example</strong></h3> <p>The example demonstrates applying the function to multiple bacterial taxa using loop iteration in Python, maximizing the efficiency of the Python API. You can adapt the code by replacing it with your own input data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define a list of target genera: Fermenting bacteria
</span><span class="n">genera</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Acetobacter</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Gluconacetobacter</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lentibacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Brevibacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Erwinia</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Enterobacter</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Pantoea</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Kosakonia</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Companilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Schleiferilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Ligilactobacillus</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Lactiplantibacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Loigolactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Paucilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Limosilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Fructilactobacillus</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Acetilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Secundilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lentilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Carnobacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Weissella</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Oenococcus</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Enterococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Tetragenococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Streptococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lactococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Pediococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Periweissella</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Leuconostoc</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Marinilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Alkalibacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Eggerthella</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Propionibacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Staphylococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Kocuria</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize an empty DataFrame to store all lineages
</span><span class="n">df_all_lineages</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">()</span>

<span class="c1"># Loop through each genus and get the taxonomy lineage
</span><span class="k">for</span> <span class="n">genus</span> <span class="ow">in</span> <span class="n">genera</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Processing genus: </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">lineage</span> <span class="o">=</span> <span class="nf">get_taxonomy_lineage</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">,</span> <span class="n">taxon_name</span><span class="o">=</span><span class="n">genus</span><span class="p">)</span>
    
    <span class="c1"># Skip if lineage is None or does not belong to Bacteria
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">lineage</span> <span class="ow">or</span> <span class="n">lineage</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">Bacteria</span><span class="sh">"</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Skipping genus: </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="s"> (No lineage or not Bacteria)</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">continue</span>

    <span class="c1"># Create a DataFrame for the current genus
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="n">df_genus_lineage</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">([</span><span class="n">lineage</span><span class="p">[</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">Domain</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Kingdom</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Phylum</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Class</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Order</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Family</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">df_genus_lineage</span><span class="p">[</span><span class="sh">'</span><span class="s">Genus</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">genus</span>
        
        <span class="c1"># Append to the main DataFrame
</span>        <span class="n">df_all_lineages</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df_all_lineages</span><span class="p">,</span> <span class="n">df_genus_lineage</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error processing genus </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>The first rows of the output data table is as follows.</p> <h3 id="reference">Reference</h3> <p><a href="https://www.ncbi.nlm.nih.gov/books/NBK53758/">NCBI taxonomy database help document</a></p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="taxonomy,"/><category term="microbiome,"/><category term="biopython,"/><category term="ncbi-api"/><summary type="html"><![CDATA[Problem/Aim]]></summary></entry><entry><title type="html">How to Reconstruct Disease Onset Age Distributions Using Summary Statistics: A Rare Disease Use Case in Python</title><link href="https://davidzhao1015.github.io/blog/2025/reconstruct-age-distribution/" rel="alternate" type="text/html" title="How to Reconstruct Disease Onset Age Distributions Using Summary Statistics: A Rare Disease Use Case in Python"/><published>2025-07-09T00:00:00+00:00</published><updated>2025-07-09T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/reconstruct-age-distribution</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/reconstruct-age-distribution/"><![CDATA[<p>In rare disease research, one of the most frustrating barriers is the lack of individual-level data. While disease onset age is critical for modeling epidemiological burden and designing trials, most published studies only report summary statistics like medians or quartiles.</p> <p>But what if you could reconstruct a full onset age distribution from just these summary stats?</p> <p>In this post, I’ll walk you through how to simulate granular age-at-onset profiles using Python. We’ll use anti-GABABR autoimmune encephalitis (AIE) as a real-world example and fit three candidate distributions—log-normal, Weibull, and generalized gamma—using a technique called quantile matching.</p> <p>You’ll learn how to:</p> <ul> <li>Fit distributions to summary statistics using constrained optimization</li> <li>Simulate individual-level onset ages</li> <li>Compare model fit across distributions</li> <li>Estimate age-band proportions with bootstrap confidence intervals</li> </ul> <p>Whether you’re working on a burden-of-disease model, HTA submission, or just exploring rare disease analytics, this post will give you a reproducible template to start from.</p> <p>The reusable Python notebook is available at <a href="https://github.com/davidzhao1015/rebuild-age-distribution/blob/main/onset-age-dist-construction-AIE.ipynb">here</a></p> <hr/> <h2 id="step-1-background-and-input-data">Step 1: Background and Input Data</h2> <p>Our example comes from <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11087025/">Lamblin et al. (2024)</a>, who reported onset age statistics for anti-GABABR AIE:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">median</span> <span class="o">=</span> <span class="mi">66</span>
<span class="n">q1</span> <span class="o">=</span> <span class="mi">61</span> 
<span class="n">q3</span> <span class="o">=</span> <span class="mi">72</span>
<span class="nb">min</span> <span class="o">=</span> <span class="mi">19</span>
<span class="nb">max</span> <span class="o">=</span> <span class="mi">88</span>
<span class="n">mean</span> <span class="o">=</span> <span class="mi">67</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">111</span>
</code></pre></div></div> <p>For our modeling, we’ll focus on the three quantiles (Q1, median, Q3).</p> <hr/> <h2 id="step-2-quantile-matching-to-fit-distributions">Step 2: Quantile Matching to Fit Distributions</h2> <p>We’ll use constrained optimization to find the best-fitting parameters such that theoretical quantiles match the reported ones.</p> <p>Here’s the objective function for a log-normal distribution:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">lognorm</span>
<span class="kn">from</span> <span class="n">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Empirical quantiles
</span><span class="n">empirical_q</span> <span class="o">=</span> <span class="p">[</span><span class="mi">61</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">72</span><span class="p">]</span>  

<span class="c1"># Objective function: Minimize squared differences between model and empirical quantiles
</span><span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">if</span> <span class="n">sigma</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="nf">lognorm</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span>
    <span class="n">theo_q</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">ppf</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">theo_q</span><span class="p">)</span> <span class="o">-</span> <span class="n">empirical_q</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Initial guess for mu and sigma
</span><span class="n">initial_guess</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">66</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">]</span>

<span class="c1"># Optimize
</span><span class="n">result</span> <span class="o">=</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">initial_guess</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">5</span><span class="p">)])</span>

<span class="n">mu_fit</span><span class="p">,</span> <span class="n">sigma_fit</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">x</span>
</code></pre></div></div> <p>Fitted parameters:</p> <ul> <li>μ (log-scale mean): 4.192</li> <li>σ (log-scale SD): 0.123</li> </ul> <p>This defines the objective function for optimization:</p> <ul> <li>Takes parameters mu and sigma as input</li> <li>Returns infinity if sigma is non-positive (constraint)</li> <li>Calculates theoretical quantiles using the log-normal distribution with given parameters</li> <li>Returns the sum of squared differences between theoretical and empirical quantiles (this is what we want to minimize)</li> </ul> <p>We repeat this process for <code class="language-plaintext highlighter-rouge">Weibull</code> and <code class="language-plaintext highlighter-rouge">generalized gamma</code> as well.</p> <hr/> <h2 id="step-3-visualizing-the-simulated-distribution">Step 3: Visualizing the Simulated Distribution</h2> <p>Once we’ve fitted the distribution, we simulate 10,000 onset ages and plot:</p> <ul> <li>Left: Histogram of simulated ages</li> <li>Right: CDF overlay comparing empirical and model quantiles</li> </ul> <p>Example output for log-normal:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lognormal_dist_cdf.png" sizes="95vw"/> <img src="/assets/img/lognormal_dist_cdf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This dual-panel visualization helps assess how well the simulated distribution captures the shape and tails of the reported data.</p> <hr/> <h2 id="step-4-comparing-goodness-of-fit">Step 4: Comparing Goodness-of-Fit</h2> <p>We assess fit using the sum of squared differences between simulated and empirical quantiles:</p> <table> <thead> <tr> <th><strong>Distribution</strong></th> <th><strong>Sum of Squared Errors</strong></th> </tr> </thead> <tbody> <tr> <td>Log-normal</td> <td>0.0916</td> </tr> <tr> <td>Weibull</td> <td>0.5238</td> </tr> <tr> <td>Generalized Gamma</td> <td>0.0772 (best fit)</td> </tr> </tbody> </table> <p>Unsurprisingly, the generalized gamma distribution—known for its flexibility—performed best.</p> <hr/> <h2 id="step-5-estimating-age-band-proportions-with-bootstrap-ci">Step 5: Estimating Age-Band Proportions with Bootstrap CI</h2> <p>Let’s say we want to estimate the proportion of patients in three age bands:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">-</span> <span class="o">&lt;</span><span class="mi">12</span> <span class="n">years</span>
<span class="o">-</span> <span class="mi">12</span><span class="err">–</span><span class="mi">17</span> <span class="n">years</span>
<span class="o">-</span> <span class="err">≥</span><span class="mi">18</span> <span class="n">years</span>
</code></pre></div></div> <p>We use bootstrapping (1,000 iterations) to estimate proportions and their 95% confidence intervals:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">age_bands</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">18</span><span class="p">),</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">100</span><span class="p">)]</span>
<span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">gengamma</span>

<span class="c1"># Using best-fit generalized gamma parameters
</span><span class="n">bootstrap_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
    <span class="n">sim_ages</span> <span class="o">=</span> <span class="nf">gengamma</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mf">26.611</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">1.583</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">8.409</span><span class="p">).</span><span class="nf">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">proportions</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">sim_ages</span> <span class="o">&gt;=</span> <span class="n">low</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">sim_ages</span> <span class="o">&lt;</span> <span class="n">high</span><span class="p">))</span> <span class="nf">for </span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span> <span class="ow">in</span> <span class="n">age_bands</span><span class="p">]</span>
    <span class="n">bootstrap_results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">proportions</span><span class="p">)</span>
</code></pre></div></div> <p>Resulting summary:</p> <table> <thead> <tr> <th><strong>Age Band</strong></th> <th><strong>Mean Proportion</strong></th> <th><strong>95% CI Lower</strong></th> <th><strong>95% CI Upper</strong></th> </tr> </thead> <tbody> <tr> <td>0–12</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> </tr> <tr> <td>12–18</td> <td>0.0000</td> <td>0.0000</td> <td>0.0000</td> </tr> <tr> <td>18+</td> <td>0.9999</td> <td>0.9997</td> <td>1.0000</td> </tr> </tbody> </table> <p>Visualization:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/age_band_barplot.png" sizes="95vw"/> <img src="/assets/img/age_band_barplot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This clearly shows the distribution is concentrated in adult-onset.</p> <hr/> <h2 id="conclusion-and-reuse">Conclusion and Reuse</h2> <p>This method allows you to simulate realistic onset age distributions using only summary statistics. It’s particularly valuable in rare disease modeling, where raw data are scarce but precise modeling inputs are needed.</p> <p>You can adapt this notebook by:</p> <ul> <li>Swapping in new quantiles from another disease or study</li> <li>Testing additional distribution families (e.g., skew-normal)</li> <li>Extending to model time to diagnosis or treatment delay</li> </ul> <p>Whether you’re building HTA models or conducting evidence synthesis, this is a scalable, data-light, and transparent tool to add to your workflow.</p>]]></content><author><name></name></author><category term="statistics"/><category term="age-at-onset,"/><category term="distribution,"/><category term="rare-disease,"/><category term="python"/><summary type="html"><![CDATA[In rare disease research, one of the most frustrating barriers is the lack of individual-level data. While disease onset age is critical for modeling epidemiological burden and designing trials, most published studies only report summary statistics like medians or quartiles.]]></summary></entry><entry><title type="html">A Beginner’s Hands-On Guide to Meta-Analysis and Confidence Intervals</title><link href="https://davidzhao1015.github.io/blog/2025/meta-analysis-in-R/" rel="alternate" type="text/html" title="A Beginner’s Hands-On Guide to Meta-Analysis and Confidence Intervals"/><published>2025-06-16T00:00:00+00:00</published><updated>2025-06-16T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/meta-analysis-in-R</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/meta-analysis-in-R/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This post serves as a <strong>hands-on tutorial for beginners</strong> who are learning how to conduct <strong>meta-analysis</strong> in the context of <strong>microbiome research</strong>. Whether you’re synthesizing findings from multiple studies or comparing gut microbiome diversity across populations, this guide is designed to help you get started with clarity and confidence.</p> <h3 id="what-youll-gain-from-this-tutorial">What You’ll Gain from This Tutorial</h3> <ul> <li><strong>Clear, minimal explanations</strong> of key concepts in fixed-effect and random-effects meta-analysis—no prior advanced statistics required</li> <li><strong>Reusable R code snippets</strong> using the well-documented meta package, making it easy to apply the methods to your own dataset</li> <li><strong>Practical guidance</strong> on choosing appropriate models, weighting strategies, and confidence interval methods, tailored to common challenges in microbiome data</li> </ul> <p>Whether you’re a graduate student, postdoc, or early-career researcher, this tutorial will help you bridge the gap between statistical theory and real-world implementation—so you can focus on the biological insights that matter.</p> <hr/> <h2 id="what-you-need-to-know-first">What You Need to Know First</h2> <h3 id="1-weighting-parametric-vs-non-parametric-approaches">1. Weighting: Parametric vs. Non-parametric Approaches</h3> <p>The goal of meta-analysis is to synthesize findings across studies to estimate a single pooled effect size (e.g., odds ratio, risk difference, mean difference).</p> <p>Since not all studies are equally informative, each study is assigned a <strong>weight</strong> that determines its contribution to the pooled estimate.</p> <p>Two common approaches to weighting:</p> <ul> <li><strong>Sample size–based weighting</strong> (non-parametric): Larger studies receive more weight based solely on their sample sizes. This approach is simpler but does not account for the variability in effect size estimates.</li> <li><strong>Inverse-variance weighting</strong> (parametric): Weights are assigned as the inverse of the variance of each study’s effect size. This method prioritizes more precise studies and is the basis for most modern meta-analytic models.</li> </ul> <h3 id="2-fixed-vs-random-effects-models-and-their-assumptions">2. Fixed vs. Random Effects Models and Their Assumptions</h3> <p>Meta-analysis models differ in how they conceptualize the true effect:</p> <ul> <li><strong>Fixed-effect model</strong> assumes all studies estimate the same true effect size. Differences between studies arise only due to sampling error.</li> <li><strong>Random-effects model</strong> assumes that each study estimates a different (yet related) true effect size. Differences reflect both <strong>within-study variance</strong> (sampling error) and <strong>between-study heterogeneity</strong> (true variation in effects).</li> </ul> <p>The fixed-effect model is appropriate when studies are highly similar (homogeneous), while the random-effects model is more flexible and widely used when heterogeneity exists.</p> <h3 id="3-between-study-heterogeneity-τ">3. Between-study Heterogeneity (τ²)</h3> <p><strong>Between-study heterogeneity</strong> reflects real differences in effect sizes due to factors like study population, intervention, or setting.</p> <p>In a random-effects model, this variability is quantified by <strong>τ² (tau-squared)</strong>.</p> <p>Several estimators are available to compute τ², including:</p> <ul> <li><strong>DerSimonian–Laird (DL)</strong> – the most commonly used.</li> <li><strong>Restricted Maximum Likelihood (REML)</strong> – more accurate in small samples.</li> <li><strong>Paule–Mandel</strong>, <strong>Empirical Bayes</strong>, etc.</li> </ul> <h3 id="4-95-confidence-interval-of-the-pooled-effect">4. 95% Confidence Interval of the Pooled Effect</h3> <p>The <strong>95% confidence interval (CI)</strong> around the pooled effect size represents the uncertainty of the estimate — that is, the range in which the true effect is expected to lie 95% of the time.</p> <p>For <strong>fixed-effect models</strong>, the CI is narrower because only sampling error is considered.</p> <p>For <strong>random-effects models</strong>, the CI is wider due to the inclusion of between-study heterogeneity (τ²).</p> <p>Methods to compute CIs include:</p> <ul> <li><strong>Normal approximation (Wald-type)</strong> – common default.</li> <li><strong>Knapp–Hartung adjustment</strong> – improves coverage in random-effects models, especially with few studies.</li> </ul> <hr/> <h2 id="step-by-step-guide">Step-by-Step Guide</h2> <p>This section introduces two common approaches to synthesizing study results in meta-analysis: the <strong>fixed-effect model</strong>and the <strong>random-effects model</strong>. A guiding question is: <em>Do we assume that all studies estimate a single true effect size?</em> If yes, use a fixed-effect model; if not, or if there is heterogeneity, use a random-effects model.</p> <h3 id="fixed-effect-model">Fixed-Effect Model</h3> <p><strong>Step 1: Calculate Effect Sizes for Individual Studies</strong></p> <p>At the first place, we calculate the effect size for each study included in the meta-analysis. Effect size quantifies the strength of a difference or relationship between variables. Common types include mean difference, standardized mean difference, odds ratio, and correlation.</p> <hr/> <p><strong>Step 2: Assign Weights</strong></p> <p>To account for how much each study contributes to the overall effect estimate, we assign <strong>weights</strong> to each study.</p> <p>There are two common approaches:</p> <p><strong>a) Sample Size–Based Weighting</strong></p> <p>Each study is weighted according to its sample size:</p> \[w_i = n_i\] <p>where \(n_i\) is the sample size of study \(i\).</p> <p>We may also use normalized weights or w̃_i:</p> \[w̃_i = n_i / ∑n_i\] <p>This approach is simple while it does not account for differences in measurement precision across studies.</p> <p><strong>b) Variance-based Weighting</strong></p> <p>A more statistically rigorous method is to weight each study inversely proportional to the <strong>variance</strong> of its effect size estimate:</p> \[w_i = 1 / v_i\] <p>where \(v_i\) is the estimated variance of the effect size \(x_i\) from study \(i\).</p> <ul> <li>This approach gives <strong>greater weight to more precise studies</strong> (those with smaller variances).</li> <li>It is the standard method used in most fixed-effect meta-analysis.</li> <li>In practice, the variance v_i is often derived from standard errors, confidence intervals, or reported summary statistics.</li> </ul> <table> <thead> <tr> <th><strong>Effect Size Type</strong></th> <th><strong>Variance Formula (</strong>\(v_i\)<strong>)</strong></th> </tr> </thead> <tbody> <tr> <td>Mean Difference (2 groups)</td> <td>\(v_i = (SD₁² / n₁) + (SD₂² / n₂)\)</td> </tr> <tr> <td>Standardized Mean Difference (SMD)</td> <td>\(v_i = (n₁ + n₂)/(n₁ × n₂) + d²/(2 × (n₁ + n₂))\)</td> </tr> <tr> <td>Single Group Mean</td> <td>\(v_i = SD² / n\)</td> </tr> </tbody> </table> <hr/> <p><strong>Step 3: Calculate the Weighted Mean Effect Size</strong></p> <p>The overall pooled effect size under the fixed-effect model is computed as:</p> \[μ* = ∑(w_i × x_i) / ∑w_i\] <p>where x_i is the effect size estimate from study i, and w_i is its assigned weight.</p> <hr/> <p><strong>Step 4: Estimate the Variance of the Weighted Mean</strong></p> <p>Assuming a fixed-effect model and using inverse-variance weights or sample-size weights, the variance of the pooled effect is estimated as:</p> \[Var(μ*) = 1 / ∑w_i\] <hr/> <p><strong>Step 5: Calculate the Standard Error</strong></p> <p>The standard error (SE) of the pooled effect size is the square root of its variance:</p> \[SE = √Var(μ*)\] <hr/> <p><strong>Step 6: Compute the 95% Confidence Interval</strong></p> <p>Using the normal (Z) distribution, the 95% confidence interval for the pooled effect size is:</p> \[μ* ± 1.96 × SE\] <p>Here, the critical value, Z-score 1.96 leaves 97.5% of the standard normal distribution (mean = 0, SD = 1) to the left of it.</p> <hr/> <h3 id="random-effects-models">Random Effects Models</h3> <p>Random-effects models account for the possibility that true effect sizes differ across studies, acknowledging heterogeneity beyond sampling error.</p> <p><strong>Step 1: Test for Heterogeneity</strong></p> <p>We calculate the <strong>Q statistic</strong> to test for heterogeneity among studies:</p> \[Q = ∑ w_i^FE × (x_i − μ*_FE)²\] <p>where w_i^FE is fixed-effect weights, μ*_FE is fixed-effect pooled mean</p> <p>A large Q indicates substantial between-study heterogeneity—meaning real differences in effect sizes may exist.</p> <p>The <a href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/heterogeneity.html#i-squared">Higgins &amp; Thompson’s I²</a>, derived from Q, is an alternative metric that quantifies the percentage of variation due to heterogeneity rather than chance.</p> <hr/> <p><strong>Step 2: Estimate the Between-Study Variance (τ²)</strong></p> <p>We now estimate how much true effects differ between studies - this variability is called between-study variance, or <strong>τ².</strong></p> <p>Several estimators are available:</p> <ul> <li><strong>DL (DerSimonian and Laird)</strong> - a widely used default method</li> <li><strong>ML (Maximum Likelihood)</strong></li> <li><strong>REML (Restricted Maximum Likelihood)</strong></li> </ul> <p>These methods differ in how they estimate uncertainty, especially in small samples.</p> <hr/> <p><strong>Step 3: Determine Random-Effects Weights</strong></p> <p>Each study is now weighted based on both within-study variance and between-study variance:</p> \[w_i = 1 / (v_i + τ²)\] <p>where v_i: within-study variance, and τ²: between-study variance</p> <p>This formula reflects a study with less precision and higher heterogeneity dataset gets less weight.</p> <hr/> <p><strong>Step 4: Calculate the Random-Effects Weighted Mean</strong></p> <p>We then pool the study effect size using a weighted average:</p> \[μ* = ∑ (w_i × x_i) / ∑ w_i\] <p>This gives a more generalizable average effect across varying study contexts.</p> <hr/> <p><strong>Step 5: Calculate the Standard Error</strong></p> <p>The uncertainty around the pooled effect size is estimated by:</p> \[SE = √(1 / ∑ w_i)\] <p>This SE will be larger than under the fixed-effect model due to added uncertainty from heterogeneity.</p> <hr/> <p><strong>Step 6: Choose a Confidence Interval Method</strong></p> <p>There are multiple options for calculating the 95% confidence interval around the pooled estimate in random-effects models:</p> <p><strong>a) z-distribution Method</strong></p> <ul> <li>Assumes a normal distribution</li> <li>Does <strong>not</strong> adjust for uncertainty in estimating τ²</li> </ul> \[CI = μ* ± 1.96 × SE\] <hr/> <p><strong>b) t-distribution Method</strong></p> <ul> <li>Uses the t-distribution with k−1 degrees of freedom</li> <li>Accounts for small-sample (n &lt; 30) uncertainty better than the z-method</li> </ul> \[CI = μ* ± t_(k−1, 1−α/2) × SE\] <hr/> <p><strong>c) Hartung-Knapp (HK) Method</strong></p> <p>This method gives wider but more reliable intervals, especially when the number of studies is small or heterogeneity is high.</p> <p>First, calculate the improved variance estimate:</p> \[Var_w(μ*) = ∑ w_i × (x_i − μ*)² / [(k − 1) × ∑ w_i]\] <p>Then, construct the confidence interval:</p> \[CI = μ* ± t_(k−1, 1−α/2) × √Var_w(μ*)\] <hr/> <h2 id="flowchart-overview">Flowchart Overview</h2> <p>The flowchart below visually summarizes the <strong>critical steps</strong> involved in conducting a meta-analysis using either a <strong>fixed-effect</strong> or a <strong>random-effects model</strong>. It is designed to help readers quickly grasp the overall workflow and key distinctions between the two approaches.</p> <ul> <li><strong>Decision points</strong> appear at the top, guiding model selection based on assumptions about study heterogeneity.</li> <li><strong>Color coding</strong> is used to clearly distinguish the <strong>fixed-effect pathway</strong> from the <strong>random-effects pathway</strong>.</li> <li><strong>Dashed lines</strong> highlight key <strong>outcome variables</strong> at each stage (e.g., pooled effect size, standard error, confidence intervals).</li> <li>Steps flow logically from calculating individual effect sizes to pooling, estimating uncertainty, and reporting the final results.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/meta_analysis_workflow_decision_tree.png" sizes="95vw"/> <img src="/assets/img/meta_analysis_workflow_decision_tree.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This diagram complements the written walkthrough by providing a high-level visual guide to the computational and statistical logic behind each model type.</p> <h2 id="easy-r-steps-for-meta-analysis">Easy R Steps for Meta-Analysis</h2> <h3 id="example-dataset">Example Dataset</h3> <p>To make the code examples more practical and relevant to microbiome researchers, this post uses a real dataset from a <strong>published meta-analysis</strong> on the impact of <strong>exclusive breastfeeding</strong> on <strong>infant gut microbiome diversity</strong>.</p> <p>The data were drawn from <strong>high-quality, peer-reviewed studies</strong>, including contributions from respected researchers such as <strong>Dr. Louise Kuhn</strong> and <strong>Dr. Anita Kozyrskyj</strong>, who kindly shared accessible summary statistics and metadata. Dr. Kozyrskyj was also my supervisor during my postdoctoral fellowship, and her support made it possible to reproduce this meta-analysis for educational purposes.</p> <p>The dataset below includes:</p> <ul> <li>Study reference</li> <li>Sample size</li> <li>Effect size (diversity difference)</li> <li>Standard error of the effect size</li> </ul> <p>Notably, the <strong>Subramanian et al. study</strong> (conducted in Bangladesh) had the <strong>largest sample size</strong> and therefore the <strong>highest precision</strong> (i.e., lowest standard error), contributing significantly to the meta-analysis.</p> <table> <thead> <tr> <th>Study</th> <th>Sample Size</th> <th>Diversity Diff.</th> <th>SE</th> </tr> </thead> <tbody> <tr> <td>Subramanian et al., 2014 (Bangladesh)</td> <td>322</td> <td>0.26</td> <td>0.0718</td> </tr> <tr> <td>Azad et al., 2015 (Canada)</td> <td>167</td> <td>0.33</td> <td>0.1583</td> </tr> <tr> <td>Bender et al., 2016 (Haiti)</td> <td>48</td> <td>-0.11</td> <td>0.3474</td> </tr> <tr> <td>Wood et al., 2018 (South Africa)</td> <td>143</td> <td>0.31</td> <td>0.2235</td> </tr> <tr> <td>Pannaraj et al., 2017 (USA(CA/FL))</td> <td>230</td> <td>0.37</td> <td>0.1492</td> </tr> <tr> <td>Sordillo et al., 2017 (USA(CA/MA/MO))</td> <td>220</td> <td>0.77</td> <td>0.1971</td> </tr> <tr> <td>Thompson et al., 2015 (USA(NC))</td> <td>21</td> <td>0.3</td> <td>0.4239</td> </tr> </tbody> </table> <hr/> <h3 id="computational-tools">Computational Tools</h3> <p>To support reproducibility and hands-on learning, this use case provides <strong>re-usable R code snippets</strong> that implement two essential steps in a typical microbiome meta-analysis:</p> <ol> <li><strong>Fixed-effect and random-effects models</strong> using inverse-variance weighting</li> <li><strong>Forest plot visualization</strong> of individual and pooled study results</li> </ol> <p>After a brief comparison of available tools, the <strong>R ecosystem</strong> was chosen over Python due to its <strong>more mature, stable, and flexible support for meta-analysis</strong>, especially in microbiome and clinical research contexts.</p> <p>The analysis was implemented using the <strong>meta R package</strong>, which is:</p> <ul> <li>Well-documented and actively maintained</li> <li>Supported by detailed tutorials and vignettes</li> <li>Beginner-friendly, yet robust enough for advanced use</li> </ul> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Perform fixed-effect and random-effect models on diversity difference using inverse variance </span><span class="w">
</span><span class="n">weightmicrobiome_sdd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">meta</span><span class="o">::</span><span class="n">metagen</span><span class="p">(</span><span class="n">TE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">diversity_diff</span><span class="p">,</span><span class="w"> </span><span class="c1"># Study effect size                                </span><span class="w">
                                    </span><span class="n">seTE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">se</span><span class="p">,</span><span class="w"> </span><span class="c1"># Standard error of effect sizes                               </span><span class="w">
                                    </span><span class="n">studlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">study</span><span class="p">,</span><span class="w"> </span><span class="c1"># Study labels                               </span><span class="w">
                                    </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">microbiome_df</span><span class="p">,</span><span class="w"> </span><span class="c1"># Data frame containing statistical information</span><span class="w">
                                    </span><span class="n">common</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="c1"># Conduct fixed-effect model meta-analysis</span><span class="w">
                                    </span><span class="n">random</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="c1"># Conduct random-effect model meta-analysis                               </span><span class="w">
                                    </span><span class="n">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="c1"># Not print prediction interval                               </span><span class="w">
                                    </span><span class="n">method.I2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Q"</span><span class="p">,</span><span class="w"> </span><span class="c1"># Method used to estimate heterogeneity statistics I^2                               </span><span class="w">
                                    </span><span class="n">method.tau</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"DL"</span><span class="p">,</span><span class="w"> </span><span class="c1"># DerSimonian-Laird estimator                               </span><span class="w">
                                    </span><span class="n">method.tau.ci</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"J"</span><span class="p">,</span><span class="w"> </span><span class="c1"># Method by Jackson (2013)                               </span><span class="w">
                                    </span><span class="n">method.random.ci</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"HK"</span><span class="w"> </span><span class="c1"># Method by Hartung and Knapp (2001a/b)                               </span><span class="w">
                                    </span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <h4 id="understanding-key-arguments-in-metametagen">Understanding key arguments in <code class="language-plaintext highlighter-rouge">meta::metagen()</code></h4> <p>To support beginners who may have only basic familiarity with meta-analysis, each function in this post includes <strong>annotated explanations of its core arguments</strong>. In particular, the R function meta::metagen()—used to compute fixed- and random-effects models—contains several <strong>powerful arguments</strong> that greatly increase its flexibility.</p> <p>Among these, four arguments stand out as especially important:</p> <ul> <li><strong>method.I2</strong> – Controls how the <strong>I² statistic</strong> (a measure of heterogeneity) is calculated.</li> <li><strong>method.tau</strong> – Specifies the method for estimating the <strong>between-study variance (τ²)</strong>, such as DerSimonian-Laird, REML, or ML.</li> <li><strong>method.tau.ci</strong> – Determines how the <strong>confidence interval for τ²</strong> is calculated.</li> <li><a href="http://method.random.ci/"><strong>method.random.ci</strong></a> – Selects the method used to calculate the <strong>confidence interval around the random-effects pooled estimate</strong> (e.g., classical vs. Hartung-Knapp).</li> </ul> <p>These arguments offer <strong>flexibility and customization</strong>, especially when tailoring the analysis to match study heterogeneity or small-sample concerns. However, <strong>intentional use requires a solid understanding</strong> of their conceptual foundations.</p> <p>The <strong>earlier sections of this post</strong> walk through these core concepts to help you build the minimal understanding needed to make informed choices.</p> <p>For a deeper dive and full list of options, consult the <a href="https://cran.r-project.org/web/packages/meta/meta.pdf">official documentation of the meta package</a>. Exploring these arguments will help you fully leverage the power of metagen() in your own microbiome meta-analyses.</p> <hr/> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">summary</span><span class="p">(</span><span class="n">microbiome_sdd</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">                          </span><span class="m">95</span><span class="o">%-CI           %</span><span class="n">W</span><span class="p">(</span><span class="n">common</span><span class="p">)</span><span class="w">  </span><span class="o">%W(random)
Subramanian et al., 2014 (Bangladesh)   0.2600 [ 0.1193; 0.4007]    57.3       40.7
Azad et al., 2015 (Canada)              0.3300 [ 0.0197; 0.6403]    11.8       15.7
Bender et al., 2016 (Haiti)            -0.1100 [-0.7909; 0.5709]     2.4        4.0
Wood et al., 2018 (South Africa)        0.3100 [-0.1281; 0.7481]     5.9        8.9
Pannaraj et al., 2017 (USA(CA/FL))      0.3700 [ 0.0776; 0.6624]    13.3       17.1
Sordillo et al., 2017 (USA(CA/MA/MO))   0.7700 [ 0.3837; 1.1563]     7.6       11.0
Thompson et al., 2015 (USA(NC))         0.3000 [-0.5308; 1.1308]     1.6        2.7

Number of studies: k = 7

                         95%-</span><span class="n">CI</span><span class="w">                   </span><span class="n">z</span><span class="o">|</span><span class="n">t</span><span class="w">     </span><span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="w">
</span><span class="n">Common</span><span class="w"> </span><span class="n">effect</span><span class="w"> </span><span class="n">model</span><span class="o">:</span><span class="w">     </span><span class="m">0.3162</span><span class="w"> </span><span class="p">[</span><span class="m">0.2097</span><span class="p">;</span><span class="w"> </span><span class="m">0.4227</span><span class="p">]</span><span class="w">   </span><span class="m">5.82</span><span class="w">   </span><span class="o">&lt;</span><span class="w"> </span><span class="m">0.0001</span><span class="w">
</span><span class="n">Random</span><span class="w"> </span><span class="n">effects</span><span class="w"> </span><span class="n">model</span><span class="o">:</span><span class="w">    </span><span class="m">0.3367</span><span class="w"> </span><span class="p">[</span><span class="m">0.1602</span><span class="p">;</span><span class="w"> </span><span class="m">0.5132</span><span class="p">]</span><span class="w">   </span><span class="m">4.67</span><span class="w">     </span><span class="m">0.0034</span><span class="w">

</span><span class="n">Quantifying</span><span class="w"> </span><span class="n">heterogeneity</span><span class="w"> </span><span class="p">(</span><span class="n">with</span><span class="w"> </span><span class="m">95</span><span class="o">%-CIs):
tau^2 = 0.0073 [0.0000; 0.2032]; tau = 0.0855 [0.0000; 0.4508]
I^2 = 20.6%</span><span class="w"> </span><span class="p">[</span><span class="m">0.0</span><span class="o">%; 64.0%</span><span class="p">];</span><span class="w"> </span><span class="n">H</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1.12</span><span class="w"> </span><span class="p">[</span><span class="m">1.00</span><span class="p">;</span><span class="w"> </span><span class="m">1.67</span><span class="p">]</span><span class="w">

</span><span class="n">Test</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">heterogeneity</span><span class="o">:</span><span class="w">
</span><span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">7.56</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.2723</span><span class="w">

</span><span class="n">Details</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">meta</span><span class="o">-</span><span class="n">analysis</span><span class="w"> </span><span class="n">methods</span><span class="o">:</span><span class="w">
</span><span class="o">-</span><span class="w"> </span><span class="n">Inverse</span><span class="w"> </span><span class="n">variance</span><span class="w"> </span><span class="n">method</span><span class="w">
</span><span class="o">-</span><span class="w"> </span><span class="n">DerSimonian</span><span class="o">-</span><span class="n">Laird</span><span class="w"> </span><span class="n">estimator</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">tau</span><span class="o">^</span><span class="m">2</span><span class="w">
</span><span class="o">-</span><span class="w"> </span><span class="n">Jackson</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">CI</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">tau</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">tau</span><span class="w">
</span><span class="o">-</span><span class="w"> </span><span class="n">I</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="n">calculation</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">Q</span><span class="w">
</span><span class="o">-</span><span class="w"> </span><span class="n">Hartung</span><span class="o">-</span><span class="n">Knapp</span><span class="w"> </span><span class="n">adjustment</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">random</span><span class="w"> </span><span class="n">effects</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="p">(</span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <h3 id="draw-forest-plot">Draw forest plot</h3> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">png</span><span class="p">(</span><span class="n">file</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"forestplot.png"</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">800</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1200</span><span class="p">)</span><span class="w">

</span><span class="n">meta</span><span class="o">::</span><span class="n">forest</span><span class="p">(</span><span class="n">microbiome_sdd</span><span class="p">,</span><span class="w">             
            </span><span class="n">sortvar</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TE</span><span class="p">,</span><span class="w"> </span><span class="c1"># Sort study by effect sizes in increasing order             </span><span class="w">
            </span><span class="n">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="c1"># Not show prediction interval             </span><span class="w">
            </span><span class="n">leftcols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"study"</span><span class="p">,</span><span class="w"> </span><span class="s2">"sample_size"</span><span class="p">,</span><span class="w"> </span><span class="s2">"diversity_diff"</span><span class="p">,</span><span class="w"> </span><span class="s2">"se"</span><span class="p">),</span><span class="w">             
            </span><span class="n">leftlabs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"Study"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Sample Size"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Diversity Diff."</span><span class="p">,</span><span class="w"> </span><span class="s2">"SE"</span><span class="p">),</span><span class="w">             
            </span><span class="n">print.tau2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span><span class="n">dev.off</span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/forestplot.png" sizes="95vw"/> <img src="/assets/img/forestplot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="interpret-the-forest-plot">Interpret the Forest Plot</h4> <p>This forest plot compares the results of seven studies evaluating <strong>gut microbiome diversity difference (DD)</strong> between groups. It shows both individual study results and pooled estimates under fixed-effect and random-effects models.</p> <p><strong>1. Study Data (Left Side)</strong></p> <ul> <li>Each row corresponds to a single study included in the meta-analysis.</li> <li><strong>DD</strong> stands for <strong>diversity difference</strong>, which is the effect size used in this analysis.</li> <li><strong>SE</strong> stands for <strong>standard error</strong>, which reflects the precision of the effect size estimate. Smaller SE means higher precision.</li> </ul> <p><strong>2. Confidence Intervals and Visual Elements (Middle Area)</strong></p> <ul> <li><strong>Horizontal lines</strong> show the <strong>95% confidence interval (CI)</strong> for each study’s effect size.</li> <li><strong>Squares</strong> represent the <strong>point estimate</strong> of each study. Larger squares indicate <strong>greater weight</strong> in the analysis.</li> <li>The <strong>vertical solid line</strong> at 0 is the <strong>line of no effect</strong>—values to the right suggest a positive effect.</li> <li><strong>Diamond shapes</strong> summarize the <strong>pooled effect estimates</strong>: <ul> <li>The gray diamond: fixed-effect (common-effect) model</li> <li>The white diamond: random-effects model</li> </ul> </li> <li>The <strong>red horizontal bar</strong> under the diamond is the <strong>prediction interval</strong>, indicating the likely range of effect sizes in <strong>future studies</strong>.</li> </ul> <p><strong>3. Result Summary (Right Side)</strong></p> <ul> <li>The <strong>95% CI</strong> for each study shows the uncertainty around its effect estimate.</li> <li>The columns <strong>Weight (common)</strong> and <strong>Weight (random)</strong> show how much each study contributes to the respective model. <ul> <li>In the <strong>fixed-effect model</strong>, Study 1 carries <strong>57.3%</strong> of the weight due to its very low SE (high precision).</li> <li>In the <strong>random-effects model</strong>, weights are more evenly distributed because the model also accounts for <strong>between-study heterogeneity</strong> (τ²).</li> </ul> </li> </ul> <p><strong>4. Heterogeneity (Bottom Statistics)</strong></p> <ul> <li><strong>I² = 20.6%</strong>: Indicates <strong>low to moderate heterogeneity</strong>—i.e., some variation in effect sizes across studies, but not extreme.</li> <li><strong>p = 0.2723</strong>: The test for heterogeneity is <strong>not statistically significant</strong>, meaning we <strong>cannot reject</strong> the idea that all studies may share one common effect size.</li> </ul> <p><strong>Summary Interpretation</strong></p> <ul> <li>The pooled effect sizes from the <strong>fixed-effect model (0.32)</strong> and the <strong>random-effects model (0.33)</strong> are very similar, which suggests that the result is <strong>robust and stable</strong>.</li> <li>Since mild heterogeneity is present, the <strong>random-effects model is more appropriate</strong> for generalization.</li> <li>The <strong>prediction interval [0.09, 0.57]</strong> suggests that <strong>future studies</strong> are still likely to show a <strong>positive effect</strong>, though the effect size may vary in strength.</li> </ul> <hr/> <h2 id="practical-recommendations">Practical Recommendations</h2> <p><strong>Fixed-Effects Models:</strong> Use the standard <em>z-</em>distribution method with size-weighted means when the number of studies is large.</p> <p><strong>Random-Effects Models:</strong> Prefer the <strong>weighted variance confidence interval, such as HK,</strong> method for better coverage and less sensitive to which estimator used for τ².</p> <p><strong>Sample Size Weighting</strong> is appropriate when:</p> <ul> <li>Variance estimates are unavailable or unreliable</li> <li>Interpretability is a priority</li> <li>Study precisions are comparable</li> </ul> <p><strong>Best Practice:</strong> Report both <strong>parametric</strong> and <strong>nonparametric</strong> weighted results to test the robustness of conclusions.</p> <hr/> <h2 id="references">References</h2> <ul> <li>Perplexity research <a href="https://www.perplexity.ai/search/introduce-step-to-step-guide-t-RFYI0pQZRvmm.9bK664ABg">report</a></li> <li>Doing meta-analysis in R <a href="https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/">Chapter 3-6</a></li> </ul>]]></content><author><name></name></author><category term="statistics"/><category term="meta-analysis,"/><category term="R,"/><category term="microbiome"/><summary type="html"><![CDATA[Introduction This post serves as a hands-on tutorial for beginners who are learning how to conduct meta-analysis in the context of microbiome research. Whether you’re synthesizing findings from multiple studies or comparing gut microbiome diversity across populations, this guide is designed to help you get started with clarity and confidence.]]></summary></entry><entry><title type="html">Evaluating Confidence Interval Methods under Skewness: Practical Insights from Simulation</title><link href="https://davidzhao1015.github.io/blog/2025/mean-interval-estimators/" rel="alternate" type="text/html" title="Evaluating Confidence Interval Methods under Skewness: Practical Insights from Simulation"/><published>2025-05-02T00:00:00+00:00</published><updated>2025-05-02T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/mean-interval-estimators</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/mean-interval-estimators/"><![CDATA[<h2 id="introduction"><strong>Introduction</strong></h2> <p>Estimating the mean and its confidence interval (CI) is a fundamental task in statistical analysis. Confidence intervals quantify the uncertainty of an estimate, providing a range of plausible values for a population parameter based on sampled data.</p> <p>However, <strong>when data distributions are skewed or overdispersed</strong>, standard CI methods may no longer perform reliably:</p> <ul> <li>They might produce intervals that are <strong>too narrow</strong>, failing to capture the true mean often enough (under-coverage).</li> <li>Or, they might become <strong>overly conservative</strong>, yielding unnecessarily wide intervals.</li> </ul> <p>This is particularly relevant in fields like <strong>healthcare</strong> and <strong>rare disease research</strong>, where data such as <strong>hospital length of stay (LOS)</strong> commonly exhibit <strong>right-skewed</strong> distributions with a few extreme outliers. In such cases, choosing an appropriate CI estimation method becomes critical for making valid inferences.</p> <h3 id="motivation-for-benchmarking-methods"><strong>Motivation for Benchmarking Methods</strong></h3> <p>Different methods—such as the <strong>t-distribution</strong>, <strong>z-distribution</strong>, and <strong>bootstrap percentile approaches</strong>—are often used interchangeably for CI estimation. Yet, their performance can vary substantially depending on:</p> <ul> <li>Sample size</li> <li>Data skewness</li> <li>Variability (overdispersion)</li> </ul> <p>This project benchmarks these methods under simulated skewed data conditions to evaluate:</p> <ul> <li><strong>Coverage Probability</strong>: How often does the CI contain the true mean?</li> <li><strong>Interval Width</strong>: How wide is the resulting confidence interval?</li> </ul> <p>The goal is to provide practical insights for analysts working with small, skewed samples, ensuring more reliable and interpretable statistical conclusions.</p> <p><a href="https://mybinder.org/v2/gh/davidzhao1015/mean-interval-estimators-benchmark/HEAD?urlpath=%2Fdoc%2Ftree%2Finterval-estimate-mean-benchmark_20250509_DZ.ipynb"><img src="https://mybinder.org/badge_logo.svg" alt="Binder"/></a></p> <hr/> <h2 id="methods"><strong>Methods</strong></h2> <h3 id="confidence-interval-estimation-methods"><strong>Confidence Interval Estimation Methods</strong></h3> <p>We compared three statistical methods to estimate the 95% confidence interval of the sample mean:</p> <ol> <li> <p><strong>t-distribution method</strong></p> <p>Calculates CI using the sample mean, sample standard deviation, and t-critical value with degrees of freedom (n-1).</p> </li> <li> <p><strong>z-distribution method</strong></p> <p>Similar to the t-method but uses the normal z-critical value, assuming population standard deviation is known (in practice, sample SD was used).</p> </li> <li> <p><strong>Bootstrap percentile method</strong></p> <p>Resamples the observed sample with replacement.</p> <p>The CI is constructed from the 2.5th and 97.5th percentiles of the resampled means.</p> </li> </ol> <h3 id="simulation-of-skewed-data"><strong>Simulation of Skewed Data</strong></h3> <p>To benchmark different methods for constructing confidence intervals (CIs) of the mean, we simulated data resembling the distribution of hospital length of stay in autoimmune encephalitis patients. Specifically:</p> <ul> <li>Data were generated from a <strong>log-normal distribution</strong>, a typical choice for modeling right-skewed and overdispersed outcomes like hospital stay durations.</li> <li>The <strong>true mean</strong> was set to <strong>20 days</strong>, with <strong>true standard deviations</strong> of <strong>5, 10, 20, and 40 days</strong>.</li> <li>For each scenario, the corresponding log-space parameters (mu and sigma) were computed to ensure realistic skewness.</li> <li>A large population of <strong>10,000 observations</strong> was generated per scenario to represent the “true” population.</li> </ul> <h3 id="simulation-study-design"><strong>Simulation Study Design</strong></h3> <p>To evaluate the performance of these methods under varying conditions:</p> <ul> <li><strong>Sample sizes</strong> tested: <strong>10, 30, and 100</strong>.</li> <li><strong>Skewness levels</strong> tested: corresponding to <strong>sigma = 0.4, 0.8, and 1.2</strong> in log space.</li> <li>For each combination, we repeated the sampling and CI estimation <strong>1,000 times</strong>.</li> </ul> <h3 id="performance-metrics"><strong>Performance Metrics</strong></h3> <p>Two metrics were computed to assess method performance:</p> <ul> <li><strong>Coverage Probability</strong>: Proportion of simulations where the constructed CI contained the true population mean.</li> <li><strong>Interval Width</strong>: Average length of the confidence intervals across simulations.</li> </ul> <h3 id="visualization"><strong>Visualization</strong></h3> <p>Bar plots were used to visualize:</p> <ul> <li>The coverage probabilities of each method relative to the nominal 95% level.</li> <li>The average interval widths, illustrating the trade-off between precision and accuracy.</li> </ul> <hr/> <h2 id="results"><strong>Results</strong></h2> <h3 id="coverage-probability"><strong>Coverage Probability</strong></h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/barplot_coverage.png" sizes="95vw"/> <img src="/assets/img/barplot_coverage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>A bar plot comparing the <strong>coverage probability</strong> of three methods (t-distribution, z-distribution, bootstrap) across:</p> <ul> <li>Different <strong>sample sizes</strong> (10, 30, 100)</li> <li>Different <strong>skewness levels</strong> (0.4, 0.8, 1.2)</li> </ul> <h3 id="interval-width"><strong>Interval Width</strong></h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/barplot_precision.png" sizes="95vw"/> <img src="/assets/img/barplot_precision.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>A bar plot comparing the <strong>average width (length) of confidence intervals</strong> for the same methods, sample sizes, and skew levels.</p> <hr/> <h2 id="discussion"><strong>Discussion</strong></h2> <p>This simulation study compared three common methods—<strong>t-distribution</strong>, <strong>z-distribution</strong>, and <strong>bootstrap percentile</strong>—for constructing 95% confidence intervals (CIs) of the mean in right-skewed, overdispersed data, mimicking hospital length of stay in rare disease contexts.</p> <h3 id="key-findings"><strong>Key Findings:</strong></h3> <ol> <li><strong>Coverage Probability</strong>: <ul> <li>The <strong>t-distribution method</strong> consistently provided coverage probabilities closest to the <strong>nominal 95% level</strong>, even with small sample sizes and increased skewness.</li> <li>The <strong>bootstrap method</strong>, while flexible, showed notable <strong>under-coverage</strong> in small and highly skewed samples (e.g., &lt;90% for n=10, skew=1.2).</li> <li>The <strong>z-distribution method</strong> tended to under-cover as well, performing worse than t-distribution but slightly better than bootstrap in high skew conditions.</li> </ul> </li> <li><strong>Interval Width</strong>: <ul> <li><strong>Bootstrap intervals were consistently narrower</strong> than those from t- and z-methods, reflecting its efficiency.</li> <li><strong>t-distribution intervals were widest</strong>, indicating a more conservative approach to uncertainty.</li> <li>With larger sample sizes (n=100), the differences in interval width narrowed, and all methods performed better.</li> </ul> </li> </ol> <h3 id="interpretation"><strong>Interpretation:</strong></h3> <p>These results highlight a <strong>bias-variance trade-off</strong>:</p> <ul> <li><strong>Bootstrap</strong> offers more precise (narrower) intervals but at the cost of coverage reliability, especially in small, skewed samples.</li> <li><strong>t-distribution</strong> sacrifices precision (wider intervals) to maintain more accurate coverage.</li> <li><strong>z-distribution</strong> is suboptimal when sample sizes are small and data is skewed, as it assumes normality which is not met.</li> </ul> <h3 id="practical-implications"><strong>Practical Implications:</strong></h3> <ul> <li>For <strong>small sample studies with skewed data</strong> (common in rare disease research), the <strong>t-distribution method is safer</strong> for mean estimation, despite its conservatism.</li> <li><strong>Bootstrap methods should be applied cautiously</strong> when sample size is limited or data is heavily skewed.</li> <li>With <strong>larger samples</strong>, all methods converge in performance, making bootstrap’s efficiency more appealing.</li> </ul> <hr/> <h2 id="-code-snippets"><strong>🧑‍💻 Code Snippets</strong></h2> <p>To help you apply these methods in your own analysis, here are reusable Python code snippets for constructing confidence intervals (CIs) of the mean using <strong>t-distribution</strong>, <strong>z-distribution</strong>, and <strong>bootstrap methods</strong>. These functions are included in the full <a href="https://github.com/davidzhao1015/mean-interval-estimators-benchmark">Jupyter notebook</a> for reproducibility.</p> <h3 id="-t-distribution-confidence-interval"><strong>📏 t-distribution Confidence Interval</strong></h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from scipy import stats
import numpy as np

def ci_t(sample):
	n = len(sample)
	m = np.mean(sample)
	se = np.std(sample, ddof=1) / np.sqrt(n)
	t_crit = stats.t.ppf(0.975, df=n-1)
	return m - t_crit * se, m + t_crit * sedef ci_t(sample, ci_level=0.95):
	    """Compute t-distribution confidence interval for the sample mean."""
	    n = len(sample)
	    sample_mean = np.mean(sample)
	    sample_se = np.std(sample, ddof=1) / np.sqrt(n)
	    t_crit = stats.t.ppf(0.5 + ci_level / 2, df=n-1)
	    lower = sample_mean - t_crit * sample_se
	    upper = sample_mean + t_crit * sample_se
	    return lower, upper
</code></pre></div></div> <hr/> <h3 id="-z-distribution-confidence-interval"><strong>📏 z-distribution Confidence Interval</strong></h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def ci_z(sample):
    n = len(sample)
    m = np.mean(sample)
    se = np.std(sample, ddof=1) / np.sqrt(n)
    z_crit = stats.norm.ppf(0.975)
    return m - z_crit * se, m + z_crit * se
</code></pre></div></div> <hr/> <h3 id="-bootstrap-percentile-confidence-interval"><strong>🔁 Bootstrap Percentile Confidence Interval</strong></h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def ci_bootstrap(sample, n_bootstraps=1000, ci=0.95):
    n = len(sample)
    means = []

    for _ in range(n_bootstraps):
        resample = np.random.choice(sample, size=n, replace=True)
        means.append(np.mean(resample))

    alpha = 1 - ci
    lower = np.percentile(means, 100 * (alpha / 2))
    upper = np.percentile(means, 100 * (1 - alpha / 2))
    return lower, upper
</code></pre></div></div> <hr/> <h3 id="-how-to-use"><strong>📝 How to Use:</strong></h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Example usage with your sample data:
sample_data = np.random.lognormal(mean=np.log(20), sigma=0.8, size=30)

t_ci = ci_t(sample_data)
z_ci = ci_z(sample_data)
bootstrap_ci = ci_bootstrap(sample_data, n_bootstraps=1000)

print(f"t-distribution CI: {t_ci}")
print(f"z-distribution CI: {z_ci}")
print(f"Bootstrap CI: {bootstrap_ci}")
</code></pre></div></div> <hr/> <h3 id="-full-notebook--reproducibility"><strong>🔗 Full Notebook &amp; Reproducibility</strong></h3> <p>For full simulation workflows, including population data generation, performance metrics, and visualization:</p> <p>👉 View the Jupyter Notebook on <a href="https://github.com/davidzhao1015/mean-interval-estimators-benchmark">GitHub</a><br/> Or, view the notebook on Binder with no installing dependencies <a href="https://mybinder.org/v2/gh/davidzhao1015/mean-interval-estimators-benchmark/HEAD?urlpath=%2Fdoc%2Ftree%2Finterval-estimate-mean-benchmark_20250509_DZ.ipynb"><img src="https://mybinder.org/badge_logo.svg" alt="Binder"/></a></p>]]></content><author><name></name></author><category term="statistics"/><category term="confidence-intervals,"/><category term="statistics,"/><category term="mean,"/><category term="methods-comparison"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Comparing Methods for Estimating 95% Confidence Intervals of Proportions: Wald, Wilson, and Equal-Tailed Jeffreys Prior</title><link href="https://davidzhao1015.github.io/blog/2025/benchmark-interval-prop/" rel="alternate" type="text/html" title="Comparing Methods for Estimating 95% Confidence Intervals of Proportions: Wald, Wilson, and Equal-Tailed Jeffreys Prior"/><published>2025-04-01T00:00:00+00:00</published><updated>2025-04-01T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/benchmark-interval-prop</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/benchmark-interval-prop/"><![CDATA[<blockquote> <p><strong>How much can you trust a number when dealing with rare events?</strong></p> <p>In statistics, it’s not just about getting an estimate — it’s about knowing how uncertain that estimate really is.</p> </blockquote> <h2 id="introduction"><strong>Introduction</strong></h2> <p>Estimating confidence intervals (CIs) for proportions is a fundamental task in statistical analysis, particularly in fields like epidemiology, public health, and rare disease research.</p> <p>Confidence intervals provide a measure of uncertainty around point estimates like incidence rates, prevalence, and response rates, helping researchers and decision-makers gauge the reliability of their findings.</p> <p>However, constructing accurate confidence intervals for proportions is not always straightforward—especially when dealing with small sample sizes or rare events, where traditional methods may fall short.</p> <p>In this post, we focus on three widely used methods for estimating 95% confidence intervals for proportions:</p> <ul> <li> <p><strong>Wald Method:</strong></p> <p>A traditional approach based on the normal approximation, known for its simplicity but prone to inaccuracies in small samples or when proportions are close to 0 or 1.</p> </li> <li> <p><strong>Wilson Method:</strong></p> <p>An improved frequentist method that adjusts for the shortcomings of Wald, offering more reliable intervals that stay within logical bounds.</p> </li> <li> <p><strong>Equal-Tailed Jeffreys Prior Method:</strong></p> <p>A Bayesian approach using a non-informative prior, known for its excellent frequentist properties and particularly strong performance in rare event settings.</p> </li> </ul> <p>Through simulations designed to mimic rare disease scenarios, we will benchmark these methods, providing both empirical insights and practical Python code to guide method selection in real-world analyses.</p> <h2 id="purpose-of-the-post">Purpose of the post</h2> <p>Which method gives the most reliable confidence interval for proportions? The short answer: it depends. As highlighted by the classic review by Brown et al., the performance of CI estimation methods can vary based on factors like sample size and how close the true proportion is to 0 or 1.</p> <p>This post puts three popular methods—Wald, Wilson, and the equal-tailed Jeffreys prior—to the test using simulated data. By benchmarking their performance, especially in rare disease contexts where small samples are common, we aim to provide practical guidance for choosing the right method—along with reusable Python code for each.</p> <hr/> <h2 id="understanding-confidence-intervals-for-proportions"><strong>Understanding Confidence Intervals for Proportions</strong></h2> <p>In statistical analysis, a <strong>confidence interval (CI)</strong> provides a range of plausible values for an unknown population parameter based on observed data.</p> <p>When estimating a <strong>proportion</strong>—such as the incidence of a rare disease, the prevalence of a microbial species, or a treatment success rate—the confidence interval quantifies the uncertainty around the point estimate.</p> <p>Rather than relying solely on a single value (e.g., “2% prevalence”), a 95% confidence interval might tell us that the true prevalence is likely between 1.5% and 2.7%, offering a more complete picture of the estimate’s reliability.</p> <p>Mathematically, a 95% CI means that if we were to repeat the study many times under the same conditions, about 95% of the constructed intervals would contain the true proportion.</p> <hr/> <h3 id="why-accurate-interval-estimation-matters"><strong>Why Accurate Interval Estimation Matters</strong></h3> <p>Accurate estimation of confidence intervals is critically important in fields like <strong>epidemiology</strong> and <strong>microbiome research</strong>, where:</p> <ul> <li> <p><strong>Rare Disease Studies:</strong></p> <p>Small sample sizes and extremely low event rates are common. Overly narrow or inaccurate intervals can falsely suggest high precision, leading to misguided conclusions about disease risk, treatment efficacy, or healthcare burden.</p> </li> <li> <p><strong>Microbiome Studies:</strong></p> <p>The presence or abundance of microbial taxa can be rare or highly variable. Reliable intervals help interpret findings with appropriate caution, avoiding overinterpretation of noisy or sparse data.</p> </li> </ul> <p>Poorly constructed intervals—especially those that underestimate uncertainty—can mislead both researchers and policymakers, leading to flawed decision-making, missed treatment opportunities, or inappropriate resource allocation.</p> <p>Thus, selecting a method that maintains valid coverage while offering reasonable precision is not merely a technical preference; it is essential for trustworthy science.</p> <hr/> <h2 id="overview-of-the-methods"><strong>Overview of the Methods</strong></h2> <p>Building on insights from Brown et al., here’s a practical overview of the Wald, Wilson, and Jeffreys methods—covering key concepts, formulas, and typical use cases.</p> <h3 id="wald-method"><strong>Wald Method</strong></h3> <ul> <li> <p><strong>Formula:</strong></p> \[\hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}\] <p></p> </li> <li> <p><strong>Summary:</strong></p> <p>Simple and widely taught, but often inaccurate—especially with small samples or proportions near 0 or 1.</p> </li> <li> <p><strong>Limitations:</strong></p> <p>Poor coverage, unstable even with moderate to large samples; not recommended for rare event settings.</p> </li> </ul> <h3 id="wilson-method"><strong>Wilson Method</strong></h3> <ul> <li> <p><strong>Formula:</strong></p> \[\frac{\hat{p} + \frac{z^2}{2n} \pm z \sqrt{ \frac{\hat{p}(1 - \hat{p})}{n} + \frac{z^2}{4n^2} }}{1 + \frac{z^2}{n}}\] <p></p> </li> <li> <p><strong>Summary:</strong></p> <p>A corrected method that improves coverage and keeps intervals within [0,1].</p> </li> <li> <p><strong>Advantages:</strong></p> <p>Reliable even with small samples; recommended for real-world use over Wald.</p> </li> </ul> <h3 id="jeffreys-method"><strong>Jeffreys Method</strong></h3> <ul> <li> <p><strong>Formula:</strong></p> <p>Based on Beta posterior:</p> \[\text{Beta}(X+0.5, n-X+0.5)\] <p></p> </li> <li> <p><strong>Summary:</strong></p> <p>A Bayesian-based method with strong frequentist performance, especially for rare events.</p> </li> <li> <p><strong>Advantages:</strong></p> <p>Excellent coverage, stable near boundaries, highly suitable for small sample and rare event studies.</p> </li> </ul> <hr/> <h2 id="estimating-confidence-intervals-a-hands-on-example"><strong>Estimating Confidence Intervals: A Hands-On Example</strong></h2> <p>Before diving into large-scale simulations, let’s first walk through a simple example to see how the three methods—Wald, Wilson, and Jeffreys—work in practice.</p> <p>We’ll assume:</p> <ul> <li><strong>Sample size (n):</strong> 2000</li> <li><strong>Number of cases (x):</strong> 1</li> </ul> <p>Using Python’s statsmodels package, we can easily calculate the 95% confidence intervals for the proportion estimate using each method.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">statsmodels.stats.proportion</span> <span class="kn">import</span> <span class="n">proportion_confint</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Set the random seed for reproducibility
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># Define parameters
</span><span class="n">cases</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="c1"># 1. Wald method
</span><span class="n">ci_wald_estimate</span> <span class="o">=</span> <span class="nf">proportion_confint</span><span class="p">(</span><span class="n">cases</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">normal</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">95% Wald CI: [</span><span class="si">{</span><span class="n">ci_wald_estimate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">ci_wald_estimate</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">]</span><span class="sh">'</span><span class="p">)</span> 

<span class="c1"># 2. Wilson method
</span><span class="n">ci_wilson_estimate</span> <span class="o">=</span> <span class="nf">proportion_confint</span><span class="p">(</span><span class="n">cases</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">wilson</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">95% Wilson CI: [</span><span class="si">{</span><span class="n">ci_wilson_estimate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">ci_wilson_estimate</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">]</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># 3. Jeffreys method
</span><span class="n">ci_jeffreys_estimate</span> <span class="o">=</span> <span class="nf">proportion_confint</span><span class="p">(</span><span class="n">cases</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">jeffreys</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">95% Jeffreys CI: [</span><span class="si">{</span><span class="n">ci_jeffreys_estimate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">ci_jeffreys_estimate</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">]</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <h3 id="output">Output:</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">95</span><span class="o">%</span> <span class="n">Wald</span> <span class="n">CI</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0015</span><span class="p">]</span>
<span class="mi">95</span><span class="o">%</span> <span class="n">Wilson</span> <span class="n">CI</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.0028</span><span class="p">]</span>
<span class="mi">95</span><span class="o">%</span> <span class="n">Jeffreys</span> <span class="n">CI</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.0023</span><span class="p">]</span>
</code></pre></div></div> <h3 id="key-observations">Key Observations</h3> <ul> <li><strong>Wald method</strong> technically stays within the [0,1] range, but its lower bound rounds to zero, reflecting how poorly it behaves for very small proportions.</li> <li><strong>Wilson method</strong> corrects for this issue and offers a wider, safer interval that remains fully positive.</li> <li><strong>Jeffreys method</strong> also produces a valid and slightly narrower interval compared to Wilson, benefiting from Bayesian adjustments that naturally avoid the zero-boundary problem.</li> </ul> <p>This small example already shows a clear trend: for rare events (like in rare disease epidemiology), robust methods like Wilson and Jeffreys provide more reliable and interpretable intervals than the traditional Wald method.</p> <p>In the next section, we’ll benchmark these methods more systematically using simulated datasets across a range of sample sizes and true proportions.</p> <hr/> <h2 id="comparing-the-methods"><strong>Comparing the Methods</strong></h2> <p>To systematically evaluate the performance of the Wald, Wilson, and Jeffreys methods, we conducted a benchmark analysis using simulated data designed to mimic real-world rare disease studies.</p> <p>We intentionally defined:</p> <ul> <li><strong>Sample sizes</strong> ranging from 30 to 30,000, representing single-center cohorts up to large national registries.</li> <li><strong>True event proportions</strong> ranging from 0.0005 to 0.02, reflecting the extremely low rates typical in rare diseases.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define simulation parameters
</span><span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">30000</span><span class="p">]</span>
<span class="n">true_proportion</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0005</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">]</span>
<span class="n">n_simulations</span> <span class="o">=</span> <span class="mi">1000</span>
</code></pre></div></div> <h3 id="performance-metrics"><strong>Performance Metrics</strong></h3> <p>We evaluated each method based on two key metrics:</p> <ul> <li><strong>Coverage Probability:</strong> How often the 95% confidence interval contains the true proportion (i.e., accuracy).</li> <li><strong>Average Confidence Interval Width:</strong> Reflecting the precision of the estimate (narrower is better, assuming coverage is adequate).</li> </ul> <p>This setup allows us to explore an important practical question:</p> <blockquote> <p>Which method strikes the best balance between accuracy and precision, especially under challenging rare disease conditions?</p> </blockquote> <hr/> <h3 id="coverage-probability"><strong>Coverage Probability</strong></h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/barplot_coverage_facet_tall.png" sizes="95vw"/> <img src="/assets/img/barplot_coverage_facet_tall.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The figure above shows how the coverage probability varies across different sample sizes and true proportions for the three methods: Wald, Wilson, and Jeffreys. The dashed red line marks the target 95% coverage.</p> <h4 id="key-observations-1"><strong>Key Observations:</strong></h4> <ul> <li><strong>Jeffreys Method:</strong> <ul> <li>Jeffreys consistently maintains or exceeds 95% coverage across all combinations of sample sizes and true proportions.</li> <li>Even in very small samples (e.g., n=30) and extremely low proportions (e.g., 0.0005), Jeffreys remains robust.</li> <li>This highlights its strength for rare event scenarios where maintaining coverage is critical.</li> </ul> </li> <li><strong>Wilson Method:</strong> <ul> <li>Wilson performs comparably to Jeffreys in many scenarios, particularly as sample size increases.</li> <li>However, at very small sample sizes (e.g., n=30, 100) and very rare proportions (e.g., 0.0005–0.001), Wilson coverage sometimes drops slightly below 95%.</li> <li>It becomes increasingly reliable as sample size grows.</li> </ul> </li> <li><strong>Wald Method:</strong> <ul> <li>Wald shows poor coverage in almost all small-to-moderate sample size settings, often falling far below the 95% target.</li> <li>At very low true proportions (e.g., 0.005 or lower), even increasing sample size to 1000 does not fully rescue the Wald method.</li> <li>Only with very large sample sizes (n=30,000) and higher true proportions (e.g., 0.02) does Wald approach acceptable coverage.</li> </ul> </li> </ul> <h4 id="additional-nuances"><strong>Additional Nuances:</strong></h4> <ul> <li><strong>Effect of True Proportion:</strong> <ul> <li>Lower true proportions (e.g., 0.0005, 0.001) make it harder for methods to maintain ideal coverage, especially noticeable for Wald and, to a lesser extent, Wilson.</li> <li>As true proportion increases (e.g., 0.02), all methods tend to perform better, but differences still remain.</li> </ul> </li> <li><strong>Effect of Sample Size:</strong> <ul> <li>Larger sample sizes systematically improve coverage for all methods, but Wald still lags behind unless the sample size is very large.</li> </ul> </li> </ul> <h4 id="practical-implications"><strong>Practical Implications:</strong></h4> <ul> <li><strong>Jeffreys method</strong> is the most dependable choice for rare event settings where preserving nominal confidence levels is crucial.</li> <li><strong>Wilson method</strong> is a strong practical alternative, especially in moderate-to-large sample settings, offering better behavior than Wald.</li> <li><strong>Wald method</strong> should generally be avoided for rare disease applications unless working with extremely large cohorts and relatively higher event proportions.</li> </ul> <hr/> <h3 id="average-confidence-interval-width"><strong>Average Confidence Interval Width</strong></h3> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/barplot_precision_facet.png" sizes="95vw"/> <img src="/assets/img/barplot_precision_facet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>95% CI width by methods controlling for the sample sizes and true proportions</p> <p>The figure above illustrates the average length of 95% confidence intervals across different sample sizes and true proportions, comparing the Wald, Wilson, and Jeffreys methods.</p> <h4 id="key-observations-2"><strong>Key Observations:</strong></h4> <ul> <li><strong>Wald Method:</strong> <ul> <li>Wald consistently produces the narrowest confidence intervals across all settings.</li> <li>However, given Wald’s poor coverage performance (as seen in the previous figure), these narrower intervals are misleading — they falsely suggest higher precision while actually underestimating the true uncertainty.</li> <li>Thus, the apparent precision of Wald should be interpreted with caution: it’s a <em>false sense</em> of accuracy.</li> </ul> </li> <li><strong>Wilson and Jeffreys Methods:</strong> <ul> <li>Wilson intervals are wider than Jeffreys across almost all settings, particularly noticeable when both sample size is small and true proportion is low.</li> <li>Jeffreys consistently achieves <strong>slightly narrower intervals</strong> than Wilson while still maintaining better or comparable coverage.</li> <li>This suggests Jeffreys method offers <strong>superior precision among the reliable methods</strong>.</li> </ul> </li> <li><strong>Effect of Sample Size:</strong> <ul> <li>As sample size increases, the width of the confidence intervals shrinks for all methods, as expected.</li> <li>When sample size reaches 30,000, all methods yield very narrow intervals, and differences between methods become minimal.</li> </ul> </li> <li><strong>Effect of True Proportion:</strong> <ul> <li>Larger true proportions (e.g., 0.02) generally lead to wider intervals compared to extremely low proportions (e.g., 0.0005), because variance is inherently higher when the probability is farther from 0 or 1.</li> </ul> </li> </ul> <hr/> <h2 id="practical-considerations"><strong>Practical Considerations</strong></h2> <p>When selecting a method to construct confidence intervals for proportions—especially in rare disease contexts—it is crucial to match the method to the study’s characteristics. Below are key guidelines and common pitfalls to watch for:</p> <p><strong>Choosing the Appropriate Method</strong></p> <table> <thead> <tr> <th><strong>Scenario</strong></th> <th><strong>Recommended Method</strong></th> <th><strong>Reason</strong></th> </tr> </thead> <tbody> <tr> <td>Very small sample size (n ≤ 100) and rare events</td> <td>Jeffreys</td> <td>Best coverage; robust even with extreme rarity</td> </tr> <tr> <td>Moderate sample size (n ~ 300–1000), low prevalence</td> <td>Jeffreys or Wilson</td> <td>Both offer reliable coverage; Jeffreys slightly narrower intervals</td> </tr> <tr> <td>Large sample size (n ≥ 10,000), moderate proportions (e.g., 0.01–0.02)</td> <td>Wilson or Jeffreys</td> <td>Minor differences; both safe choices</td> </tr> <tr> <td>Extremely large sample size (n ≥ 30,000) with not-so-rare events</td> <td>Wilson (or even Wald)</td> <td>Wald acceptable only under these conditions</td> </tr> </tbody> </table> <h3 id="potential-pitfalls-and-how-to-avoid-them"><strong>Potential Pitfalls and How to Avoid Them</strong></h3> <ul> <li><strong>Pitfall 1: Blindly Using the Wald Method</strong> <ul> <li><strong>Issue:</strong> Wald intervals can be too narrow and may miss the true proportion, especially with small samples or low event rates.</li> <li><strong>Solution:</strong> Prefer Wilson or Jeffreys methods for rare events or small sample studies.</li> </ul> </li> <li><strong>Pitfall 2: Overemphasizing Narrow Intervals</strong> <ul> <li><strong>Issue:</strong> A narrow interval is meaningless if coverage probability is poor (i.e., interval doesn’t capture the true proportion reliably).</li> <li><strong>Solution:</strong> Prioritize methods with strong coverage (like Jeffreys and Wilson) before focusing on interval width.</li> </ul> </li> <li><strong>Pitfall 3: Assuming “One Size Fits All”</strong> <ul> <li><strong>Issue:</strong> The best method depends on both sample size and true event rate.</li> <li><strong>Solution:</strong> Tailor your method choice based on study characteristics (refer to the table above).</li> </ul> </li> </ul> <hr/> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>Choosing the right method to estimate confidence intervals for proportions is critical—especially when studying rare events in small cohorts, as is often the case in rare disease research.</p> <p>This analysis highlights that:</p> <ul> <li><strong>Jeffreys method</strong> consistently delivers the best balance: reliable coverage and high precision across challenging scenarios.</li> <li><strong>Wilson method</strong> remains a strong, practical alternative, particularly when ease of interpretation or computational simplicity is needed.</li> <li><strong>Wald method</strong>, despite its simplicity, should be avoided unless working with extremely large samples and relatively high event rates.</li> </ul> <p>In rare disease studies where sample sizes are often small and events are extremely rare, robust methods like <strong>Jeffreys</strong> or <strong>Wilson</strong> are not just preferable—they are essential for trustworthy statistical inference.</p> <p>When precision matters, and lives or health policy decisions could depend on these estimates, <strong>choose your method thoughtfully</strong>.</p> <p>Good science begins with good measurement.</p> <hr/> <h2 id="putting-it-into-practice"><strong>Putting It Into Practice</strong></h2> <p>To help you apply these methods in your own work, I’ve prepared clean, reusable Python code that:</p> <ul> <li>Calculates Wald, Wilson, and Jeffreys confidence intervals</li> <li>Benchmarks performance across different sample sizes and true proportions</li> <li>Summarizes key metrics like coverage probability and interval width</li> </ul> <p>You can easily adapt the code to:</p> <ul> <li>Analyze your own datasets</li> <li>Test other confidence levels (e.g., 90%, 99%)</li> <li>Extend the benchmarking to new methods</li> </ul> <p>Whether you’re working on rare disease epidemiology, microbiome studies, or any research involving proportions, these code snippets are ready to plug into your analysis.</p> <p>Access the full code <a href="https://github.com/davidzhao1015/interval-estimate-benchmark/blob/main/compare-interval-est-methods.py">here</a>.</p>]]></content><author><name></name></author><category term="statistics"/><category term="confidence-intervals,"/><category term="statistics,"/><category term="proportions,"/><category term="methods-comparison"/><summary type="html"><![CDATA[How much can you trust a number when dealing with rare events? In statistics, it’s not just about getting an estimate — it’s about knowing how uncertain that estimate really is.]]></summary></entry><entry><title type="html">Modeling Diagnostic Delays in Rare Disease: A Survival Analysis Case Study in Python</title><link href="https://davidzhao1015.github.io/blog/2025/delay-diganosis-survival-analysis/" rel="alternate" type="text/html" title="Modeling Diagnostic Delays in Rare Disease: A Survival Analysis Case Study in Python"/><published>2025-03-03T00:00:00+00:00</published><updated>2025-03-03T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/delay-diganosis-survival-analysis</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/delay-diganosis-survival-analysis/"><![CDATA[<h1 id="introduction-why-survival-analysis">Introduction: Why Survival Analysis?</h1> <p>Rare diseases often come with a hidden burden—the <strong>delay in diagnosis</strong>. For patients with <strong>acid sphingomyelinase deficiency (ASMD)</strong>, a genetic disorder with highly variable onset and progression, the diagnostic journey can be long and uncertain. Those with <strong>chronic neurovisceral ASMD (NPD A/B)</strong> and <strong>chronic visceral ASMD (NPD B)</strong> may face <strong>years</strong> before receiving a definitive diagnosis, impacting their treatment options and long-term health outcomes.</p> <p>But how long do these delays typically last? Can we <strong>quantify</strong> the diagnostic journey and predict the likelihood of diagnosis at different time points?</p> <p>This is where <strong>survival analysis</strong> comes in. By modeling <strong>time-to-diagnosis</strong>, we can estimate <strong>median diagnostic timelines</strong>, compare different patient subgroups, and even predict when a patient is most likely to receive a diagnosis based on their clinical history. In this article, I’ll walk through the <strong>fundamentals of survival analysis</strong> and share how I applied it to ASMD patient data using Python.</p> <p>You can explore the full <a href="https://github.com/davidzhao1015/survival-analysis-case-example/blob/main/survival-analysis-ASMD-example.ipynb">notebook</a> and <a href="https://github.com/davidzhao1015/survival-analysis-case-example/blob/main/age-at-diagnosis-asmd.xlsx">dataset</a> to try the analysis yourself 🚀</p> <h2 id="my-learning-journey-in-survival-analysis">My Learning Journey in Survival Analysis</h2> <h3 id="why-i-started-exploring-survival-analysis">Why I started exploring survival analysis</h3> <p>I became interested in survival analysis because it answers a critical question: <strong>How likely is an event (such as diagnosis) to occur at a given time point, considering different covariates?</strong></p> <p>This type of analysis is widely used in: • <strong>Disease progression modeling</strong> – Understanding how long it takes for a condition to worsen. • <strong>Precision medicine</strong> – Predicting patient outcomes based on medical history. • <strong>Clinical trials</strong> – Estimating time-to-recovery, disease recurrence, or survival rates.</p> <p>Unlike conventional regression models, <strong>survival analysis can handle censored data</strong>—cases where the event of interest hasn’t occurred yet by the time of data collection. This makes it a powerful tool in both medical research and epidemiology.</p> <h3 id="key-takeaways-from-my-learning-process">Key takeaways from my learning process</h3> <p>As I explored survival analysis, I focused on three major areas:</p> <ol> <li><strong>Understanding the statistical foundations</strong> <ul> <li>The concept of <strong>censored data</strong> and why it’s important.</li> <li>Common survival models: <strong>Kaplan-Meier curves, parametric regression models, and Cox proportional hazards models</strong>.</li> <li><strong>Model selection</strong> using AIC to determine the best fit.</li> </ul> </li> <li><strong>Implementing survival analysis in Python</strong> <ul> <li>Learning by reproducing examples from <strong>tutorials and documentation</strong>.</li> <li>Using the <strong>lifelines</strong> library for survival modeling and visualization.</li> <li>Writing clean, reusable code for different types of survival models.</li> </ul> </li> <li><strong>Applying knowledge to a real-world case study</strong> <ul> <li>Choosing the right statistical models: <strong>non-parametric, semi-parametric, or parametric</strong>.</li> <li>Interpreting model coefficients <strong>in the context of a medical study</strong>.</li> <li>Creating <strong>effective and insightful plots</strong> to communicate results.</li> </ul> </li> </ol> <h3 id="how-this-method-applies-beyond-asmd">How this method applies beyond ASMD</h3> <p>While this case study focuses on <strong>time-to-diagnosis in ASMD</strong>, the approach can be generalized to many other scenarios:</p> <ul> <li>The <strong>analysis workflow</strong> can be applied to different time-to-event studies.</li> <li>The <strong>parametric models</strong> used in survival analysis can be adjusted for other medical research questions.</li> <li>The <strong>Python code</strong> can be repurposed for different datasets with minimal modifications.</li> </ul> <h3 id="what-this-post-covers">What this post covers</h3> <p>In this blog post, I’ll share:</p> <p>✅ A <strong>beginner-friendly introduction</strong> to survival analysis.</p> <p>✅ <strong>Helpful resources</strong> for learning survival analysis and Python implementation.</p> <p>✅ A <strong>step-by-step case study</strong> using ASMD diagnosis data.</p> <p>🚀 For the full code and interactive analysis, check out my <a href="https://github.com/davidzhao1015/survival-analysis-case-example/blob/main/survival-analysis-ASMD-example.ipynb">Jupyter Notebook</a></p> <p>Let’s dive in!</p> <h1 id="fundamentals-of-survival-analysis">Fundamentals of Survival Analysis</h1> <h2 id="what-is-survival-analysis">What is survival analysis?</h2> <p>Survival analysis is a statistical approach used to model <strong>time-to-event data</strong>—where the outcome of interest is the time until an event occurs. This event could be <strong>time to disease relapse</strong>, <strong>time until a diagnosis</strong>, or even <strong>time to treatment failure</strong>. Despite the name, survival analysis isn’t just about survival—it’s about understanding when an event is likely to happen.</p> <p>One of the key advantages of survival analysis is its ability to handle <strong>censored data</strong>—cases where:</p> <ul> <li><strong>Left censoring</strong> occurs when the event happened before the study began, but the exact time is unknown.</li> <li><strong>Right censoring</strong> happens when the event has not yet occurred by the end of the study period.</li> </ul> <p>Traditional regression models struggle with these challenges, but survival models are specifically designed to <strong>account for incomplete data</strong>, making them essential in epidemiology, clinical research, and beyond.</p> <h2 id="key-concepts-explained-simply">Key concepts explained simply</h2> <p><strong>Censoring</strong>: Not every subject in a study experiences the event of interest. Censoring allows us to <strong>include incomplete observations</strong>, making survival analysis more robust than conventional regression methods.</p> <p><strong>Kaplan-Meier Curve</strong>: A <strong>Kaplan-Meier curve</strong> provides a visual representation of survival probabilities over time. It plots:</p> <ul> <li><strong>Time (X-axis)</strong> vs. <strong>Probability of event-free survival (Y-axis)</strong>.</li> <li>It helps estimate how likely the event (e.g., diagnosis) will occur by a given time.</li> </ul> <p><strong>Log-Rank Test</strong>: A statistical test used to <strong>compare survival distributions</strong> between two or more independent groups. For example, it helps determine whether patients with different ASMD subtypes experience significantly different diagnostic delays.</p> <p><strong>Cox Proportional Hazards Model</strong>: A regression model that evaluates the <strong>effect of multiple variables</strong> on survival time. It helps answer questions like:</p> <ul> <li>Do certain clinical factors increase or decrease the likelihood of earlier diagnosis?</li> <li>How do different ASMD subtypes compare in terms of diagnostic delay?</li> </ul> <h2 id="resources-i-found-helpful">Resources I found helpful</h2> <p>If you’re new to survival analysis, these resources helped me grasp the fundamentals and apply them in Python:</p> <p>📺 <strong>Video Tutorial Series:</strong> <a href="https://www.youtube.com/watch?v=Wo9RNcHM_bs">Survival Analysis by DATAtab</a></p> <p>📖 <strong>Python Documentation:</strong> <a href="https://lifelines.readthedocs.io/">Lifelines Survival Analysis Library</a></p> <h1 id="asmd-case-study-analyzing-time-to-diagnosis">ASMD Case Study: Analyzing Time-to-Diagnosis</h1> <h2 id="understanding-the-dataset">Understanding the dataset</h2> <p>For this analysis, the age-at-diagnosis data derived from a <a href="https://www.sciencedirect.com/science/article/pii/S1096719216300580">scientific publication</a> by Cassiman et al. (2016) on ASMD patient outcomes.</p> <p>Each row in the dataset represents an <strong>individual patient</strong>, with the following key variables:</p> <ul> <li><strong>Time since birth (years):</strong> Age at which an event (diagnosis) occurs.</li> <li><strong>Diagnosis (event):</strong> Binary indicator of whether the patient was diagnosed (<strong>1 = diagnosed, 0 = censored</strong>).</li> <li><strong>AB subtype:</strong> Indicates disease type (<strong>0 = Type B, 1 = Type AB</strong>).</li> <li><strong>CH subtype:</strong> Indicates age group (<strong>0 = Child, 1 = Adult</strong>).</li> </ul> <p>Since some patients remained undiagnosed at the time of data collection, <strong>censored data</strong> is present—making survival analysis an ideal approach for estimating diagnostic timelines.</p> <p>👉 Next, we apply survival analysis techniques to uncover diagnostic trends and patterns.</p> <h2 id="why-survival-analysis-fits-this-problem">Why survival analysis fits this problem</h2> <p>One of the key challenges in studying <strong>diagnostic delays in ASMD</strong> is that different patient subgroups likely experience <strong>varying timelines</strong> before receiving a diagnosis. Some individuals may be diagnosed early, while others remain undiagnosed for an extended period. This variability makes it difficult to analyze the data using conventional statistical methods.</p> <p>Additionally, not all patients in the dataset have received a diagnosis at the time of study. These <strong>“still undiagnosed”</strong> cases are what we call <strong>censored data</strong>—we know that their diagnosis hasn’t happened yet, but we don’t know exactly when it will occur. Traditional regression models struggle to handle this type of incomplete data, which is where <strong>survival analysis excels</strong>.</p> <ul> <li><strong>Captures time-to-event data:</strong> Instead of treating diagnosis as a simple yes/no outcome, survival analysis allows us to model <strong>when</strong> the diagnosis occurs.</li> <li><strong>Handles censored data effectively:</strong> Patients who haven’t been diagnosed yet aren’t excluded from the analysis—they are incorporated appropriately using survival functions.</li> <li><strong>Compares different patient subgroups:</strong> By applying Kaplan-Meier curves and Cox regression models, we can compare <strong>diagnostic delays</strong> between ASMD subtypes.</li> </ul> <p>By leveraging survival analysis, we can <strong>estimate the probability of diagnosis over time</strong>, understand which patients face the longest delays, and potentially identify clinical factors that contribute to earlier or later diagnosis.</p> <h2 id="applying-survival-analysis-to-asmd-data">Applying Survival Analysis to ASMD Data</h2> <h3 id="step-by-step-survial-analysis-in-python">Step-by-Step Survial Analysis in Python</h3> <p><strong>🗃️ Data Loading and Exploration</strong></p> <ul> <li>Loaded age-at-diagnosis data from Excel using pandas.</li> <li>Each row represented a patient, with columns for age at diagnosis, event occurrence (diagnosed or not), and ASMD subtype indicators (AB, CH).</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">diagnosis_age_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_excel</span><span class="p">(</span><span class="sh">"</span><span class="s">age-at-diagnosis-asmd.xlsx</span><span class="sh">"</span><span class="p">)</span>
<span class="n">diagnosis_age_df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div> <table> <thead> <tr> <th>Time since birth (year)</th> <th>Diagnosis</th> <th>AB</th> <th>CH</th> </tr> </thead> <tbody> <tr> <td>0.10</td> <td>1</td> <td>1</td> <td>0</td> </tr> <tr> <td>0.28</td> <td>1</td> <td>1</td> <td>0</td> </tr> <tr> <td>0.36</td> <td>1</td> <td>1</td> <td>0</td> </tr> <tr> <td>0.45</td> <td>1</td> <td>1</td> <td>0</td> </tr> <tr> <td>0.45</td> <td>1</td> <td>1</td> <td>0</td> </tr> </tbody> </table> <p>📊 <strong>Kaplan-Meier Survival Curves</strong></p> <ul> <li>Used lifelines’ KaplanMeierFitter to visualize survival (i.e., undiagnosed) probability over time.</li> <li>Stratified patients by subtype: <ul> <li>Type B Adult: AB = 0, CH = 0</li> <li>Type B Child: AB = 0, CH = 1</li> <li>Type AB: AB = 1</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">lifelines</span> <span class="kn">import</span> <span class="n">KaplanMeierFitter</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">kmf</span> <span class="o">=</span> <span class="nc">KaplanMeierFitter</span><span class="p">()</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">diagnosis_age_df</span><span class="p">[</span><span class="sh">"</span><span class="s">Time since birth (year)</span><span class="sh">"</span><span class="p">]</span>
<span class="n">E</span> <span class="o">=</span> <span class="n">diagnosis_age_df</span><span class="p">[</span><span class="sh">"</span><span class="s">Diagnosis</span><span class="sh">"</span><span class="p">]</span>

<span class="n">type_b_adult</span> <span class="o">=</span> <span class="p">(</span><span class="n">diagnosis_age_df</span><span class="p">[</span><span class="sh">"</span><span class="s">AB</span><span class="sh">"</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">diagnosis_age_df</span><span class="p">[</span><span class="sh">"</span><span class="s">CH</span><span class="sh">"</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">type_b_child</span> <span class="o">=</span> <span class="p">(</span><span class="n">diagnosis_age_df</span><span class="p">[</span><span class="sh">"</span><span class="s">AB</span><span class="sh">"</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">diagnosis_age_df</span><span class="p">[</span><span class="sh">"</span><span class="s">CH</span><span class="sh">"</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">type_ab</span> <span class="o">=</span> <span class="p">(</span><span class="n">diagnosis_age_df</span><span class="p">[</span><span class="sh">"</span><span class="s">AB</span><span class="sh">"</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>

<span class="n">kmf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="n">type_b_adult</span><span class="p">],</span> <span class="n">E</span><span class="p">[</span><span class="n">type_b_adult</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Type B Adult</span><span class="sh">"</span><span class="p">).</span><span class="nf">plot_survival_function</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">kmf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="n">type_b_child</span><span class="p">],</span> <span class="n">E</span><span class="p">[</span><span class="n">type_b_child</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Type B Child</span><span class="sh">"</span><span class="p">).</span><span class="nf">plot_survival_function</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">kmf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="n">type_ab</span><span class="p">],</span> <span class="n">E</span><span class="p">[</span><span class="n">type_ab</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Type AB</span><span class="sh">"</span><span class="p">).</span><span class="nf">plot_survival_function</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Kaplan-Meier Curve of Age at Diagnosis by Subtypes</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time since birth (year)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Survival Probability</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/survival_KM_curve.png" sizes="95vw"/> <img src="/assets/img/survival_KM_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 1. The Kaplan-Meier plot for the three subtypes (Type B Adult, Type B Child, Type AB)</p> <p>Key observations:</p> <ul> <li>Type AB and Type B Child are diagnosed early: <ul> <li>Steep drop in survival curves before age 5</li> <li>Most diagnoses occur in early childhood</li> </ul> </li> <li>Type B Adult experiences delayed diagnosis: <ul> <li>Gradual decline in survival curve over several decades</li> <li>Many are diagnosed in middle age or later</li> </ul> </li> <li>Subtypes show clear separation: <ul> <li>Distinct survival curves across groups</li> <li>Statistically significant differences confirmed by log-rank test</li> </ul> </li> </ul> <p>📈 <strong>Statistical Comparison</strong></p> <p>Conducted a log-rank test to compare survival curves between subtypes and assess whether diagnostic delays differed significantly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">lifelines</span> <span class="kn">import</span> <span class="n">WeibullFitter</span><span class="p">,</span> <span class="n">ExponentialFitter</span><span class="p">,</span> <span class="n">LogNormalFitter</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">WeibullFitter</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">E</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Weibull</span><span class="sh">"</span><span class="p">),</span>
    <span class="nc">ExponentialFitter</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">E</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Exponential</span><span class="sh">"</span><span class="p">),</span>
    <span class="nc">LogNormalFitter</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">E</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Log-Normal</span><span class="sh">"</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>

<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">plot_survival_function</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">aic</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">AIC_</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mf">0.8</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">models</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="n">model</span><span class="p">),</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">_label</span><span class="si">}</span><span class="s"> AIC: </span><span class="si">{</span><span class="n">aic</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Model Comparison by AIC</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time since birth (year)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Survival Probability</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/survival_AIC.png" sizes="95vw"/> <img src="/assets/img/survival_AIC.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 2. The survival functions overlaid with AIC values.</p> <p>🧮 <strong>Parametric Model Fitting</strong></p> <ul> <li>Fit various models including Weibull, Log-Normal, Exponential, Log-Logistic, and Generalized Gamma.</li> <li>Compared model fit using AIC (Akaike Information Criterion).</li> <li>Selected Log-Normal AFT as the best-performing model.</li> </ul> <p>🔍 <strong>Prediction by Subtype</strong></p> <ul> <li>Applied the best model to predict survival (undiagnosed) probability curves for each subtype across a 0–100 year timespan.</li> <li>Visualized predictions with subtype-specific curves—helpful for clinicians and researchers interpreting diagnosis timelines.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">lifelines</span> <span class="kn">import</span> <span class="n">LogNormalAFTFitter</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">lognorm_aft</span> <span class="o">=</span> <span class="nc">LogNormalAFTFitter</span><span class="p">()</span>
<span class="n">lognorm_aft</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">diagnosis_age_df</span><span class="p">,</span> <span class="n">duration_col</span><span class="o">=</span><span class="sh">"</span><span class="s">Time since birth (year)</span><span class="sh">"</span><span class="p">,</span> <span class="n">event_col</span><span class="o">=</span><span class="sh">"</span><span class="s">Diagnosis</span><span class="sh">"</span><span class="p">)</span>

<span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">"</span><span class="s">AB</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">CH</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]})</span>
<span class="n">times</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="n">survival_probs</span> <span class="o">=</span> <span class="n">lognorm_aft</span><span class="p">.</span><span class="nf">predict_survival_function</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="n">times</span><span class="o">=</span><span class="n">times</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">survival_probs</span><span class="p">.</span><span class="nf">plot</span><span class="p">()</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">"</span><span class="s">Type B Adult</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Type B Child</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Type AB</span><span class="sh">"</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Predicted Survival Curve – Log-Normal AFT</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time since birth (year)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Survival Probability</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/survival_prediction.png" sizes="95vw"/> <img src="/assets/img/survival_prediction.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Figure 3. The predicted survival function showing how diagnosis probability changes by subtype.</p> <h2 id="findings--implications">Findings &amp; Implications</h2> <p>This analysis showed clear differences in time-to-diagnosis across ASMD subtypes. Kaplan-Meier curves revealed that Type AB and childhood-onset Type B patients were diagnosed earlier, while adult-onset Type B cases experienced the longest delays. The Log-Normal AFT model provided the best fit for modeling these differences and enabled prediction of diagnosis probabilities by subtype.</p> <p>While this example focuses on ASMD, the same survival modeling techniques can be adapted to other rare diseases to quantify diagnostic delays, compare patient subgroups, or support screening research. The methods demonstrated here—handling censored data, fitting multiple survival models, and interpreting time-to-event probabilities—are broadly useful in medical data analysis.</p> <h1 id="4-key-learning--takeaways">4. Key learning &amp; takeaways</h1> <p>Here are some key lessons I gained while working through this survival analysis project:</p> <p>What I Learned About Survival Analysis</p> <ul> <li>There are three major categories of survival models: <ul> <li>Non-parametric (e.g., Kaplan-Meier): flexible, assumption-free, and good for descriptive analysis</li> <li>Parametric (e.g., Weibull, Log-Normal): useful for prediction and interpretation when assumptions are met</li> <li>Semi-parametric (e.g., Cox regression): interpretable and flexible for covariates, without requiring full distributional assumptions</li> </ul> </li> <li>AIC (Akaike Information Criterion) is a valuable tool for selecting the best-fit model among several options.</li> <li>When visualizing survival curves with lifelines, it’s helpful to: <ul> <li>Include 95% confidence intervals to convey uncertainty</li> <li>Display at-risk tables to show how many events (diagnoses or censored cases) are observed over time</li> </ul> </li> </ul> <p>Practical Challenges and How I Overcame Them</p> <ul> <li>Survival analysis differs conceptually from typical regression. Grasping its unique assumptions and knowing how to interpret coefficients was essential to making sense of the results.</li> <li>Implementing survival models in Python involved trial and error. I relied on: <ul> <li>lifelines documentation</li> <li>Reproducing basic examples</li> <li>Experimentation and patience when adapting methods to real-world data, which is often messier than toy examples</li> </ul> </li> </ul> <p>Tips for Beginners</p> <ul> <li>Start with the fundamentals: understand basic statistical concepts and what survival analysis is trying to solve.</li> <li>Gain working knowledge of key survival algorithms (Kaplan-Meier, Cox, Log-Normal, etc.).</li> <li>Be patient—coding survival models takes attention to detail and a willingness to try, tweak, and retry. Real-world datasets rarely behave like textbook examples, so iteration is part of the process.</li> </ul>]]></content><author><name></name></author><category term="statistics"/><category term="survival-analysis;"/><category term="rare-disease;"/><category term="ASMD;"/><category term="python"/><summary type="html"><![CDATA[Introduction: Why Survival Analysis?]]></summary></entry><entry><title type="html">From Microbiology to Bioinformatics: How Embracing New Skills Transformed My Career</title><link href="https://davidzhao1015.github.io/blog/2025/my-career-journey/" rel="alternate" type="text/html" title="From Microbiology to Bioinformatics: How Embracing New Skills Transformed My Career"/><published>2025-02-22T00:00:00+00:00</published><updated>2025-02-22T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/my-career-journey</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/my-career-journey/"><![CDATA[<div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/career-transform-infographics.png" sizes="95vw"/> <img src="/assets/img/career-transform-infographics.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="introduction"><strong>Introduction</strong></h2> <p>Have you ever considered how embracing a new skill set could transform your career?</p> <p>Transitioning from one field to another can be daunting, but it’s also an incredible opportunity to reinvent yourself and explore uncharted territory.</p> <p>For me, the journey from microbiology to bioinformatics has been profoundly transformative, teaching me the power of blending experimental and computational sciences to tackle complex biological challenges.</p> <h2 id="from-microbiology-to-bioinformatics-a-natural-evolution"><strong>From Microbiology to Bioinformatics: A Natural Evolution</strong></h2> <p>My career began with a PhD in Food Science, focusing on the genomic and phenotypic adaptation of <em>Lactobacillus reuteri</em> in fermented foods like sourdough. Early on, I was captivated by how microorganisms influence health and nutrition.</p> <p>This fascination deepened during my PhD, where collaborative comparative genomics projects highlighted the transformative role of bioinformatics in modern research. These experiences sowed the seeds for my eventual shift to computational biology.</p> <h2 id="embracing-change-the-catalyst-for-my-transition"><strong>Embracing Change: The Catalyst for My Transition</strong></h2> <p>A career break for family care and the constraints of the pandemic pushed me to re-evaluate my professional goals. These circumstances highlighted the growing importance of bioinformatics skills, motivating me to embrace this field fully.</p> <p>The transition felt daunting, but it also presented an exciting opportunity to expand my expertise and tackle new challenges.</p> <h2 id="building-skills-through-a-structured-approach"><strong>Building Skills Through a Structured Approach</strong></h2> <p>To bridge the gap, I followed a three-step learning process that I still recommend:</p> <p>1️⃣ <strong>Build the Basics</strong>: I used platforms like Coursera and DataCamp to gain foundational knowledge in R programming and machine learning.</p> <p>2️⃣ <strong>Practice Through Projects</strong>: Guided mini-projects allowed me to apply my skills to real-world problems.</p> <p>3️⃣ <strong>Real-World Applications</strong>: Working on clinical and microbiome datasets helped me tackle unstructured challenges while honing my programming expertise.</p> <p>These steps empowered me to work in areas like microbial genomics, software development for metabolomics, and statistical modeling of complex datasets.</p> <h2 id="bridging-disciplines-where-the-magic-happens"><strong>Bridging Disciplines: Where the Magic Happens</strong></h2> <p>What distinguishes my journey is the ability to bridge microbiology and bioinformatics.</p> <p>My background in experimental microbiology allows me to understand raw data, such as sequencing or qPCR outputs, while my computational skills enable me to derive actionable insights.</p> <p>This interdisciplinary approach has been pivotal in interpreting infant microbiome data and tackling complex research questions.</p> <h2 id="lessons-learned-and-advice-for-aspiring-bioinformaticians"><strong>Lessons Learned and Advice for Aspiring Bioinformaticians</strong></h2> <p>Transitioning to a new field requires resilience and adaptability. Here are my key lessons from this journey:</p> <p>🔑 Start small and maintain consistency in your learning.</p> <p>🔑 Utilize online resources and workshops to gain practical skills.</p> <p>🔑 Embrace your unique background as a strength; diversity in expertise drives innovation.</p> <p>For anyone interested in bioinformatics, this is an exciting time. Regardless of whether you come from a biological, statistical, or computational background, your skills are valuable.</p> <p>The key is to embrace continuous learning and foster interdisciplinary collaboration.</p> <h2 id="what-about-you"><strong>What About You?</strong></h2> <p>Have you ever transitioned to a new field? How did you overcome challenges and embrace new opportunities?</p> <p>I’d love to hear about your journey—please share your experiences in the comments!</p>]]></content><author><name></name></author><category term="career development"/><category term="bionformatics,"/><category term="microbiology,"/><category term="career-transition,"/><category term="learning-journey"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Beyond Heatmaps: Mapping Two Variables with Plotly in Public Health</title><link href="https://davidzhao1015.github.io/blog/2025/plotly-maps-public-health/" rel="alternate" type="text/html" title="Beyond Heatmaps: Mapping Two Variables with Plotly in Public Health"/><published>2025-01-27T00:00:00+00:00</published><updated>2025-01-27T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/plotly-maps-public-health</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/plotly-maps-public-health/"><![CDATA[<h2 id="background">Background</h2> <p>In public health and epidemiology, data visualization isn’t just about aesthetics—it’s about clarity, impact, and storytelling. Choropleth maps, which color-code geographic areas based on variable intensity, are a go-to tool for illustrating spatial trends in disease burden, health behaviors, or population risk factors.</p> <p>But what if you want to compare <strong>two variables simultaneously</strong> on the same geographic map?</p> <p>This is a common challenge in health data storytelling. For instance, imagine you’re studying the <strong>distribution of a dietary habit</strong> and the <strong>associated disease outcomes</strong>—how do you present this relationship clearly and intuitively?</p> <p><strong>💡 Innovation Highlight</strong> Overlay a <strong>bubble map</strong> on top of a choropleth map. The base layer (choropleth) provides a regional context for one variable, while the overlaid bubbles show the intensity of another, allowing for quick visual comparisons.</p> <p>This post shows how to combine two variables on a single map using a dual-layer approach — choropleth for spatial intensity + bubble size for a second variable. It’s a simple but powerful storytelling technique.</p> <h3 id="-real-world-example">📌 Real-World Example</h3> <p>In 2020, a <a href="https://www.medrxiv.org/content/10.1101/2020.07.06.20147025v1">preprint</a> suggested that <strong>fermented vegetable consumption</strong> might be inversely associated with <strong>COVID-19 mortality</strong> in Europe—even after adjusting for confounding factors. I wanted to explore that hypothesis using public data and an interactive map.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/snapshort_final_map.png" sizes="95vw"/> <img src="/assets/img/snapshort_final_map.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This interactive map overlays fermented food intake (bubbles) on COVID-19 death rates (color). Scroll down to learn how it’s built.</p> <hr/> <h2 id="-quick-start">📦 <strong>Quick Start</strong></h2> <ol> <li>Install dependencies</li> <li>Download the dataset (<a href="https://github.com/davidzhao1015/plotly-bubble-choropleth/tree/main/input_csv">link</a>)</li> <li>Run the notebook <a href="https://mybinder.org/v2/gh/davidzhao1015/plotly-bubble-choropleth/main?urlpath=%2Fdoc%2Ftree%2Finteractive-map-covid-fermented-food_v3.ipynb"><img src="https://mybinder.org/badge_logo.svg" alt="Binder"/></a></li> <li>Explore the interactive map</li> </ol> <hr/> <h2 id="what-youll-need-to-recreate-this-map"><strong>What You’ll Need to Recreate This Map</strong></h2> <p>To recreate the interactive map and plots in this post, you’ll need a few Python packages commonly used in data science:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Core data handling
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="c1"># Interactive map visualization
</span><span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span> <span class="c1"># Creating choropleth and scatter_geo maps
</span><span class="kn">import</span> <span class="n">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span> <span class="c1"># Overlaying plot layers and annotation
</span>
<span class="c1"># For geospatial data 
</span><span class="kn">import</span> <span class="n">requests</span> <span class="c1"># Fetching GeoJSON data from remote URL 
</span><span class="kn">import</span> <span class="n">json</span> <span class="c1"># Parsing JSON data
</span><span class="kn">import</span> <span class="n">pycountry</span> <span class="c1"># Mapping country names to ISO alpha-3 codes
</span>
<span class="c1"># For exporting maps
</span><span class="kn">import</span> <span class="n">plotly.io</span> <span class="k">as</span> <span class="n">pio</span> <span class="c1"># Exporting Plotly figures as HTML
</span><span class="kn">import</span> <span class="n">kaleido</span>  <span class="c1"># Saving Plotly maps as static images (optional)
</span></code></pre></div></div> <p>💡 <strong>Tip:</strong> This tutorial uses Python packages like pandas, plotly, and pycountry. You can install all dependencies by running:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div> <p>Or check out the full notebook on <a href="https://github.com/yourusername/repo-name">GitHub</a> to explore the code and interactive map.</p> <h3 id="why-plotlyexpress"><strong>Why plotly.express?</strong></h3> <p>For this project, I used plotly.express to create <strong>interactive choropleth and bubble maps</strong>. Unlike traditional plotting libraries like matplotlib, plotly.express lets you explore maps by hovering, zooming, and clicking — all right inside your browser or Jupyter Notebook.</p> <p>It’s a powerful yet beginner-friendly way to bring geographic data to life.</p> <p>The official tutorials for choropleth are <a href="https://plotly.com/python/choropleth-maps/">here</a>, and for bubble map is <a href="https://plotly.com/python/bubble-maps/">here</a>.</p> <hr/> <h2 id="preparing-the-epidemiological-data"><strong>Preparing the Epidemiological Data</strong></h2> <p>Before visualizing our map, we first need to get the data into the right shape. In this case, we’re combining datasets on:</p> <ul> <li>COVID-19 mortality rates (per million people)</li> <li>Fermented food consumption (e.g. sauerkraut, pickled vegetables)</li> </ul> <p>Let’s walk through the key steps.</p> <p><strong>📂 1. Load Data from CSV</strong></p> <p>We read the data using pandas — a Python package that makes working with tables easy:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Load COVID-19 mortality rate dataset
</span><span class="n">COVID_death_pop_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">COVID_mortality_rate.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Load fermented vegetables dataset
</span><span class="n">avg_consumption_country</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">fermented_food_consumption.csv</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p><strong>🔄 2. Merge Datasets</strong></p> <p>We then renamed some columns for clarity and merged additional information, like ISO codes and population:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Merge fermented vegetable data with COVID-19 mortality rate data
</span><span class="n">eu_avg_consumption_COVID_death_pop_df</span> <span class="o">=</span> <span class="n">eu_avg_consumption_country</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">COVID_death_pop_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="sh">'</span><span class="s">Country</span><span class="sh">'</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Inspect first rows in the merged dataset
</span><span class="n">eu_avg_consumption_COVID_death_pop_df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>   
</code></pre></div></div> <p>The frist rows in the merged dataset:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">Country</span>	<span class="n">Average</span> <span class="n">Consumption</span>	<span class="n">Year</span>	<span class="n">Population</span>	<span class="n">Deaths</span>	<span class="n">Death</span> <span class="n">Rate</span>
<span class="mi">0</span>	<span class="n">France</span>	<span class="mf">1.135681</span>	<span class="mi">2020</span>	<span class="mf">67473651.0</span>	<span class="mf">9284524.0</span>	<span class="mf">0.137602</span>
<span class="mi">1</span>	<span class="n">France</span>	<span class="mf">1.135681</span>	<span class="mi">2021</span>	<span class="mf">67728568.0</span>	<span class="mf">38470807.0</span>	<span class="mf">0.568014</span>
<span class="mi">2</span>	<span class="n">France</span>	<span class="mf">1.135681</span>	<span class="mi">2022</span>	<span class="mf">67957053.0</span>	<span class="mf">54424558.0</span>	<span class="mf">0.800867</span>
<span class="mi">3</span>	<span class="n">France</span>	<span class="mf">1.135681</span>	<span class="mi">2023</span>	<span class="mf">68172977.0</span>	<span class="mf">11230468.0</span>	<span class="mf">0.164735</span>
<span class="mi">4</span>	<span class="n">Czechia</span>	<span class="mf">5.578666</span>	<span class="mi">2020</span>	<span class="mf">10693939.0</span>	<span class="mf">600899.0</span>	<span class="mf">0.056191</span>

</code></pre></div></div> <p><strong>🌍 3. Add Country Codes for Mapping</strong></p> <p>Using pycountry, we convert country names to ISO 3-letter codes so they can be matched to the map:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pycountry</span>

<span class="c1"># List of EU countries
</span><span class="n">eu_countries</span> <span class="o">=</span> <span class="n">eu_avg_consumption_COVID_death_pop_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Country</span><span class="sh">'</span><span class="p">].</span><span class="nf">unique</span><span class="p">().</span><span class="nf">tolist</span><span class="p">()</span>   

<span class="c1"># Dictionary of country names and their corresponding alpha_3 codes
</span><span class="n">country_alpha3</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">country</span> <span class="ow">in</span> <span class="n">eu_countries</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">country_data</span> <span class="o">=</span> <span class="n">pycountry</span><span class="p">.</span><span class="n">countries</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">country</span><span class="p">)</span>
        <span class="c1"># print(country_data.alpha_3)
</span>        <span class="n">country_alpha3</span><span class="p">[</span><span class="n">country</span><span class="p">]</span> <span class="o">=</span> <span class="n">country_data</span><span class="p">.</span><span class="n">alpha_3</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">country</span><span class="si">}</span><span class="s"> not found</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="how-do-we-get-map-data-in-python"><strong>How Do We Get Map Data in Python?</strong></h2> <p>To create interactive maps, we need two things:</p> <ol> <li>A base map that knows the shape of each country (like a digital atlas 📐)</li> <li>A way to match our data (e.g. COVID deaths, food consumption) to the correct country</li> </ol> <p>In this project, we used two helpful Python tools to achieve that:</p> <p><strong>🧩 1. requests + json: Loading the World Map</strong></p> <p>We downloaded a GeoJSON file — which is a special file format that stores map shapes — using Python’s requests module:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">json</span>

<span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json</span><span class="sh">'</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">geojson_data</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Filter the geojson for EU 
</span><span class="n">eu_geojson</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">FeatureCollection</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">features</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">feature</span> <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">geojson_data</span><span class="p">[</span><span class="sh">"</span><span class="s">features</span><span class="sh">"</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">feature</span><span class="p">[</span><span class="sh">"</span><span class="s">properties</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">]</span> <span class="ow">in</span> <span class="n">targeted_countries</span>
    <span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>🏷️ 2. pycountry: Matching Country Names to ISO Codes</strong></p> <p>Our datasets (like food consumption) use country names such as “Germany” or “Vietnam”. But map files often use short codes like “DEU” or “VNM”.</p> <p>We used the pycountry module to automatically convert country names into standardized ISO codes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pycountry</span>

<span class="c1"># List of EU countries
</span><span class="n">eu_countries</span> <span class="o">=</span> <span class="n">eu_avg_consumption_COVID_death_pop_df</span><span class="p">[</span><span class="sh">'</span><span class="s">Country</span><span class="sh">'</span><span class="p">].</span><span class="nf">unique</span><span class="p">().</span><span class="nf">tolist</span><span class="p">()</span>   

<span class="c1"># Dictionary of country names and their corresponding alpha_3 codes
</span><span class="n">country_alpha3</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">country</span> <span class="ow">in</span> <span class="n">eu_countries</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">country_data</span> <span class="o">=</span> <span class="n">pycountry</span><span class="p">.</span><span class="n">countries</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">country</span><span class="p">)</span>
        <span class="c1"># print(country_data.alpha_3)
</span>        <span class="n">country_alpha3</span><span class="p">[</span><span class="n">country</span><span class="p">]</span> <span class="o">=</span> <span class="n">country_data</span><span class="p">.</span><span class="n">alpha_3</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">country</span><span class="si">}</span><span class="s"> not found</span><span class="sh">"</span><span class="p">)</span>
        
 <span class="nf">print</span><span class="p">(</span><span class="n">country_alpha3</span><span class="p">)</span>
</code></pre></div></div> <p>Together, these two steps make it possible to visualize complex health and nutrition data on an interactive world map!</p> <hr/> <h2 id="choropleth-map--background-color-by-average-fermented-vegetables-consumption"><strong>Choropleth Map – Background Color by Average Fermented Vegetables Consumption</strong></h2> <p>This map shows average fermented vegetable consumption by country, shaded by intensity:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a Choropleth map (for country colors) based on fermented vegetable consumption
</span><span class="n">food_map</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">choropleth</span><span class="p">(</span>
    <span class="n">data_map_2020</span><span class="p">,</span>
    <span class="n">locations</span><span class="o">=</span><span class="sh">"</span><span class="s">iso_alpha</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">Average Consumption</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">hover_name</span><span class="o">=</span><span class="sh">"</span><span class="s">Country</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">scope</span><span class="o">=</span><span class="sh">"</span><span class="s">europe</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">projection</span><span class="o">=</span><span class="sh">"</span><span class="s">natural earth</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">color_continuous_scale</span><span class="o">=</span><span class="sh">'</span><span class="s">Plasma</span><span class="sh">'</span>
<span class="p">)</span>
</code></pre></div></div> <p>🖼️ ⬇️ Example Output</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/snapshot_choropleth_map_fermented_food.png" sizes="95vw"/> <img src="/assets/img/snapshot_choropleth_map_fermented_food.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="bubble-overlay--covid-19-mortality-rates"><strong>Bubble Overlay – COVID-19 mortality rates</strong></h2> <p>We then add circle markers to indicate COVID-19 mortality rates per country. Bigger bubbles = more intake.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span>

<span class="n">bubble_map</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">scatter_geo</span><span class="p">(</span><span class="n">data_map_2020</span><span class="p">,</span>
                            <span class="n">locations</span><span class="o">=</span><span class="sh">"</span><span class="s">iso_alpha</span><span class="sh">"</span><span class="p">,</span>
                            <span class="n">hover_name</span><span class="o">=</span><span class="sh">"</span><span class="s">Country</span><span class="sh">"</span><span class="p">,</span>
                            <span class="n">size</span><span class="o">=</span><span class="sh">"</span><span class="s">Death Rate</span><span class="sh">"</span><span class="p">,</span>
                            <span class="n">scope</span><span class="o">=</span><span class="sh">"</span><span class="s">europe</span><span class="sh">"</span><span class="p">,</span>
                            <span class="n">projection</span><span class="o">=</span><span class="sh">"</span><span class="s">natural earth</span><span class="sh">"</span><span class="p">,</span>
                            <span class="n">opacity</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="c1"># Set opacity level for better visibility
</span>                            <span class="n">size_max</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                            <span class="n">color_continuous_scale</span><span class="o">=</span><span class="n">px</span><span class="p">.</span><span class="n">colors</span><span class="p">.</span><span class="n">sequential</span><span class="p">.</span><span class="n">Plasma</span><span class="p">)</span>
</code></pre></div></div> <p>🖼️ ⬇️ Example Output:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/snapshot_bubble_map_COVID.png" sizes="95vw"/> <img src="/assets/img/snapshot_bubble_map_COVID.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="combine-both-layers"><strong>Combine Both Layers</strong></h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Combine both layers
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="p">.</span><span class="nc">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">food_map</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">bubble_map</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Why These Steps Matter</strong></p> <p>This combination allows us to visualize two variables on the same map:</p> <ul> <li>Red shading = how much fermented food people eat</li> <li>Bubble size = how bad COVID-19 outcomes were</li> </ul> <p>It opens up exploratory insights like:</p> <blockquote> <p>“Do countries with more fermented vegetable consumption have lower COVID-19 death rates?”</p> </blockquote> <p>This dual-layer approach makes it intuitive to compare variables geographically.</p> <h3 id="fine-tuning-the-map-for-clarity--style"><strong>Fine-Tuning the Map for Clarity &amp; Style</strong></h3> <p>Once we’ve layered the choropleth and bubbles, we fine-tune the map to make it easier to understand and more visually polished.</p> <p>Here are some key adjustments made in the notebook:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Improve layout
</span><span class="n">fig</span><span class="p">.</span><span class="nf">update_geos</span><span class="p">(</span>
    <span class="n">scope</span><span class="o">=</span><span class="sh">"</span><span class="s">europe</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># Only show European countries
</span>    <span class="n">showcoastlines</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
    <span class="n">showland</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">landcolor</span><span class="o">=</span><span class="sh">"</span><span class="s">lightgray</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">projection_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span>
    <span class="n">coloraxis_colorbar_title</span><span class="o">=</span><span class="sh">"</span><span class="s">Fermented Vegetable Consumption</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">coloraxis_colorscale</span><span class="o">=</span><span class="sh">"</span><span class="s">RdYlBu</span><span class="sh">"</span> <span class="p">,</span> <span class="c1"># Change color scale
</span>    <span class="n">width</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span>
    <span class="n">coloraxis_colorbar</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span>
        <span class="n">orientation</span><span class="o">=</span><span class="sh">"</span><span class="s">h</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Set colorbar horizontal
</span>        <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Fermented Vegetable Consumption</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">title_side</span><span class="o">=</span><span class="sh">"</span><span class="s">top</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">title_font_size</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">thickness</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="c1"># Adjust colorbar width
</span>        <span class="nb">len</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>  <span class="c1"># Adjust colorbar height (relative size)
</span>        <span class="n">x</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>  <span class="c1"># Move colorbar horizontally
</span>        <span class="n">y</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>  <span class="c1"># Move colorbar vertically
</span>    <span class="p">)</span>
<span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span>
        <span class="n">text</span><span class="o">=</span><span class="sh">"</span><span class="s">Fermented Vegetable Consumption and COVID-19 Death Rate in Europe (2020)</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>  <span class="c1"># Center the title
</span>        <span class="n">y</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span>  <span class="c1"># Position it above the colorbar
</span>        <span class="n">xanchor</span><span class="o">=</span><span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Ensure proper centering
</span>        <span class="n">yanchor</span><span class="o">=</span><span class="sh">"</span><span class="s">top</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Anchor at the top
</span>        <span class="n">font</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span>
            <span class="n">size</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span>  <span class="c1"># Increase font size for better readability
</span>            <span class="n">family</span><span class="o">=</span><span class="sh">"</span><span class="s">Arial, sans-serif</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Use a professional font
</span>            <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Set color (adjust if needed)
</span>            <span class="n">weight</span><span class="o">=</span><span class="sh">"</span><span class="s">bold</span><span class="sh">"</span>  <span class="c1"># Bolden the title (alternative: use "&lt;b&gt;Title&lt;/b&gt;" in text)
</span>        <span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">update_layout</span><span class="p">(</span>
    <span class="n">coloraxis_colorbar</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span>
        <span class="n">orientation</span><span class="o">=</span><span class="sh">"</span><span class="s">h</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Horizontal colorbar
</span>        <span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">y</span><span class="o">=-</span><span class="mf">0.15</span><span class="p">,</span>  <span class="c1"># Move below the map
</span>        <span class="nb">len</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">thickness</span><span class="o">=</span><span class="mi">10</span>
    <span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div> <p><strong>These tweaks:</strong></p> <ul> <li>Geographic layout</li> <li>Colorbar customization</li> <li>Title configuration</li> <li>Additional colorbar adjustment</li> </ul> <h2 id="annotate-country-names-to-the-map"><strong>Annotate country names to the map</strong></h2> <p>To make the map more professional-looking and easier to interpret, we apply layout settings:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Create the DataFrame
</span><span class="n">country_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">Country</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">Austria</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Belgium</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Bulgaria</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Bosnia and Herzegovina</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Germany</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Estonia</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Finland</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">France</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">United Kingdom</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Greece</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Croatia</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Hungary</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Latvia</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Montenegro</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Netherlands</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Poland</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Portugal</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Romania</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Slovenia</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Sweden</span><span class="sh">"</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">ISO3</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">AUT</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">BEL</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">BGR</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">BIH</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">DEU</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">EST</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FIN</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">FRA</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">GBR</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">GRC</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">HRV</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">HUN</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">LVA</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">MNE</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">NLD</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">POL</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">PRT</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">ROU</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">SVN</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">SWE</span><span class="sh">"</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">Lat</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mf">47.5162</span><span class="p">,</span> <span class="mf">50.5039</span><span class="p">,</span> <span class="mf">42.7339</span><span class="p">,</span> <span class="mf">43.9159</span><span class="p">,</span> <span class="mf">51.1657</span><span class="p">,</span> <span class="mf">58.5953</span><span class="p">,</span> <span class="mf">61.9241</span><span class="p">,</span> <span class="mf">46.6034</span><span class="p">,</span> <span class="mf">55.3781</span><span class="p">,</span> <span class="mf">39.0742</span><span class="p">,</span> <span class="mf">45.1</span><span class="p">,</span> <span class="mf">47.1625</span><span class="p">,</span> <span class="mf">56.8796</span><span class="p">,</span> <span class="mf">42.7087</span><span class="p">,</span> <span class="mf">52.1326</span><span class="p">,</span> <span class="mf">51.9194</span><span class="p">,</span> <span class="mf">39.3999</span><span class="p">,</span> <span class="mf">45.9432</span><span class="p">,</span> <span class="mf">46.1512</span><span class="p">,</span> <span class="mf">60.1282</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">Lon</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mf">14.5501</span><span class="p">,</span> <span class="mf">4.4699</span><span class="p">,</span> <span class="mf">25.4858</span><span class="p">,</span> <span class="mf">17.6791</span><span class="p">,</span> <span class="mf">10.4515</span><span class="p">,</span> <span class="mf">25.0136</span><span class="p">,</span> <span class="mf">25.7482</span><span class="p">,</span> <span class="mf">1.8883</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4360</span><span class="p">,</span> <span class="mf">21.8243</span><span class="p">,</span> <span class="mf">15.2</span><span class="p">,</span> <span class="mf">19.5033</span><span class="p">,</span> <span class="mf">24.6032</span><span class="p">,</span> <span class="mf">19.3744</span><span class="p">,</span> <span class="mf">5.2913</span><span class="p">,</span> <span class="mf">19.1451</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.2245</span><span class="p">,</span> <span class="mf">24.9668</span><span class="p">,</span> <span class="mf">14.9955</span><span class="p">,</span> <span class="mf">18.6435</span><span class="p">]</span>
<span class="p">})</span>

<span class="kn">import</span> <span class="n">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span>

<span class="c1"># Create the country label layer (scattergeo)
</span><span class="n">country_labels</span> <span class="o">=</span> <span class="n">go</span><span class="p">.</span><span class="nc">Scattergeo</span><span class="p">(</span>
    <span class="n">locationmode</span><span class="o">=</span><span class="sh">"</span><span class="s">ISO-3</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">lon</span><span class="o">=</span><span class="n">country_data</span><span class="p">[</span><span class="sh">"</span><span class="s">Lon</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">lat</span><span class="o">=</span><span class="n">country_data</span><span class="p">[</span><span class="sh">"</span><span class="s">Lat</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">text</span><span class="o">=</span><span class="n">country_data</span><span class="p">[</span><span class="sh">"</span><span class="s">Country</span><span class="sh">"</span><span class="p">],</span>  <span class="c1"># Display country names
</span>    <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Only text (no markers)
</span>    <span class="n">textfont</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="sh">"</span><span class="s">Arial</span><span class="sh">"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="sh">"</span><span class="s">bold</span><span class="sh">"</span><span class="p">),</span>  <span class="c1"># Adjust font
</span>    <span class="n">textposition</span><span class="o">=</span><span class="sh">"</span><span class="s">top center</span><span class="sh">"</span><span class="p">,</span>  
    <span class="n">showlegend</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>

<span class="c1"># Add to your existing Plotly figure
</span><span class="n">fig</span><span class="p">.</span><span class="nf">add_trace</span><span class="p">(</span><span class="n">country_labels</span><span class="p">)</span>
</code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/snapshot_added_country_names.png" sizes="95vw"/> <img src="/assets/img/snapshot_added_country_names.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Explore the full interactive version on <a href="https://davidzhao1015.github.io/plotly-bubble-choropleth/">GitHub</a>.</p> <h2 id="exporting-the-map"><strong>Exporting the Map</strong></h2> <p>Want to save your plot as an image or HTML? Use:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pio</span><span class="p">.</span><span class="nf">write_image</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="sh">'</span><span class="s">fermented_vegetable_consumption_COVID_death_rate_europe_2020.png</span><span class="sh">'</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="sh">'</span><span class="s">png</span><span class="sh">'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>                  <span class="c1"># Save as PNG
</span>
<span class="n">pio</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="nb">file</span><span class="o">=</span><span class="sh">'</span><span class="s">index.html</span><span class="sh">'</span><span class="p">,</span> <span class="n">auto_open</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>       <span class="c1"># Save as interactive webpage
</span></code></pre></div></div> <p>This step is great for sharing visuals in presentations or embedding interactive plots on your own blog or website.</p> <p>Together, these finishing touches elevate your map from “functional” to “insightful and polished.” They also make the visualization more friendly to readers who are new to data exploration or unfamiliar with the dataset.</p> <hr/> <h2 id="what-does-the-map-tell-us"><strong>What Does the Map Tell Us?</strong></h2> <p>As we zoom into this interactive map, a few patterns emerge:</p> <ul> <li>Countries like Hungary and Romania — with relatively high intake of fermented vegetables — show noticeably lower COVID-19 mortality in this 2020 dataset.</li> <li>On the other hand, countries with lower fermented food consumption, such as the UK or Belgium, show higher death rates.</li> <li>While this map doesn’t prove causality, it does support the hypothesis from [the original study] that dietary habits might influence immune resilience — a fascinating intersection of public health and nutrition.</li> </ul> <p>Of course, many other factors (like healthcare infrastructure or testing policies) may also be at play. But that’s the beauty of this kind of map — it opens up questions and encourages deeper exploration.</p>]]></content><author><name></name></author><category term="data-visualization;"/><category term="epidemiology;"/><category term="plolty;"/><category term="choropleth-map;"/><category term="fermented-food;"/><category term="COVID-19"/><summary type="html"><![CDATA[Background In public health and epidemiology, data visualization isn’t just about aesthetics—it’s about clarity, impact, and storytelling. Choropleth maps, which color-code geographic areas based on variable intensity, are a go-to tool for illustrating spatial trends in disease burden, health behaviors, or population risk factors.]]></summary></entry><entry><title type="html">How to Import Excel Files in Python</title><link href="https://davidzhao1015.github.io/blog/2024/import-spreadsheet-python/" rel="alternate" type="text/html" title="How to Import Excel Files in Python"/><published>2024-12-24T00:00:00+00:00</published><updated>2024-12-24T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2024/import-spreadsheet-python</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2024/import-spreadsheet-python/"><![CDATA[<h2 id="purpose">Purpose</h2> <p>Ever struggled to import Excel data into Python for analysis? This post will guide you step-by-step on how to use <strong>Pandas</strong> to handle Excel files effortlessly, even for complex datasets.</p> <hr/> <h2 id="excel-data-essentials">Excel Data Essentials</h2> <p>Excel is a go-to tool for managing and analyzing data. Let’s refresh some key terms to ensure smooth communication:</p> <ul> <li><strong>Workbook</strong>: The entire Excel file.</li> <li><strong>Worksheet</strong>: Individual sheets (or tabs) within the workbook.</li> <li><strong>Header</strong>: Labels at the top defining columns (e.g., A, B, C).</li> <li><strong>Cells</strong>: Data units located at row-column intersections, like A1.</li> </ul> <p>If these terms feel familiar, great! If not, think of them as the building blocks for working with Excel in Python.</p> <hr/> <h2 id="everyday-functionality-importing-excel-files-in-pandas">Everyday Functionality: Importing Excel Files in Pandas</h2> <h3 id="real-world-scenario-metabolomics-data">Real-World Scenario: Metabolomics Data</h3> <p>Imagine you’re analyzing LC/MS metabolomics data from animal samples, with additional metadata. This was my experience at the metabolomics research center, where I worked with a dataset containing:</p> <ol> <li><strong>Biomarker Assay Worksheet</strong>: Measurements for over 100 metabolites.</li> </ol> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/snapshot_biomarker_worksheet.png" sizes="95vw"/> <img src="/assets/img/snapshot_biomarker_worksheet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ol> <li><strong>Metadata Worksheet</strong>: Sample information.</li> </ol> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/snapshot_metadata_worksheet.png" sizes="95vw"/> <img src="/assets/img/snapshot_metadata_worksheet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <hr/> <h2 id="step-by-step-guide">Step-by-Step Guide</h2> <h3 id="1-import-multiple-worksheets">1. Import Multiple Worksheets</h3> <p>Start by loading a specific worksheet while skipping descriptive rows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span> 

<span class="n">df_biomarker_assay</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_excel</span><span class="p">(</span>
    <span class="sh">'</span><span class="s">example-biomarker-assay.xlsx</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">sheet_name</span><span class="o">=</span><span class="sh">'</span><span class="s">Biomarker assay</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">skiprows</span><span class="o">=</span><span class="mi">11</span>
<span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; df_biomarker_assay.head()
  Sample ID  Creatinine    Glycine     Alanine     Serine Histamine  ...  C16:1OH     C16OH     C18:2     C18:1       C18   C18:1OH
0  LODs(uM)    0.443131   0.859107    0.340909   0.042433  0.013605  ...  0.04906  0.042617  0.058089  0.041081  0.023651  0.056784
1         1   14.600000  51.600000  264.000000  65.100000     &lt; LOD  ...    &lt; LOD     &lt; LOD     &lt; LOD     &lt; LOD     &lt; LOD     &lt; LOD
2         2   14.000000  98.100000  338.000000  87.000000     &lt; LOD  ...    &lt; LOD     &lt; LOD     &lt; LOD     &lt; LOD     &lt; LOD     &lt; LOD
3         3   11.200000  92.000000  329.000000  74.500000     &lt; LOD  ...    &lt; LOD     &lt; LOD     &lt; LOD  0.045089     &lt; LOD     &lt; LOD
4         4   11.600000  77.500000  200.000000  62.400000     &lt; LOD  ...    &lt; LOD     &lt; LOD     &lt; LOD   0.04134   0.02505     &lt; LOD

[5 rows x 144 columns]
</code></pre></div></div> <h3 id="2-select-specific-rows-and-columns">2. Select Specific Rows and Columns</h3> <p>To limit the data to a manageable range:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_biomarker_assay_selected</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_excel</span><span class="p">(</span>
    <span class="sh">'</span><span class="s">example-biomarker-assay.xlsx</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">sheet_name</span><span class="o">=</span><span class="sh">'</span><span class="s">Biomarker assay</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">skiprows</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span>
    <span class="n">usecols</span><span class="o">=</span><span class="sh">'</span><span class="s">A:EN</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">nrows</span><span class="o">=</span><span class="mi">31</span>
<span class="p">)</span>
</code></pre></div></div> <h3 id="3-verify-non-empty-rows-and-columns">3. Verify Non-Empty Rows and Columns</h3> <p>Check for completeness in the imported data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nonempty_rows</span> <span class="o">=</span> <span class="n">df_biomarker_assay_selected</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">how</span><span class="o">=</span><span class="sh">'</span><span class="s">all</span><span class="sh">'</span><span class="p">).</span><span class="n">index</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span>
<span class="n">count_rows</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">nonempty_rows</span><span class="p">)</span>

<span class="n">nonempty_cols</span> <span class="o">=</span> <span class="n">df_biomarker_assay_selected</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">how</span><span class="o">=</span><span class="sh">'</span><span class="s">all</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">columns</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span>
<span class="n">count_cols</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">nonempty_cols</span><span class="p">)</span>
</code></pre></div></div> <h3 id="4-validate-data-import">4. Validate Data Import</h3> <p>Inspect the first and last rows of your dataset to verify that the import matches the source.</p> <hr/> <h2 id="special-use-cases-with-pandas">Special Use Cases with Pandas</h2> <h3 id="1-formula-generated-data">1. Formula-Generated Data</h3> <p>Excel formulas, like those generating the column in the <strong>Metadata</strong> worksheet, retain their calculated values when imported:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_metadata</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_excel</span><span class="p">(</span><span class="sh">'</span><span class="s">example-biomarker-assay.xlsx</span><span class="sh">'</span><span class="p">,</span> <span class="n">sheet_name</span><span class="o">=</span><span class="sh">'</span><span class="s">Metadata</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <h3 id="2-filtered-data">2. Filtered Data</h3> <p>Filtered tables (e.g., showing only filtered rows in Excel) are fully imported, including the hidden rows.</p> <h3 id="3-binary-workbook-files-xlsb">3. Binary Workbook Files (.xlsb)</h3> <p>For <code class="language-plaintext highlighter-rouge">.xlsb</code> files, use the <code class="language-plaintext highlighter-rouge">pyxlsb</code> library to read the data.</p> <hr/> <h2 id="best-practices-for-importing-excel-files">Best Practices for Importing Excel Files</h2> <p>To streamline your workflow, follow these tips:</p> <ul> <li>Use <code class="language-plaintext highlighter-rouge">skiprows</code> to ignore unnecessary content.</li> <li>Specify <code class="language-plaintext highlighter-rouge">usecols</code> and <code class="language-plaintext highlighter-rouge">nrows</code> for better performance.</li> <li>Confirm data integrity by checking non-empty rows and columns.</li> <li>Choose the correct engine for specialized file formats (e.g., <code class="language-plaintext highlighter-rouge">.xlsb</code>).</li> </ul> <p>By following these steps, you’ll efficiently prepare your data for analysis, no matter the complexity of the Excel files.</p> <hr/> <h2 id="final-thoughts">Final Thoughts</h2> <p>Importing Excel files in Python doesn’t have to be daunting. With a clear process, even the most complex datasets become manageable. Try these steps on your own files, and let me know how it goes!</p> <p>For any questions or advanced use cases, feel free to drop a comment or reach out. Happy coding!</p> <hr/> <h2 id="references">References</h2> <ul> <li><a href="https://support.microsoft.com/en-us/office/file-formats-that-are-supported-in-excel-0943ff2c-6014-4e8d-aaea-b83d51d46247">Excel file format</a></li> <li><a href="https://support.microsoft.com/en-us/office/overview-of-excel-tables-7ab0bb7d-3a9e-4b56-a3c9-6c94334e492c">Overview of Excel table</a></li> <li><a href="https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html">Pandas API doc</a></li> </ul>]]></content><author><name></name></author><category term="statistics"/><category term="python;"/><category term="spreadsheet;"/><category term="data-analysis"/><summary type="html"><![CDATA[Purpose Ever struggled to import Excel data into Python for analysis? This post will guide you step-by-step on how to use Pandas to handle Excel files effortlessly, even for complex datasets.]]></summary></entry><entry><title type="html">Top R Packages for GO Enrichment Analysis: topGO vs globaltest Explained</title><link href="https://davidzhao1015.github.io/blog/2024/go-enrichment/" rel="alternate" type="text/html" title="Top R Packages for GO Enrichment Analysis: topGO vs globaltest Explained"/><published>2024-11-22T00:00:00+00:00</published><updated>2024-11-22T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2024/go-enrichment</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2024/go-enrichment/"><![CDATA[<h2 id="1-introduction"><strong>1. Introduction</strong></h2> <p>Gene Ontology (GO) enrichment analysis is a cornerstone of gene expression studies. It helps researchers identify biological processes, molecular functions, and cellular components that are overrepresented in a set of genes, offering insights into the underlying biology. Several statistical methods can be used for GO enrichment analysis, including Fisher’s exact test, the Kolmogorov-Smirnov test, and the global test.</p> <p>This article aims to simplify the process of choosing an appropriate R package for GO enrichment analysis by introducing two popular Bioconductor packages: <strong>topGO</strong> and <strong>globaltest</strong>. While both packages are widely used, their distinct statistical methods and outputs can make it challenging to choose the right tool for your study. This guide compares these packages, highlights their differences, and provides practical examples to help researchers, especially those with busy lab schedules, efficiently integrate GO analysis into their workflows.</p> <h2 id="2-overview-of-bioconductor-packages"><strong>2. Overview of Bioconductor Packages</strong></h2> <p><strong>topGO</strong></p> <p><strong>topGO</strong> is a versatile R package for GO enrichment analysis, well-suited for identifying specific GO terms that are enriched among differentially expressed genes. It primarily employs statistical methods like Fisher’s exact test and the Kolmogorov-Smirnov test, making it a reliable choice for detecting overrepresentation in gene lists. With its robust functionality and detailed documentation, topGO is a go-to tool for exploring gene-level associations in various biological datasets.</p> <p><strong>globaltest</strong></p> <p><strong>globaltest</strong> takes a different approach by assessing whether specific GO terms are associated with clinical outcomes or other continuous variables. It uses the global test methodology, which evaluates associations at a more holistic level compared to gene-specific tests. This makes it particularly valuable for studies where the research question involves linking GO terms to phenotypic data, such as disease progression or treatment response.</p> <p><strong>Key Differences in Statistical Approaches</strong></p> <p>Both packages are highly ranked in the Bioconductor repository due to their active maintenance and comprehensive documentation. However, their underlying statistical methods set them apart:</p> <p>• <strong>topGO</strong>: Uses Fisher’s exact test or the Kolmogorov-Smirnov test to test the null hypothesis that no specific GO terms are enriched in a set of genes.</p> <p>• <strong>globaltest</strong>: Employs the global test to evaluate the null hypothesis that no association exists between a set of genes and a clinical outcome or phenotype.</p> <p><strong>Use Case Comparison</strong></p> <p>• <strong>topGO</strong> is ideal for researchers seeking to uncover enriched biological processes in differentially expressed genes.</p> <p>• <strong>globaltest</strong> is better suited for studies focused on linking GO terms to clinical or phenotypic outcomes, such as identifying functional pathways associated with disease progression.</p> <p>By understanding these distinctions, researchers can choose the package that best aligns with their study objectives.</p> <h2 id="3-setting-environment"><strong>3. Setting Environment</strong></h2> <p>Setting up includes ensuring the required packages are installed and loaded.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Install globaltest Biocondcutor package</span><span class="w">
</span><span class="n">BiocManager</span><span class="o">::</span><span class="n">install</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s2">"globaltest"</span><span class="p">,</span><span class="w"> </span><span class="s2">"topGO"</span><span class="p">,</span><span class="w"> </span><span class="s2">"golubEsets"</span><span class="p">,</span><span class="w"> </span><span class="s2">"vsn"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hu6800.db"</span><span class="p">,</span><span class="w"> </span><span class="s2">"GO.db"</span><span class="p">,</span><span class="w"> </span><span class="s2">"AnnotationDbi"</span><span class="p">,</span><span class="w"> </span><span class="s2">"annotate"</span><span class="p">,</span><span class="w"> </span><span class="s2">"topGO"</span><span class="p">,</span><span class="w"> </span><span class="s2">"ALL"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Biobase"</span><span class="p">,</span><span class="w"> </span><span class="s2">"limma"</span><span class="p">))</span><span class="w">  

</span><span class="c1"># Load the globaltest package</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">topGO</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">globaltest</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">golubEsets</span><span class="p">)</span><span class="w"> 
</span><span class="n">library</span><span class="p">(</span><span class="n">vsn</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">hu6800.db</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">AnnotationDbi</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">methods</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">annotate</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">limma</span><span class="p">)</span><span class="w"> 
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <h2 id="4-data-prepration"><strong>4. Data Prepration</strong></h2> <p>Data preprocessing and normalization.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load the Golub training data set consisting of 7129 genes and 38 samples (27 ALL and 11 AML)</span><span class="w">
</span><span class="n">data</span><span class="p">(</span><span class="n">Golub_Train</span><span class="p">,</span><span class="w"> </span><span class="n">package</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"golubEsets"</span><span class="p">)</span><span class="w">  

</span><span class="c1">## Normalize the data using the VSN package</span><span class="w">
</span><span class="n">Golub_Train_VSN</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vsn</span><span class="o">::</span><span class="n">vsn2</span><span class="p">(</span><span class="n">exprs</span><span class="p">(</span><span class="n">Golub_Train</span><span class="p">))</span><span class="w"> 
</span></code></pre></div></div> <p>Gene expression data before the normalization process:</p> <p>A matrix: 5 x 10 of type int</p> <table> <thead> <tr> <th></th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>5</th> <th>6</th> <th>7</th> <th>8</th> <th>9</th> <th>10</th> </tr> </thead> <tbody> <tr> <td>AFFX-BioB-5_at</td> <td>-214</td> <td>-139</td> <td>-76</td> <td>-135</td> <td>-106</td> <td>-138</td> <td>-72</td> <td>-413</td> <td>5</td> <td>-88</td> </tr> <tr> <td>AFFX-BioB-M_at</td> <td>-153</td> <td>-73</td> <td>-49</td> <td>-114</td> <td>-125</td> <td>-85</td> <td>-144</td> <td>-260</td> <td>-127</td> <td>-105</td> </tr> <tr> <td>AFFX-BioB-3_at</td> <td>-58</td> <td>-1</td> <td>-307</td> <td>265</td> <td>-76</td> <td>215</td> <td>238</td> <td>7</td> <td>106</td> <td>42</td> </tr> <tr> <td>AFFX-BioC-5_at</td> <td>88</td> <td>283</td> <td>309</td> <td>12</td> <td>168</td> <td>71</td> <td>55</td> <td>-2</td> <td>268</td> <td>219</td> </tr> <tr> <td>AFFX-BioC-3_at</td> <td>-295</td> <td>-264</td> <td>-376</td> <td>-419</td> <td>-230</td> <td>-272</td> <td>-399</td> <td>-541</td> <td>-210</td> <td>-178</td> </tr> </tbody> </table> <p>Gene expression data after the normalization process:</p> <p>A matrix: 5 x 10 of type dbl</p> <table> <thead> <tr> <th></th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> <th>5</th> <th>6</th> <th>7</th> <th>8</th> <th>9</th> <th>10</th> </tr> </thead> <tbody> <tr> <td>AFFX-BioB-5_at</td> <td>5.053873</td> <td>5.396673</td> <td>5.972362</td> <td>5.549766</td> <td>5.337167</td> <td>5.411235</td> <td>5.968888</td> <td>4.616873</td> <td>6.396420</td> <td>5.615805</td> </tr> <tr> <td>AFFX-BioB-M_at</td> <td>5.364311</td> <td>5.838160</td> <td>6.128136</td> <td>5.678733</td> <td>5.212093</td> <td>5.797844</td> <td>5.508008</td> <td>5.178958</td> <td>5.605328</td> <td>5.480476</td> </tr> <tr> <td>AFFX-BioB-3_at</td> <td>5.948439</td> <td>6.391596</td> <td>4.906233</td> <td>8.079299</td> <td>5.550368</td> <td>8.071159</td> <td>7.967052</td> <td>6.646939</td> <td>7.033943</td> <td>6.825375</td> </tr> <tr> <td>AFFX-BioC-5_at</td> <td>6.988237</td> <td>8.224862</td> <td>8.003817</td> <td>6.558152</td> <td>7.593938</td> <td>7.118814</td> <td>6.884934</td> <td>6.591159</td> <td>7.872088</td> <td>8.162249</td> </tr> <tr> <td>AFFX-BioC-3_at</td> <td>4.707586</td> <td>4.747971</td> <td>4.672928</td> <td>4.322134</td> <td>4.638834</td> <td>4.665432</td> <td>4.390771</td> <td>4.257310</td> <td>5.197148</td> <td>4.978863</td> </tr> </tbody> </table> <h2 id="5-go-enrichment-analysis-with-topgo"><strong>5. GO Enrichment Analysis with topGO</strong></h2> <p>GO enrichment with topGO</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a topGOdata object </span><span class="w">
</span><span class="n">sampleGOdata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">new</span><span class="p">(</span><span class="s2">"topGOdata"</span><span class="p">,</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Simple session"</span><span class="p">,</span><span class="w"> </span><span class="n">ontology</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"BP"</span><span class="p">,</span><span class="w"> </span><span class="n">allGenes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pvalues</span><span class="p">,</span><span class="w"> </span><span class="n">geneSel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">topDiffGenes</span><span class="p">,</span><span class="w"> </span><span class="n">nodeSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">annot</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">annFUN.db</span><span class="p">,</span><span class="w"> </span><span class="n">affyLib</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">affyLib</span><span class="p">)</span><span class="w"> 

</span><span class="c1"># Run GO enrichment analysis with Fisher's exact test </span><span class="w">
</span><span class="n">resultFisher</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">runTest</span><span class="p">(</span><span class="n">sampleGOdata</span><span class="p">,</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"classic"</span><span class="p">,</span><span class="w"> </span><span class="n">statistic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"fisher"</span><span class="p">)</span><span class="w"> 
</span></code></pre></div></div> <p>Display summary of results:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Display the results </span><span class="w">
</span><span class="n">resultFisher</span><span class="w"> 
</span></code></pre></div></div> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Description</span><span class="o">:</span><span class="w"> </span><span class="n">Simple</span><span class="w"> </span><span class="n">session</span><span class="w"> 
</span><span class="n">Ontology</span><span class="o">:</span><span class="w"> </span><span class="n">BP</span><span class="w"> 
</span><span class="s1">'classic'</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="s1">'fisher'</span><span class="w"> </span><span class="n">test</span><span class="w">
</span><span class="m">5431</span><span class="w"> </span><span class="n">GO</span><span class="w"> </span><span class="n">terms</span><span class="w"> </span><span class="n">scored</span><span class="o">:</span><span class="w"> </span><span class="m">302</span><span class="w"> </span><span class="n">terms</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">0.01</span><span class="w">
</span><span class="n">Annotation</span><span class="w"> </span><span class="n">data</span><span class="o">:</span><span class="w">
    </span><span class="n">Annotated</span><span class="w"> </span><span class="n">genes</span><span class="o">:</span><span class="w"> </span><span class="m">6234</span><span class="w"> 
    </span><span class="n">Significant</span><span class="w"> </span><span class="n">genes</span><span class="o">:</span><span class="w"> </span><span class="m">1512</span><span class="w"> 
    </span><span class="n">Min.</span><span class="w"> </span><span class="n">no.</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">genes</span><span class="w"> </span><span class="n">annotated</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">GO</span><span class="o">:</span><span class="w"> </span><span class="m">10</span><span class="w"> 
    </span><span class="n">Nontrivial</span><span class="w"> </span><span class="n">nodes</span><span class="o">:</span><span class="w"> </span><span class="m">5341</span><span class="w"> 
</span></code></pre></div></div> <p>Show the top 10 enriched GO terms:</p> <table> <thead> <tr> <th></th> <th>GO.ID &lt;chr&gt;</th> <th>Term &lt;chr&gt;</th> <th>Annotated &lt;int&gt;</th> <th>Significant &lt;int&gt;</th> <th>Expected &lt;dbl&gt;</th> <th>Rank in classicFisher &lt;int&gt;</th> <th>classicFisher &lt;chr&gt;</th> <th>classicKS &lt;chr&gt;</th> <th>elimKS &lt;chr&gt;</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>GO:0010042</td> <td>response to manganese ion</td> <td>18</td> <td>14</td> <td>4.37</td> <td>10</td> <td>2.6e-06</td> <td>2.1e-06</td> <td>2.1e-06</td> </tr> <tr> <td>2</td> <td>GO:0000002</td> <td>mitochondrial genome maintenance</td> <td>13</td> <td>8</td> <td>3.15</td> <td>199</td> <td>0.00458</td> <td>0.00015</td> <td>0.00015</td> </tr> <tr> <td>3</td> <td>GO:0044539</td> <td>long-chain fatty acid import into cell</td> <td>11</td> <td>8</td> <td>2.67</td> <td>88</td> <td>0.00095</td> <td>0.00017</td> <td>0.00017</td> </tr> <tr> <td>4</td> <td>GO:0070198</td> <td>protein localization to chromosome, telo…</td> <td>20</td> <td>13</td> <td>4.85</td> <td>43</td> <td>0.00013</td> <td>0.00018</td> <td>0.00018</td> </tr> <tr> <td>5</td> <td>GO:0071897</td> <td>DNA biosynthetic process</td> <td>109</td> <td>41</td> <td>26.44</td> <td>101</td> <td>0.00118</td> <td>1.1e-05</td> <td>0.00029</td> </tr> <tr> <td>6</td> <td>GO:0045429</td> <td>positive regulation of nitric oxide bios…</td> <td>32</td> <td>18</td> <td>7.76</td> <td>42</td> <td>0.00010</td> <td>0.00033</td> <td>0.00033</td> </tr> <tr> <td>7</td> <td>GO:0016570</td> <td>histone modification</td> <td>44</td> <td>16</td> <td>10.67</td> <td>694</td> <td>0.04846</td> <td>0.00053</td> <td>0.00053</td> </tr> <tr> <td>8</td> <td>GO:0098869</td> <td>cellular oxidant detoxification</td> <td>70</td> <td>28</td> <td>16.98</td> <td>142</td> <td>0.00243</td> <td>0.00056</td> <td>0.00056</td> </tr> <tr> <td>9</td> <td>GO:0045820</td> <td>negative regulation of glycolytic proces…</td> <td>11</td> <td>9</td> <td>2.67</td> <td>39</td> <td>9.6e-05</td> <td>0.00056</td> <td>0.00056</td> </tr> <tr> <td>10</td> <td>GO:1903241</td> <td>U2-type prespliceosome assembly</td> <td>15</td> <td>9</td> <td>3.64</td> <td>168</td> <td>0.00332</td> <td>0.00057</td> <td>0.00057</td> </tr> </tbody> </table> <p>The GO topology graph for the top 5 enriched GO terms is shown below:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/topGO-enriched-GO.png" sizes="95vw"/> <img src="/assets/img/topGO-enriched-GO.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h2 id="6-analysis-with-globaltest"><strong>6. Analysis with globaltest</strong></h2> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">global_test_result</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">globaltest</span><span class="o">::</span><span class="n">gt</span><span class="p">(</span><span class="n">response</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ALL.AML</span><span class="p">,</span><span class="w"> </span><span class="n">alternative</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Golub_Train</span><span class="p">)</span><span class="w">

</span><span class="n">res</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">globaltest</span><span class="o">::</span><span class="n">gtGO</span><span class="p">(</span><span class="n">ALL.AML</span><span class="p">,</span><span class="w"> </span><span class="n">Golub_Train</span><span class="p">,</span><span class="n">ontology</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"BP"</span><span class="p">,</span><span class="w"> </span><span class="n">annotation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"hu6800.db"</span><span class="p">,</span><span class="w"> </span><span class="n">multtest</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"BH"</span><span class="p">)</span><span class="w"> 
</span></code></pre></div></div> <table> <thead> <tr> <th></th> <th>GO &lt;chr&gt;</th> <th>alias &lt;chr&gt;</th> <th>BH &lt;dbl&gt;</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>GO:0006979</td> <td>response to oxidative stress</td> <td>5.937219e-09</td> </tr> <tr> <td>2</td> <td>GO:0062197</td> <td>cellular response to chemical stress</td> <td>5.937219e-09</td> </tr> <tr> <td>3</td> <td>GO:0034599</td> <td>cellular response to oxidative stress</td> <td>5.937219e-09</td> </tr> <tr> <td>4</td> <td>GO:0034614</td> <td>cellular response to reactive oxygen species</td> <td>1.479059e-08</td> </tr> <tr> <td>5</td> <td>GO:0009628</td> <td>response to abiotic stimulus</td> <td>2.469754e-08</td> </tr> <tr> <td>6</td> <td>GO:0000302</td> <td>response to reactive oxygen species</td> <td>2.680335e-08</td> </tr> <tr> <td>7</td> <td>GO:0019932</td> <td>second-messenger-mediated signaling</td> <td>1.030937e-07</td> </tr> <tr> <td>8</td> <td>GO:0019722</td> <td>calcium-mediated signaling</td> <td>1.080573e-07</td> </tr> <tr> <td>9</td> <td>GO:0050921</td> <td>positive regulation of chemotaxis</td> <td>1.080573e-07</td> </tr> <tr> <td>10</td> <td>GO:0003013</td> <td>circulatory system process</td> <td>1.342398e-07</td> </tr> </tbody> </table> <h2 id="5-comparing-results"><strong>5. Comparing Results</strong></h2> <p>The results of GO enrichment analysis using <strong>topGO</strong> and <strong>globaltest</strong> provide distinct yet informative insights into the biological processes underlying the data.</p> <p><strong>Key Results and Differences:</strong></p> <p>• <strong>Significant GO Terms</strong>:</p> <p>• topGO identified 302 significant GO terms using the Fisher’s exact test, with a focus on Biological Process (BP) ontology, while globaltest highlighted 10 highly significant GO terms, such as “response to oxidative stress” (GO:0006979) and “cellular response to reactive oxygen species” (GO:0034614).</p> <p>• <strong>Methodological Nuances</strong>:</p> <p>• topGO excels in leveraging the GO hierarchy, particularly with algorithms like “elim” to minimize redundancy. This approach is effective for capturing nuanced biological pathways.</p> <p>• globaltest uses a multivariate approach that evaluates the overall association between gene sets and outcomes, making it particularly sensitive to systemic biological patterns.</p> <p><strong>Type of Questions Addressed</strong>:</p> <p>• topGO is better suited for researchers aiming to understand specific enriched processes while accounting for the GO graph topology.</p> <p>• globaltest is ideal for broader hypotheses, such as assessing the overall contribution of a gene set to a phenotype.</p> <h2 id="6-discussion"><strong>6. Discussion</strong></h2> <p>The results from topGO and globaltest are largely complementary rather than competitive. Each package offers a unique lens through which to interpret the data:</p> <p>• <strong>Complementary Insights</strong>:</p> <p>• topGO provides granular details about localized processes within the GO hierarchy.</p> <p>• globaltest identifies overarching biological associations, highlighting system-wide trends.</p> <p>• <strong>Scenarios for Combining Insights</strong>:</p> <p>• Combining topGO’s hierarchical insights with globaltest’s systemic view can help uncover both specific mechanisms and broader biological themes, particularly for complex datasets.</p> <p>• For example, a researcher could first use topGO to pinpoint key pathways and then employ globaltest to evaluate their aggregate impact.</p> <p>• <strong>Limitations and Considerations</strong>:</p> <p>• topGO assumes independence between GO terms, which may oversimplify relationships in some contexts.</p> <p>• globaltest may lose specificity in its focus on global patterns, potentially overlooking individual pathway nuances.</p> <p>• Computational efficiency may also differ: topGO’s hierarchical algorithms may demand more preprocessing, while globaltest benefits from a simpler multivariate setup.</p> <h2 id="7-conclusion"><strong>7. Conclusion</strong></h2> <p>This comparative analysis demonstrates that both topGO and globaltest offer valuable but distinct approaches to GO enrichment analysis:</p> <p><strong>Practical Takeaways</strong>:</p> <ul> <li> <p>topGO is preferred for detailed pathway analysis, especially for datasets with hierarchical biological information.</p> </li> <li> <p>globaltest excels in scenarios requiring a holistic assessment of gene set relevance to phenotypes.</p> </li> </ul> <p><strong>Encouragement for Beginners</strong>:</p> <p>New researchers are encouraged to explore both packages to deepen their understanding of GO enrichment methods. Experimenting with these tools fosters a comprehensive grasp of the biological and statistical nuances critical for robust bioinformatics analysis.</p> <h2 id="8-references-and-further-reading"><strong>8. References and Further Reading</strong></h2> <ul> <li><a href="https://bioconductor.org/packages/release/bioc/html/topGO.html">topGO: Enrichment Analysis for Gene Ontology</a></li> <li><a href="https://bioconductor.org/packages/release/bioc/html/globaltest.html">globatest: Global Test for Functional Enrichment Analysis</a></li> <li><a href="http://geneontology.org/">Gene Ontology Consortium</a></li> </ul>]]></content><author><name></name></author><category term="statistics"/><category term="bioconductor;"/><category term="R;"/><category term="topGO;"/><category term="globaltest;"/><category term="gene-ontology;"/><category term="enrichment-analysis"/><summary type="html"><![CDATA[1. Introduction]]></summary></entry></feed>