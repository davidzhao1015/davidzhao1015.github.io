<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://davidzhao1015.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://davidzhao1015.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-01T05:12:24+00:00</updated><id>https://davidzhao1015.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">When Excel Maps Fall Short: A Pragmatic Labeling Solution for HEOR Analysts</title><link href="https://davidzhao1015.github.io/blog/2026/label-excel-map/" rel="alternate" type="text/html" title="When Excel Maps Fall Short: A Pragmatic Labeling Solution for HEOR Analysts"/><published>2026-01-30T00:00:00+00:00</published><updated>2026-01-30T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2026/label-excel-map</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2026/label-excel-map/"><![CDATA[<h2 id="problem">Problem</h2> <p>Excel is one of the most widely used data tools among HEOR analysts. Its built-in map chart functionality offers a quick and intuitive way to transform complex numerical outputs into visually engaging and interpretable graphics.</p> <p>In my own work, I frequently rely on Excel map charts for client-facing slide decks. Compared with Python or R, Excel allows for rapid iteration without requiring additional programming, which is especially valuable under tight timelines.</p> <p>However, a recurring limitation arises when using Excel for epidemiological mapping: country or region names cannot always be fully displayed on the map. This results in unlabeled areas, reduced clarity, and potential misinterpretation, particularly when presenting to non-technical stakeholders.</p> <p>I initially tried to address this by manually labeling individual countries, but this approach quickly became tedious and impractical, especially for global or multi-region analyses. This led me to ask a simple question: <em>Is there a smarter way to improve labeling in Excel map charts?</em></p> <p>The answer, fortunately, is yes.</p> <p>This blog post shares a simple yet effective workaround that improves the interpretability of Excel map charts while preserving their ease of use. The approach complements Excel’s limitations rather than fighting against them.</p> <h2 id="solution">Solution</h2> <p>The workaround is based on three key steps:</p> <ul> <li><strong>Map country identifiers (IDs)</strong> to the Excel map chart instead of the epidemiological variable itself (e.g., population size or prevalence).</li> <li><strong>Adjust the default color gradient</strong> to a single, uniform color, since the mapped values (country IDs) are not intended to convey magnitude.</li> <li><strong>Attach a companion table</strong> that links each country ID to the country name and the epidemiological variable of interest (e.g., population size or disease prevalence).</li> </ul> <p>Together, these steps allow Excel map charts to remain visually clean while restoring interpretability through a structured, readable reference, making them suitable for both analytical review and client-facing communication.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Excel_map_walkaround.png" sizes="95vw"/> <img src="/assets/img/Excel_map_walkaround.png" class="img-fluid rounded z-depth-1" width="700" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Excel map chart illustrating the proposed workaround</figcaption> </figure> </div> <h2 id="why-this-works">Why This Works</h2> <p>This approach works by separating visual structure from data interpretation.</p> <p>Excel map charts are well suited for showing geographic coverage but are limited in labeling and dense annotation. Mapping country IDs, rather than epidemiological values, uses the map purely as a spatial scaffold, avoiding misleading color gradients and false signals of magnitude. Applying a single, neutral color reinforces that the map shows <em>where</em>, not <em>how much</em>.</p> <p>Interpretation is handled through a companion table, which provides precise country names and epidemiological metrics without spatial constraints. Together, the map and table deliver clarity without clutter.</p> <p>In short, this workaround leverages Excel’s strengths while bypassing its labeling limitations, resulting in a practical, interpretable visualization for HEOR and epidemiological communication.</p> <h2 id="heor-takeaway">HEOR Takeaway</h2> <p>This figure exemplifies a pragmatic HEOR principle: Use the visualization to answer one clear question, and move everything else out of the chart.</p> <p>The map answers “Which countries are included?”</p> <p>The table answers “What are the values?”</p> <p>That clarity is exactly what makes this solution defensible, scalable, and client-safe.</p>]]></content><author><name>Xin (David) Zhao</name></author><summary type="html"><![CDATA[Problem]]></summary></entry><entry><title type="html">Save Time in Excel: Automating Navigation Menus with VBA</title><link href="https://davidzhao1015.github.io/blog/2026/auto-navigation-menu/" rel="alternate" type="text/html" title="Save Time in Excel: Automating Navigation Menus with VBA"/><published>2026-01-27T00:00:00+00:00</published><updated>2026-01-27T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2026/auto-navigation-menu</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2026/auto-navigation-menu/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Excel remains one of the most widely used tools in health economics and outcomes research (HEOR) modeling. As model engines grow in size and complexity, they often span dozens of interlinked worksheets, making clear and consistent navigation essential for both model developers and reviewers.</p> <p>In practice, however, building and maintaining navigation menus across a multi-sheet workbook is tedious. Creating buttons, copying formats, updating links, and keeping everything aligned can easily consume hours of repetitive work. This is a frustration I’ve shared with many colleagues, and experienced repeatedly myself, especially when models evolve rapidly and navigation needs to be rebuilt again and again.</p> <p>To address this, I developed a VBA-powered navigation tool that automates the process. Instead of relying on extensive copy-and-paste workflows, analysts can generate clean, consistent navigation menus across individual sheets with just one or two clicks, saving time and reducing manual errors.</p> <p>In this post, I’ll introduce the core features of this tool, demonstrate how it works using a real-world model engine, and share how I plan to further refine it to better support the everyday needs of HEOR analysts.</p> <h2 id="key-capabilities-of-the-initial-release">Key Capabilities of the Initial Release</h2> <p>While this tool will continue to be developed and maintained iteratively, the first version already delivers a practical and time-saving solution for generating clean, hierarchical navigation menus in Excel model engines. It is designed with HEOR analysts in mind, reflecting the real-world needs and workflows of colleagues on my own team.</p> <p><strong>Core features in Version 1 include:</strong></p> <ul> <li><strong>Two-level hierarchical navigation menus</strong> for clear structure across complex workbooks</li> <li><strong>Fully customizable menu headers</strong> for both navigation levels</li> <li><strong>Selective inclusion of worksheets</strong>, allowing analysts to control which sheets appear in the menu</li> <li><strong>Adjustable alignment and positioning</strong> to fit different workbook layouts</li> <li><strong>Customizable color palettes</strong> to support branding or internal style guides</li> <li><strong>Ability to run VBA externally</strong>, without embedding code directly into the model workbook</li> <li><strong>User-friendly parameter controls</strong>, enabling customization without requiring VBA knowledge</li> </ul> <p>Together, these features aim to reduce repetitive setup work, improve consistency across sheets, and make large Excel-based model engines easier to navigate, for both model developers and reviewers.</p> <h2 id="how-to-use-the-tool">How to Use the Tool</h2> <p>The navigation tool is designed to work seamlessly with almost any Excel workbook, and is particularly well suited for epidemiology and HEOR model engines. Because the VBA runs externally, you don’t need to copy or embed any code into your project workbook, keeping models clean and review-friendly.</p> <p>Getting started takes just three simple steps.</p> <h3 id="step-1-copy-the-interface-sheet-into-your-target-workbook">Step 1: Copy the interface sheet into your target workbook</h3> <p>From the template workbook, locate the NavMenuMaker sheet.</p> <p>Use Excel’s Move or Copy… option to copy this single sheet into your project workbook.</p> <p>That’s the only sheet you need.</p> <h3 id="step-2-define-menu-headers-and-layout">Step 2: Define menu headers and layout</h3> <p>In the NavMenuMaker sheet, enter your preferred navigation headers in the designated fields. You can choose between:</p> <ul> <li>A single-level menu, or</li> <li>A two-level hierarchical menu, depending on your model structure</li> </ul> <p>From the same interface, you may also adjust:</p> <ul> <li>Menu alignment and position</li> <li>Color palette for branding or visual clarity</li> </ul> <p>Once the structure is defined, click Confirm and let the VBA handle the rest.</p> <p><strong>(Please keep the template workbook open while the tool is running.)</strong></p> <p>A live preview of the navigation menu appears almost instantly. You can revise headers or layout and rerun the tool as needed.</p> <h3 id="step-3-review-navigation-across-selected-sheets">Step 3: Review navigation across selected sheets</h3> <p>Finally, verify that the navigation menu is correctly placed across all selected sheets. If adjustments are needed, simply return to NavMenuMaker, update the parameters, and rerun the tool.</p> <p>The result is a fully linked navigation menu on each sheet, with clear visual highlighting of the active sheet, making large model engines easier to explore and review.</p> <p>That’s it. With just a few clicks, you can eliminate repetitive setup work and focus your time where it matters most: analysis and decision-making.</p> <p>I hope this tool saves you a meaningful amount of time, enjoy trying it out.</p> <h2 id="download-the-free-tool">Download the Free Tool</h2> <p>You’re welcome to download the first version of the tool for free from the <a href="https://github.com/davidzhao1015/auto-sheet-navigator/blob/262daac5d87867eb29080c6e988de3cf022a7b74/Working/Auto_nav_menu_VBA.xlsm">GitHub repository</a> if the introduction has sparked your interest.</p> <p>I plan to continue updating and refining the tool to make it even more useful for real-world modeling workflows.</p>]]></content><author><name>Xin (David) Zhao</name></author><summary type="html"><![CDATA[A practical VBA tool for cleaner, faster model engines]]></summary></entry><entry><title type="html">Worked Example for Reproducible Power Analysis Workflow in Python</title><link href="https://davidzhao1015.github.io/blog/2026/power-analysis-notebook/" rel="alternate" type="text/html" title="Worked Example for Reproducible Power Analysis Workflow in Python"/><published>2026-01-05T00:00:00+00:00</published><updated>2026-01-05T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2026/power-analysis-notebook</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2026/power-analysis-notebook/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/demo-power-analysis-evident.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="power-analysis;"/><category term="Jupyter;"/><category term="Python;"/><category term="Evident"/><summary type="html"><![CDATA[Power analysis for alpha diversity differences in Crohn's disease microbiome studies]]></summary></entry><entry><title type="html">Power Analysis as a Workflow for Microbiome Data</title><link href="https://davidzhao1015.github.io/blog/2026/power-analysis/" rel="alternate" type="text/html" title="Power Analysis as a Workflow for Microbiome Data"/><published>2026-01-03T00:00:00+00:00</published><updated>2026-01-03T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2026/power-analysis</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2026/power-analysis/"><![CDATA[<h2 id="background--motivation-why-reproducible-power-analysis-matters">Background &amp; Motivation: Why reproducible power analysis matters</h2> <p>In microbiome research, whether in human cohorts or animal trials, determining the appropriate sample size is essential. Studies that are too small lack the statistical power to detect meaningful biological effects, while studies that are unnecessarily large risk wasting resources and raising ethical concerns.</p> <p>Despite this, power analysis is frequently misapplied in microbiome studies.</p> <p>As both a data analyst and an academic journal reviewer, I often encounter microbiome studies that are underpowered. Importantly, these studies are rarely weakened because authors lack statistical knowledge. More often, they fall short because the assumptions underlying the power analysis are unclear, inconsistently applied, or difficult to reproduce. Common reviewer concerns include effect sizes chosen without justification, limited exploration of alternative assumptions, and power calculations that cannot be revisited when study design choices are questioned.</p> <p>These challenges are amplified by the high variability and sparsity characteristic of microbiome data, whether derived from marker-gene or shotgun sequencing. For example, a trial designed to detect differences in gut microbial diversity between treatment and control groups may fail, not because the analysis is incorrect, but because the sample size is insufficient to capture the underlying biological signal given the chosen assumptions.</p> <p>In my experience, studies that navigate peer review successfully tend to treat power analysis not as a one-off calculation, but as a structured and revisitable analytical workflow. Assumptions are made explicit, alternatives can be explored, and results can be updated transparently when reviewers ask “what if?”. This perspective motivates the framework outlined in this article and sets the stage for its executable implementation.</p> <h2 id="scope--purpose-what-this-workflow-covers">Scope &amp; Purpose: What this workflow covers</h2> <p>This article introduces the core biostatistical concepts behind power analysis and outlines a high-level analytical workflow.</p> <p>While this conceptual foundation is often sufficient to understand <em>what</em> decisions are required, applying power analysis in real studies typically involves iterating across assumptions, adapting to specific experimental designs, and selecting appropriate software tools.</p> <p>These practical considerations are beyond the scope of this introductory post, but they follow directly from the workflow described here and motivate its executable implementation.</p> <h2 id="core-concepts-informing-the-workflow">Core concepts informing the workflow</h2> <p>Before diving into sample size calculation and power analysis, it is important to revisit a few fundamental biostatistical concepts. As a microbiologist turned data analyst, my goal here is to explain these ideas as simply and accurately as possible.</p> <ul> <li>Type I Error (α error): When comparing an experimental group to a control group, we test whether an observed difference is real or due to chance. If our test statistic exceeds a critical threshold (based on the chosen significance level, α), we reject the null hypothesis. However, there is always a probability - commonly set at 5% - that we reject the null hypothesis even when it is actually true. This false-positive conclusion is called a Type I error.</li> <li>Type II Error (β error): A Type II error occurs when we fail to reject the null hypothesis even though a true difference exists between groups. In other words, it represents a missed detection. The probability of making this error is denoted by β.</li> <li>Power (1 – β): Statistical power is the probability of correctly detecting a true difference if it exists. A study with higher power is more likely to identify meaningful effects, while a low-powered study risks overlooking them.</li> <li>Significance Level (α): The significance level is the threshold probability of observing extreme results under the null hypothesis. It defines the cut-off for deciding whether an observed difference is statistically significant.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PowerAnalysis_FIg1_PowerOfTest.png" sizes="95vw"/> <img src="/assets/img/PowerAnalysis_FIg1_PowerOfTest.png" class="img-fluid rounded z-depth-1" width="700" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1. Illustration of statistical power (source: Wikipedia)</figcaption> </figure> </div> <ul> <li>Effect Size: Effect size quantifies the magnitude of difference in a biological or clinical outcome—for example, changes in alpha-diversity, relative abundance of taxa, or other microbiome-related metrics. The appropriate measure of effect size depends on the type of outcome data (continuous, categorical, or count-based). Researchers often obtain plausible effect size estimates from pilot studies or prior literature. Importantly, the smaller the effect size one aims to detect, the larger the sample size and power required to ensure reliable results.</li> <li>Population variance: The population variance quantifies spread of the outcome variable, which is an essential component in calculating critical values and then power. While it is often unknown, one can infer population variance from sample variance reported in pilot studies or literature.</li> <li>Power curve: A graphical tool that shows how statistical power changes with sample size. The upward-sloping curve illustrates that larger sample sizes generally lead to higher statistical power.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PowerAnalysis_Fig2_Sample_Sizes_Effect_on_Power.png" sizes="95vw"/> <img src="/assets/img/PowerAnalysis_Fig2_Sample_Sizes_Effect_on_Power.png" class="img-fluid rounded z-depth-1" width="700" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2. Illustration of power curve (source: Wikipedia)</figcaption> </figure> </div> <p>Together, these concepts form the foundation for sample size determination and power analysis, helping researchers balance the risks of false positives, false negatives, and practical feasibility when designing experiments.</p> <h2 id="workflow-for-reproducible-power-analysis">Workflow for reproducible power analysis</h2> <h3 id="step-1-define-the-research-question">Step 1. Define the research question</h3> <p>Begin by clearly articulating the objective of your study. The following guiding questions can help refine the research question:</p> <ul> <li>What outcome do you aim to measure or detect?</li> <li>Does the analysis involve comparing outcomes between two or more groups, or modeling associations using regression?</li> </ul> <p>Example: Detecting a difference in Shannon diversity between cases and controls.</p> <h3 id="step-2-choose-the-primary-outcome">Step 2. Choose the primary outcome</h3> <p>Identify the type of primary outcome, as this will influence both the statistical test and the power calculation approach.</p> <p>Common outcome types include:</p> <ul> <li>Continuous</li> <li>Proportion</li> <li>Binary</li> <li>Multivariate distance-based measures</li> </ul> <h3 id="step-3-specify-the-statistical-test">Step 3. Specify the statistical test</h3> <p>Select the statistical test that aligns with the research question and outcome types.</p> <ul> <li>The chosen test determines the statistical distribution used in the power calculation.</li> <li>Common examples include t-test, ANOVA, regression models, PERMANOVA, χ² tests, and others.</li> </ul> <h3 id="step-4-specify-effect-size">Step 4. Specify effect size</h3> <p>Define the <em>minimum scientifically meaningful difference</em> you want to detect.</p> <p>Effect size can be informed by:</p> <ul> <li>Pilot data</li> <li>Previous literature</li> <li>Clinician or domain-expert input</li> <li>Biological relevance (not purely statistical considerations)</li> </ul> <p>Common effect size metrics include:</p> <ul> <li>Cohen’s d (difference in means)</li> <li>R² in PERMANOVA</li> <li>Odds ratio</li> <li>hazard ratio</li> <li>Fold change in abundance</li> </ul> <h3 id="step-5-specify-design-parameters">Step 5. Specify design parameters</h3> <p>Set the key parameters that govern hypothesis testing:</p> <ul> <li>α (significance level), typically 0.05</li> <li>Power, commonly 0.80 or 0.90</li> <li>One-sided versus two-sided tests</li> <li>Multiple testing correction, if applicable</li> </ul> <h3 id="step-6-perform-the-power-or-sample-size-calculation">Step 6. Perform the power or sample size calculation</h3> <p>Estimate the required sample size using:</p> <ul> <li>The specified effect size</li> <li>The selected statistical test</li> <li>Chosen α and power levels</li> <li>The corresponding test-specific distribution (e.g., <em>t</em>, <em>Z</em>, χ², <em>F</em>, noncentral <em>F</em>)</li> </ul> <p>This can be done analytically using closed-form formulas or via simulation when assumptions are complex.</p> <h3 id="step-7-interpret-the-results">Step 7. Interpret the results</h3> <p>Finally, interpret the power analysis results in the context of study feasibility, scientific relevance, and potential limitations, and assess whether adjustments to the design are necessary.</p> <h2 id="iteration-pain-point--why-a-template-helps">Iteration pain point &amp; Why a template helps</h2> <p>In practice, researchers rarely run power analysis just once. Effect sizes are uncertain, reviewers request alternative assumptions, and multiple outcomes must be evaluated. Repeating these steps manually, especially across different tests or designs, quickly becomes time-consuming and error-prone.</p> <h2 id="summary-and-practical-next-steps-with-notebook-template">Summary and practical next steps (with Notebook template)</h2> <p>The workflow above defines <em>what</em> decisions are required for power analysis in microbiome studies.</p> <p>In practice, these steps are rarely executed once. Researchers often:</p> <ul> <li>iterate across multiple outcomes,</li> <li>explore a range of plausible effect sizes,</li> <li>respond to reviewer requests for alternative assumptions,</li> <li>and document results in a reproducible way.</li> </ul> <p>To support this process, I implemented the same workflow in a Python <a href="https://davidzhao1015.github.io/blog/2026/power-analysis-notebook/">notebook template</a>, where each step maps directly to code cells and visual outputs.</p>]]></content><author><name></name></author><category term="power-analysis,"/><category term="sample-size,"/><category term="microbiome-data"/><summary type="html"><![CDATA[A practical workflow for study design, reviewer-ready assumptions, and reproducible power analysis]]></summary></entry><entry><title type="html">How to Reconstruct a Kaplan–Meier Curve from a Published Life Table: A Step-by-Step Beginner’s Guide Using Python</title><link href="https://davidzhao1015.github.io/blog/2025/lifetable-km-curve/" rel="alternate" type="text/html" title="How to Reconstruct a Kaplan–Meier Curve from a Published Life Table: A Step-by-Step Beginner’s Guide Using Python"/><published>2025-12-01T00:00:00+00:00</published><updated>2025-12-01T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/lifetable-km-curve</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/lifetable-km-curve/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In rare-disease research, meta-analyses and systematic reviews often rely on published literature rather than raw individual patient data (IPD). For many analysts new to the field, it’s not immediately obvious that <strong>survival curves can still be reconstructed, even when IPD is unavailable</strong>.</p> <p>This tutorial demonstrates <strong>how to rebuild a Kaplan-Meier (KM) survival curve from a published life table</strong>, step by step. Once the survival function is reconstructed, you can recover meaningful time-to-event insights such as the <strong>median time to symptom onset</strong>, which is especially valuable for rare diseases where sample sizes are small and reporting formats vary.</p> <h3 id="what-you-will-learn">What you will learn</h3> <p>By the end of this tutorial, you will understand:</p> <ul> <li>How to convert a published life table into an estimated Kaplan–Meier survival curve</li> <li>How to approximate the number of events in each interval</li> <li>How to generate simulated (“pseudo”) individual-level data</li> <li>How to fit a KM model in Python using the <strong>lifelines</strong> package</li> <li>How to visualize the survival function and extract median event times</li> </ul> <h3 id="prerequisites">Prerequisites</h3> <ul> <li>Basic familiarity with Python (e.g., using pandas)</li> <li>No prior survival-analysis knowledge required <br/><br/></li> </ul> <h2 id="what-is-a-life-table-and-why-does-it-matter">What Is a Life Table, and Why Does It Matter?</h2> <p>Before jumping into code, it’s worth understanding what a life table represents.</p> <p>A <strong>life table</strong> reports the number of individuals who remain event-free at specific follow-up times. Even though it does not contain individual observations, it still retains sufficient structure to approximate a survival pattern.</p> <p>Reconstructing a KM curve from a life table is particularly useful when:</p> <ul> <li>Only aggregate results are published (common in <strong>retrospective cohorts</strong>)</li> <li>Raw KM curves are missing or provided only as images</li> <li>Conducting <strong>meta-analyses for rare diseases</strong>, where IPD is rarely available</li> <li>You need a quantitative estimate of survival metrics (median, percentiles, etc.)</li> </ul> <p>With a bit of algebra and simulation, we can recover a reasonable approximation of the original survival function.<br/><br/></p> <h2 id="workflow-overview">Workflow Overview</h2> <p>To help beginners visualize the full process, here is the conceptual workflow for reconstructing a Kaplan–Meier curve from a life table:</p> <ol> <li> <p>Import Python libraries: Load packages for data manipulation (pandas), modeling (lifelines), and visualization (matplotlib).</p> </li> <li> <p>Convert the life-table counts into a DataFrame: Enter (or load) the event-free counts at each follow-up time.</p> </li> <li> <p>Calculate the number of events per interval: Events are approximated using the drop in survival counts between time points.</p> </li> <li> <p>Expand the data into “pseudo” individual-level records: Each row becomes a simulated patient, allowing us to fit a survival model.</p> </li> <li> <p>Fit a Kaplan–Meier model using lifelines.KaplanMeierFitter: Use the simulated data to estimate the survival function.</p> </li> <li> <p>Visualize the KM curve: Plot the reconstructed survival curve to inspect its shape and behavior.</p> </li> <li> <p>Extract key survival metrics: Compute median time to event, percentiles, and other summary statistics.</p> </li> </ol> <p>This workflow provides a practical way to derive time-to-event insights when access to raw patient-level data is limited.<br/><br/></p> <h2 id="implementation">Implementation</h2> <p>Next, I will walk you through the Python code step by step, briefly highlighting the key outputs and how to interpret them.</p> <p>Steps 3 and 4 are the most important for simulating the Kaplan–Meier function and curve, and are worth particular attention during review.<br/><br/></p> <p><strong>Step 1: Import Python libraries</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">lifelines</span> <span class="c1"># Survival analysis library
</span><span class="kn">from</span> <span class="n">lifelines</span> <span class="kn">import</span> <span class="n">KaplanMeierFitter</span> <span class="c1"># Fit Kaplan-Meier model
</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div> <p><br/></p> <p><strong>Step 2. Convert the life-table counts into a DataFrame</strong></p> <p>The life table was extracted from a research paper describing the onset of interstitial lung disease (ILD) in a cohort. ILD is a symptom in which lung tissue becomes damaged and cannot fully expand or fill with air.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create lifetable DataFrame from given data
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">year</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">symptoms_free</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">745</span><span class="p">,</span> <span class="mi">592</span><span class="p">,</span> <span class="mi">382</span><span class="p">,</span> <span class="mi">262</span><span class="p">,</span> <span class="mi">171</span><span class="p">,</span> <span class="mi">97</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th>year</th> <th>symptoms_free</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>745</td> </tr> <tr> <td>1</td> <td>592</td> </tr> <tr> <td>2</td> <td>382</td> </tr> <tr> <td>3</td> <td>262</td> </tr> <tr> <td>4</td> <td>171</td> </tr> <tr> <td>5</td> <td>97</td> </tr> </tbody> </table> <p>The life table reports the number of patients who remained free of the symptom of interest over a follow-up period of up to five years.</p> <p>It provides the number of patients surviving without ILD from year 1 through year 5, as well as the number at risk at year 0, corresponding to the index date of the retrospective cohort.<br/><br/></p> <p><strong>Step 3: Calculate the number of events per interval</strong></p> <p>Next, we need to back-calculate the number of patients who developed the symptom during each time interval.</p> <p>We assume that no patients were censored and that each individual has a binary status, either with or without the symptom. Under this assumption, the number of events in a given year can be calculated as the difference between the number of symptom-free patients in the previous year and the number of symptom-free patients in the current year.</p> <p>For example, the number of events in year 1 equals the number of symptom-free patients at year 0 minus the number of symptom-free patients at year 1.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute events per interval (from symptoms_free counts)
</span><span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">events</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">symptoms_free</span><span class="sh">"</span><span class="p">].</span><span class="nf">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">symptoms_free</span><span class="sh">"</span><span class="p">]</span>
<span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">events</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># no event at time 0
</span><span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th>year</th> <th>symptoms_free</th> <th>events</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>745</td> <td>0</td> </tr> <tr> <td>1</td> <td>592</td> <td>153</td> </tr> <tr> <td>2</td> <td>382</td> <td>210</td> </tr> <tr> <td>3</td> <td>262</td> <td>120</td> </tr> <tr> <td>4</td> <td>171</td> <td>91</td> </tr> <tr> <td>5</td> <td>97</td> <td>74</td> </tr> </tbody> </table> <p><strong>4. Expand the data into “pseudo” individual-level records</strong></p> <p>Next, we recreate an individual-level dataset that includes each patient’s observed time and symptom-free status, with one row per individual. In this dataset, a value of 1 indicates that the patient developed the symptom, while 0 indicates that the patient remained symptom-free.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create expanded individual-level dataset for Kaplan-Meier fitting
</span><span class="n">records</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
    <span class="n">year</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">year</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">events</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">events</span><span class="sh">"</span><span class="p">])</span>
    <span class="c1"># event cases
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">events</span><span class="p">):</span>
        <span class="n">records</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">:</span> <span class="n">year</span><span class="p">,</span> <span class="sh">"</span><span class="s">event</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="sh">"</span><span class="s">symptoms_free</span><span class="sh">"</span><span class="p">])):</span>
    <span class="n">records</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="sh">"</span><span class="s">event</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>

<span class="n">df_long</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">records</span><span class="p">)</span>
</code></pre></div></div> <p>We then verify that the reconstructed dataset contains the correct number of patients at risk (n = 745).<br/><br/></p> <p><strong>Step 5: Fit a Kaplan–Meier model using lifelines.KaplanMeierFitter</strong></p> <p>We will fit a Kaplan–Meier function using the reconstructed dataset, applying the KaplanMeierFitter function from the lifelines module.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit Kaplan-Meier model
</span><span class="n">km</span> <span class="o">=</span> <span class="nc">KaplanMeierFitter</span><span class="p">()</span>
<span class="n">km</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">durations</span><span class="o">=</span><span class="n">df_long</span><span class="p">[</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">],</span> <span class="n">event_observed</span><span class="o">=</span><span class="n">df_long</span><span class="p">[</span><span class="sh">"</span><span class="s">event</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div> <p><br/></p> <p><strong>Step 6: Visualize the KM curve: Plot the reconstructed survival curve to inspect its shape and behavior</strong></p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot survival function
</span><span class="n">km</span><span class="p">.</span><span class="nf">plot_survival_function</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Kaplan-Meier Symptom Free Curve</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time (years)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Symptom Free Probability</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lifetable_km.png" sizes="95vw"/> <img src="/assets/img/lifetable_km.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure.</figcaption> </figure> </div> <p>The survival curve illustrates the Kaplan–Meier function from the index year through the end of the follow-up period in the retrospective cohort study.</p> <p>Each downward step within a time interval reflects the assumption that patients in that interval have a uniform probability of remaining symptom-free.<br/></p> <p><strong>Step 7: Extract key survival metrics</strong></p> <p>We are able to obtain the median time to symptom onset directly from the Kaplan–Meier function. This is useful for understanding the overall pattern of symptom development in the cohort.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Print median survival time
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Median Symptom Free Time:</span><span class="sh">"</span><span class="p">,</span> <span class="n">km</span><span class="p">.</span><span class="n">median_survival_time_</span><span class="p">)</span>
</code></pre></div></div> <p><br/><br/></p> <h2 id="method-limitations">Method Limitations</h2> <p>While reconstructing a Kaplan–Meier curve from a life table is useful, the approach has several inherent constraints:</p> <ul> <li>Assumes no censoring: Life tables do not distinguish between events and loss to follow-up. This method treats all declines as events, which may inflate event rates if censoring occurred.</li> <li>Assumes uniform timing of events: Events are distributed evenly across each interval, even though real-world data may show clustering that cannot be recovered without individual-level data.</li> <li>Not true individual-level data: The expanded dataset matches counts but does not reflect actual patient trajectories. Results should be interpreted as approximations.</li> <li>Limited by table granularity: Annual life-table data can only produce annual-level survival patterns. Finer intervals (e.g., monthly) would improve accuracy but are rarely available in published studies.<br/><br/></li> </ul> <h2 id="final-takeaways">Final Takeaways</h2> <p>Reconstructing a Kaplan–Meier curve from a life table is far simpler than it initially appears.</p> <p>With only a few rows of published counts and basic Python code, you can approximate a full survival curve and extract meaningful time-to-event insights.</p> <p>Here are the key lessons from this guide:</p> <ul> <li>Life tables contain enough information to rebuild an approximate survival function.</li> <li>By computing interval events and expanding them into pseudo–individual-level records, we can fit a KM model without IPD.</li> <li>Tools like <code class="language-plaintext highlighter-rouge">lifelines</code> make it easy to visualize survival curves and extract median survival time or other summary statistics.</li> <li>This workflow is especially valuable in systematic reviews, rare disease epidemiology, and health economic modeling, where KM curves are needed but individual-level data are rarely provided.</li> </ul> <p>Overall, this method gives researchers a pragmatic way to extract more value from published evidence, and enables a deeper understanding of survival patterns even when the original dataset is inaccessible.<br/><br/></p> <h2 id="download-the-full-notebook-and-try-it-yourself">Download the Full Notebook and Try It Yourself</h2> <p>If you’d like to explore the complete code, visualizations, and step-by-step logic, you can download the full notebook from my <a href="https://github.com/davidzhao1015/fit-km-model-from-lifetable/blob/300ea9d688650fb227d70e60e74094f34e8bfb9c/fit-km-model-from-lifetable.ipynb">GitHub repository</a>.</p> <p>The notebook includes:</p> <ul> <li>all Python code used in this tutorial</li> <li>expanded comments for beginners</li> <li>the reconstructed KM curve</li> </ul> <p>Feel free to adapt the notebook for your own meta-analysis, rare disease research, or health economic modeling projects.</p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="survival-analysis,"/><category term="evidence-synthesis,"/><category term="python,"/><category term="kaplan-meier,"/><category term="life-table"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">From Practice to Framework: Reusable PSA Tools for Cost-Effectiveness and Epidemiology Modelling</title><link href="https://davidzhao1015.github.io/blog/2025/psa-tools/" rel="alternate" type="text/html" title="From Practice to Framework: Reusable PSA Tools for Cost-Effectiveness and Epidemiology Modelling"/><published>2025-11-27T00:00:00+00:00</published><updated>2025-11-27T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/psa-tools</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/psa-tools/"><![CDATA[<h2 id="background">Background</h2> <p>This article is intended to share practical learning and hands-on experience with probabilistic sensitivity analysis (PSA), drawn from real-world health-economic work in rare-disease therapy.</p> <p>It provides a walkthrough of a reusable PSA architecture, illustrated with use cases, and introduces a probability-distribution inventory with a quick-selection guide for choosing appropriate distributions in PSA. Although the examples come from one working project, the framework is broadly applicable to cost-effectiveness and epidemiology modelling projects that require sensitivity analysis.</p> <p>The article focuses on the applied aspects of PSA, how to use statistical probability distributions, Excel’s built-in formulas, and backend VBA to implement simulations. It intentionally does not cover the theoretical foundations of statistical distributions or general Excel functionality, which are beyond its intended scope.</p> <h2 id="conceptual-workflow-for-implementing-uncertainty-simulation-in-psa">Conceptual workflow for implementing uncertainty simulation in PSA</h2> <h3 id="problem-and-solution">Problem and Solution</h3> <p>The primary goal of PSA is to quantify how uncertainty in model inputs affects the outputs of a cost-effectiveness (CE) model. PSA uses statistical simulation, typically Monte Carlo simulation, to generate thousands of model runs, each drawing inputs from predefined, plausible uncertainty ranges.</p> <p>From my learning and real-world project experience, two steps are often the most challenging when implementing PSA in Excel:</p> <ol> <li>building a robust PSA architecture (in-cell formulas and VBA), and</li> <li>selecting appropriate probability distributions for each input parameter.</li> </ol> <p>To address these challenges, I’ve developed a set of <strong>reusable PSA tools</strong> designed to make these steps easier and more consistent across projects.</p> <h3 id="conceptual-workflow">Conceptual Workflow</h3> <p>The high-level workflow for PSA includes:</p> <ol> <li>Select an appropriate probability distribution</li> <li>Retrieve deterministic inputs and standard deviations from references</li> <li>Parameterize the selected distribution</li> <li>Generate random samples</li> <li>Run the simulation</li> </ol> <p>The following sections walk through the architecture and each step in more detail.</p> <h2 id="introduction-to-two-reusable-tools">Introduction to two reusable tools</h2> <h3 id="first-tool-psa-architecture-in-excel">First Tool: PSA Architecture in Excel</h3> <p>The screenshot below illustrates the main PSA architecture in Excel, with four framed sections corresponding to the first four steps of the PSA workflow.</p> <p>I’ve also uploaded the Excel workbook containing this PSA setup for anyone interested in reviewing or applying it. Please refer to the <em>“Generate_uncertainty”</em> sheet for the full implementation.</p> <p>In brief, the visible portion of the PSA architecture includes four components:</p> <ol> <li>Selection of probability-distribution types</li> <li>Entry of deterministic inputs and standard deviations</li> <li>Parameterization of the chosen distribution</li> <li>Setup for generating random samples</li> </ol> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig1_psa_architecture_snapshot.png" sizes="95vw"/> <img src="/assets/img/fig1_psa_architecture_snapshot.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1. PSA Architecture Snapshot</figcaption> </figure> </div> <p>The hidden portion contains reusable in-cell formulas that calculate distribution parameters and generate uncertainty-adjusted random inputs. These formulas support consistent and transparent PSA implementation across projects.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig2_psa_architecture_snapshot_formula.png" sizes="95vw"/> <img src="/assets/img/fig2_psa_architecture_snapshot_formula.png" class="img-fluid rounded z-depth-1" width="1000" height="800" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2. Example of embedded in-cell formulas within the PSA architecture</figcaption> </figure> </div> <h3 id="the-second-tool-probability-distribution-inventory">The Second Tool: Probability-Distribution Inventory</h3> <p>Choosing an appropriate probability distribution for each model input is the first and most critical step in PSA.</p> <p>As demonstrated in the ISPOR good practice guidelines for uncertainty analysis, high-quality research requires selecting probability distributions that appropriately reflect the nature of the data being modeled. The guidelines also provide practical recommendations for choosing distributions tailored to different data types. The full paper is available <a href="https://www.valueinhealthjournal.com/article/S1098-3015(12)01659-2/fulltext?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1098301512016592%3Fshowall%3Dtrue">here</a>.</p> <p>This selection requires both an understanding of statistical distributions and domain knowledge in health economics or epidemiology.</p> <p>The inventory provides the key information needed to make informed decisions, including:</p> <ul> <li>the support (range) of each distribution,</li> <li>parameterization details,</li> <li>example use cases, and</li> <li>links to external references for further reading.</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig3_prob_dist_inventory.png" sizes="95vw"/> <img src="/assets/img/fig3_prob_dist_inventory.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3. Screenshot of the inventory of probability distributions commonly used in HE and Epidemiology</figcaption> </figure> </div> <p>The rightmost column also includes practical tips to support distribution selection in real-world modelling.</p> <p>A later section of this article will explain how to use this inventory more effectively.</p> <h2 id="end-to-end-walkthrough-of-the-excel-based-psa-architecture">End-to-end walkthrough of the Excel-based PSA architecture</h2> <p>As illustrated in Figure 1, this section walks through the four key PSA steps implemented in Excel, followed by an overview of the simulation-validation process and the dashboard used to visually inspect the results.</p> <h3 id="steps-corresponding-to-the-four-framed-sections">Steps (corresponding to the four framed sections)</h3> <ol> <li><strong>Select an appropriate probability distribution</strong> <ul> <li>Choose a suitable distribution for the parameter of interest. Refer to the <em>Inventory</em> sheet for guidance.</li> <li>This choice is critical because it determines the required parameterization.</li> <li>For demonstration, the sheet highlights three commonly used distributions: <strong>Beta, Gamma, and Lognormal</strong>.</li> </ul> </li> <li><strong>Enter deterministic inputs, standard deviation, and optional epsilon</strong> <ul> <li>Input the deterministic value (e.g., the mean) and its standard deviation, typically sourced from literature such as ICER (2019).</li> <li>An optional <strong>epsilon</strong> (e.g., 0.0001) can be used when the standard deviation is reported as zero; it is not needed when a non-zero SD is available.</li> </ul> </li> <li><strong>Parameterize the selected distribution</strong> <ul> <li>The model automatically calculates the required distribution parameters (e.g., Alpha, Beta, Mu, Sigma) using built-in formulas.</li> <li>Optional upper and lower limits may be entered when reported in the literature.</li> <li>Parameterization uses the previously entered mean, standard deviation, and, if applicable, epsilon.</li> </ul> </li> <li><strong>Generate random samples</strong></li> </ol> <p>Column J contains formulas that generate random samples based on the chosen distribution and its parameterization.</p> <h3 id="running-the-simulation">Running the simulation</h3> <p>To test the setup:</p> <ul> <li>Go to the <strong>Validate_simulation</strong> sheet</li> <li>Click <strong>Run simulation</strong></li> <li>Clear previous simulation results if needed</li> </ul> <p>The buttons are linked to established backend VBA.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig4_validation_simulation_psa.png" sizes="95vw"/> <img src="/assets/img/fig4_validation_simulation_psa.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4. Validation and simulation sheet screenshot</figcaption> </figure> </div> <p>Briefly, the Live row displays probability outputs for the current iteration (referencing the previous sheet), while the rows below it store results from all 1,000 iterations. Two user-friendly buttons on the right allow users to run or clear the backend VBA simulations.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig5_dashboard_psa.png" sizes="95vw"/> <img src="/assets/img/fig5_dashboard_psa.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 5. Partial screenshot of the simulation dashboard</figcaption> </figure> </div> <p>The Dashboard supports visual inspection of the simulation outputs. The histogram displays the distribution of randomly sampled values for the selected probability distribution, while the framed summary table on the right compares the mean and standard deviation of the sampled data against the deterministic inputs.</p> <h3 id="how-to-use-the-probability-distribution-inventory">How to use the probability-distribution inventory</h3> <p>As shown in Figure 3, users can begin with <strong>column “Tips for distribution selection”</strong> to quickly narrow down candidate probability distributions that align with the characteristics of the HE or Epi parameter of interest. From there, users can review essential properties of each candidate distribution, such as whether its support matches the actual range of the model input, and whether relevant use cases exist. The <strong>parameterization section</strong> and <strong>linked Wikipedia references</strong> provide additional guidance for implementing the distribution statistically.</p> <h2 id="conclusion">Conclusion</h2> <p>The two tools introduced here are designed to support users in implementing PSA in Excel through a reusable architecture, clear documentation, VBA-powered simulations, and intuitive visual inspection tools.</p> <p>The PSA architecture can be integrated into nearly any CE modelling project with only minor adjustments. The VBA scripts are also adaptable to other projects with minimal modifications.</p> <p>The probability-distribution inventory, along with its practical selection guide, helps users choose appropriate distributions for each model input with confidence.</p> <h2 id="excel-file-on-github">Excel file on Github</h2> <p>If you’re interested in exploring or applying these tools, you can download the macro-enabled Excel workbook, <strong>“PSA_Implementation_Tools_2025.11.24_DZ.xlsm,”</strong> from my GitHub repository (<a href="https://github.com/davidzhao1015/psa-tools-excel/blob/main/PSA_Implementation_Tools_2025.11.24_DZ.xlsm">here</a>).</p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="sensitivity-analysis,"/><category term="epi,"/><category term="health-economy,"/><category term="excel,"/><category term="vba"/><summary type="html"><![CDATA[Background]]></summary></entry><entry><title type="html">Parametric Survival Fit &amp;amp; PSA for Age-at-Onset in Autoimmune Disease: Generalized Gamma Workflow</title><link href="https://davidzhao1015.github.io/blog/2025/sensitivity-analysis-survival-model/" rel="alternate" type="text/html" title="Parametric Survival Fit &amp;amp; PSA for Age-at-Onset in Autoimmune Disease: Generalized Gamma Workflow"/><published>2025-11-15T00:00:00+00:00</published><updated>2025-11-15T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/sensitivity-analysis-survival-model</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/sensitivity-analysis-survival-model/"><![CDATA[<p>In rare disease epidemiology, researchers often face a familiar challenge: most published studies do not provide individual-level patient data. Instead, they report only high-level summary statistics such as quartiles, means, and total sample size. Yet for health economic modeling, burden of disease estimation, or natural history modeling, we still need to understand the full age-at-onset distribution, including its shape, skew, and uncertainty.</p> <p>This article presents a practical workflow for reconstructing that distribution using parametric survival models, with a focus on the generalized gamma distribution. We will also show how to incorporate parameter uncertainty using probabilistic sensitivity analysis (PSA) and how to compute key outputs such as diagnosis probabilities within specific age bands.</p> <p>This workflow is adapted from a full analysis <a href="sen-analysis-Gamma-MVN-Cholesky.ipynb">notebook</a>, and is intended to be immediately useful for epidemiologists, biostatisticians, and health economists working with sparse or aggregate-only datasets.</p> <h2 id="why-parametric-survival-modeling-matters">Why Parametric Survival Modeling Matters</h2> <p>When individual-level data are unavailable, modeling decisions become difficult:</p> <ul> <li>How do we estimate onset age distributions when we only know quartiles?</li> <li>How can downstream models quantify uncertainty?</li> <li>Can we still estimate age-specific probabilities, like onset before 12 or after 18?</li> </ul> <p>The answer is <em>yes</em>. Well-chosen parametric models—fit using a quantile-matching method—allow us to approximate the underlying distribution closely enough for epidemiological and health economic purposes.</p> <p>The approach also supports probabilistic uncertainty methods widely used in cost-effectiveness modeling.</p> <h2 id="summary-data-and-python-setup">Summary Data and Python Setup</h2> <h3 id="summary-statistics-used">Summary Statistics Used</h3> <p>The analysis uses the following literature-reported values. These values provide a robust representation of the distribution’s central tendency and spread.</p> <table> <thead> <tr> <th>Statistic</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Min</td> <td>19</td> </tr> <tr> <td>Q1</td> <td>61</td> </tr> <tr> <td>Median</td> <td>66</td> </tr> <tr> <td>Q3</td> <td>72</td> </tr> <tr> <td>Max</td> <td>88</td> </tr> <tr> <td>Mean</td> <td>67</td> </tr> <tr> <td>Sample Size</td> <td>111</td> </tr> </tbody> </table> <h3 id="software-environment">Software Environment</h3> <p>The full workflow uses:</p> <ul> <li><strong>NumPy</strong> for numerical operations</li> <li><strong>pandas</strong> for data structuring</li> <li><strong>SciPy</strong> for probability distributions</li> <li><strong>statsmodels</strong> for numerical Hessian estimation</li> <li><strong>matplotlib</strong> for visualization</li> </ul> <p>All version details are recorded to support reproducibility.</p> <h2 id="fitting-parametric-models-from-summary-data">Fitting Parametric Models from Summary Data</h2> <h3 id="why-use-parametric-distributions">Why Use Parametric Distributions?</h3> <p>Parametric survival distributions help us:</p> <ul> <li>Interpolate and extrapolate beyond observed quartiles</li> <li>Obtain smooth CDFs and PDFs</li> <li>Compute probabilities within arbitrary age bands</li> <li>Implement Monte Carlo–based uncertainty quantification</li> </ul> <p>This is particularly valuable in rare disease research, where datasets are often small.</p> <h3 id="quantile-matching-reconstructing-the-distribution">Quantile Matching: Reconstructing the Distribution</h3> <p>Because raw data are missing, we instead match:</p> \[(Q1, Median, Q3)_\text{model} \approx (Q1, Median, Q3)_\text{observed}\] <p>We define an objective function that penalizes deviations between model-predicted and observed quartiles, and we optimize the model’s parameters to minimize this squared error.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q1</span><span class="p">,</span> <span class="n">median</span><span class="p">,</span> <span class="n">q3</span> <span class="o">=</span> <span class="mi">61</span><span class="p">,</span> <span class="mi">66</span><span class="p">,</span> <span class="mi">72</span>
<span class="n">empirical_q</span> <span class="o">=</span> <span class="p">[</span><span class="n">q1</span><span class="p">,</span> <span class="n">median</span><span class="p">,</span> <span class="n">q3</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">gengamma_objective</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">scale</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="nf">gengamma</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">theo_q</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">ppf</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">theo_q</span><span class="p">)</span><span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">empirical_q</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span>
</code></pre></div></div> <p>This allows us to recover a plausible distribution consistent with the literature.</p> <h3 id="why-generalized-gamma">Why Generalized Gamma?</h3> <p>While lognormal and Weibull are widely used, the generalized gamma provides:</p> <ul> <li>high flexibility in skewness</li> <li>adjustable tail behavior</li> <li>ability to mimic many other survival distributions</li> </ul> <p>After fitting the model via quantile matching, we obtain parameter estimates for shape (a), power (c), and scale (s). These values define the final onset-age distribution.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">initial_guess_gengamma</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">]</span>
<span class="n">bounds_gengamma</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">0.01</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="bp">None</span><span class="p">)]</span>

<span class="n">result_gengamma</span> <span class="o">=</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">gengamma_objective</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">initial_guess_gengamma</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds_gengamma</span><span class="p">)</span>

<span class="n">a_fit_gengamma</span><span class="p">,</span> <span class="n">c_fit_gengamma</span><span class="p">,</span> <span class="n">scale_fit_gengamma</span> <span class="o">=</span> <span class="n">result_gengamma</span><span class="p">.</span><span class="n">x</span>
</code></pre></div></div> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>a</td> <td>26.6110</td> </tr> <tr> <td>c</td> <td>1.5835</td> </tr> <tr> <td>scale</td> <td>8.4092</td> </tr> </tbody> </table> <h3 id="diagnostic-checks">Diagnostic Checks</h3> <p>To validate the fit, we:</p> <ul> <li>simulate onset ages using the fitted parameters</li> <li>plot a histogram compared to the reported median</li> <li>overlay the fitted CDF against empirical quartile points</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/survival_model_diagnostic_check.png" sizes="95vw"/> <img src="/assets/img/survival_model_diagnostic_check.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This confirms that the generalized gamma model aligns well with both the median and quartile anchors.</p> <h2 id="adding-uncertainty-probabilistic-sensitivity-analysis-psa">Adding Uncertainty: Probabilistic Sensitivity Analysis (PSA)</h2> <h3 id="why-psa">Why PSA?</h3> <p>Health economic and epidemiological models require not only point estimates but also credible intervals, especially when clinical evidence is sparse.</p> <p>Parameter uncertainty in the fitted model must therefore be propagated to all downstream quantities.</p> <p>The overarching goal is to generate randomly sampled, correlated parameter sets that reflect the plausible variance and covariance structure of the original fitted models for PSA.</p> <p>The conceptual procedures are as follows,</p> <ol> <li>Identify the model parameters</li> <li>Build the variance–covariance matrix</li> <li>Calculate the Cholesky factor</li> <li>Generate random noise using MVN</li> <li>Apply the Cholesky factor to preserve correlations</li> <li>Compute the final random parameter draws</li> <li>Use the sampled parameters in the survival model</li> </ol> <h3 id="building-the-variance-covariance-matrix">Building the Variance-Covariance Matrix</h3> <p>We approximate the Hessian numerically at the parameter optimum. Inverting the Hessian yields the variance–covariance matrix of the parameter estimates.</p> <p>This matrix captures both individual parameter uncertainty and correlations among parameters.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">a_fit</span><span class="p">,</span> <span class="n">c_fit</span><span class="p">,</span> <span class="n">scale_fit</span><span class="p">])</span>
<span class="n">H</span> <span class="o">=</span> <span class="nf">approx_hess</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">,</span> <span class="n">gengamma_objective</span><span class="p">)</span>
<span class="n">vcov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: right"> </th> <th style="text-align: right">a variance/covariance</th> <th style="text-align: right">c variance/covariance</th> <th style="text-align: right">scale variance/covariance</th> </tr> </thead> <tbody> <tr> <td style="text-align: right">a</td> <td style="text-align: right">595.13</td> <td style="text-align: right">-17.76</td> <td style="text-align: right">-315.06</td> </tr> <tr> <td style="text-align: right">c</td> <td style="text-align: right">-17.76</td> <td style="text-align: right">0.55</td> <td style="text-align: right">9.63</td> </tr> <tr> <td style="text-align: right">scale</td> <td style="text-align: right">-315.06</td> <td style="text-align: right">9.63</td> <td style="text-align: right">169.34</td> </tr> </tbody> </table> <p>This is a 3×3 variance-covariance matrix for the 3 fitted parameters of the generalized gamma distribution.</p> <p>Diagonal entries = variances of each parameter.</p> <p>Example:</p> \[\mathrm{Var}(a) = 595.13, \quad \mathrm{Var}(c) = 0.551, \quad \mathrm{Var}(\text{scale}) = 169.34\] <p>Off-diagonal entries = covariances between parameters.</p> <p>Example:</p> \[\mathrm{Cov}(a, \text{scale}) = -315.06\] <h3 id="multivariate-normal-sampling">Multivariate Normal Sampling</h3> <p>Using Cholesky decomposition, we sample thousands of parameter sets from a multivariate normal distribution:</p> \[\theta^{(i)} \sim \text{MVN}(\hat{\theta},\, \Sigma)\] <p>This forms the backbone of the PSA.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">cholesky</span><span class="p">(</span><span class="n">vcov</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">standard_normal</span><span class="p">((</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">theta_draws</span> <span class="o">=</span> <span class="n">theta_hat</span> <span class="o">+</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">L</span><span class="p">.</span><span class="n">T</span>
</code></pre></div></div> <p>Even though the fitted parameters \((\mu, \log\sigma, Q)\) are point estimates, each has uncertainty, and they are correlated. For example, if log-sigma increases slightly, Q might decrease slightly to keep the model a good fit.</p> <p>To reflect this, we sample parameters from a multivariate normal distribution centered at the MLE estimates with covariance equal to the Hessian-derived covariance matrix.</p> <p>Here’s what the code does:</p> <ol> <li> <p>standard_normal((5000,3)) → generates 5,000 draws of independent noise, one for each parameter.</p> </li> <li> <p>L = cholesky(vcov) → finds a matrix L such that \(LL^\top = \text{covariance}\).<br/> This matrix converts independent noise into correlated noise with the correct variability.</p> </li> <li> <p>Z @ L.T → transforms the independent noise into noise that matches the covariance structure of the parameters.</p> </li> <li> <p>theta_hat + … → shifts the draws so they are centered at the fitted parameters.</p> </li> </ol> <p>The resulting theta_draws are 5,000 simulated sets of generalized gamma parameters that realistically reflect estimation uncertainty. These are used to visualize uncertainty in survival curves, median survival, RMST, or other model outputs.</p> <h2 id="estimating-age-band-probabilities">Estimating Age-Band Probabilities</h2> <p>For many models, we need to know the probability that onset occurs within:</p> <ul> <li>0–12 years</li> <li>12–18 years</li> <li>18+ years</li> </ul> <p>Using the CDF from each Monte Carlo draw, we compute:</p> \[P(a \leq \text{onset} &lt; b) = F(b) - F(a)\] <p>This avoids slow individual-level simulation and supports rapid PSA.</p> <p>A summary table (across ~5,000 draws) includes:</p> <ul> <li>mean probability</li> <li>standard deviation</li> <li>median</li> <li>95% uncertainty interval (2.5–97.5th percentile)</li> </ul> <table> <thead> <tr> <th>Age Band</th> <th>Mean</th> <th>SD</th> <th>Median</th> <th>CI Lower (2.5%)</th> <th>CI Upper (97.5%)</th> </tr> </thead> <tbody> <tr> <td>0–12</td> <td>2.67%</td> <td>14.00%</td> <td>0.00%</td> <td>0.00%</td> <td>44.47%</td> </tr> <tr> <td>12–18</td> <td>1.95%</td> <td>9.41%</td> <td>0.00%</td> <td>0.00%</td> <td>23.81%</td> </tr> <tr> <td>18+</td> <td>95.38%</td> <td>18.45%</td> <td>100.00%</td> <td>9.54%</td> <td>100.00%</td> </tr> </tbody> </table> <p>The results show:</p> <ul> <li>~95% of onsets occur after age 18</li> <li>0–12 and 12–18 have wide uncertainty due to very low event frequency</li> </ul> <p>This output can be plugged directly into disease models.</p> <h2 id="visualizing-uncertainty-cdf-ensembles">Visualizing Uncertainty: CDF Ensembles</h2> <p>One of the most insightful plots overlays:</p> <ul> <li>Light gray curves: thousands of CDFs generated from sampled parameters</li> <li>A blue line: mean CDF across all Monte Carlo draws</li> <li>Red points: empirical quartile anchors</li> </ul> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/psa_diag_check.png" sizes="95vw"/> <img src="/assets/img/psa_diag_check.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This reveals:</p> <ul> <li>Strong alignment near the median</li> <li>Greatest uncertainty in the tails</li> <li>Minimal probability mass before ~20 years</li> <li>A steep probability rise between 50–70 years</li> </ul> <p>The ensemble plot illustrates not just the best-fit curve, but the <em>range of plausible distributions</em> given uncertainty in the literature.</p> <h2 id="ensuring-reproducibility">Ensuring Reproducibility</h2> <p>Reproducibility is essential for transparent modeling, peer review, and regulatory or client submissions.</p> <p>At the end of the analysis, we record:</p> <ul> <li>Python version</li> <li>OS</li> <li>NumPy, pandas, SciPy, statsmodels, matplotlib versions</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Even when individual-level data are unavailable, we can still reconstruct a meaningful, validated age-at-onset distribution using parametric survival models.</p> <p>The generalized gamma distribution offers excellent flexibility, and the combination of:</p> <ul> <li>Quantile matching</li> <li>Diagnostic validation</li> <li>Probabilistic sensitivity analysis (PSA)</li> </ul> <p>This approach fits naturally into broader health economic and epidemiological frameworks, enabling:</p> <ul> <li>Estimation of onset-age probabilities</li> <li>Sensitivity and uncertainty analyses</li> <li>Population-level burden and cost projections</li> <li>Forecasting models informed by realistic evidence intervals</li> </ul>]]></content><author><name></name></author><category term="bioinformatics"/><category term="sensitivity-analysis,"/><category term="survival-model,"/><category term="epi,"/><category term="health-economy"/><summary type="html"><![CDATA[In rare disease epidemiology, researchers often face a familiar challenge: most published studies do not provide individual-level patient data. Instead, they report only high-level summary statistics such as quartiles, means, and total sample size. Yet for health economic modeling, burden of disease estimation, or natural history modeling, we still need to understand the full age-at-onset distribution, including its shape, skew, and uncertainty.]]></summary></entry><entry><title type="html">Turning Messy Symptom Text into Structured Insights</title><link href="https://davidzhao1015.github.io/blog/2025/map-hpo-symptom/" rel="alternate" type="text/html" title="Turning Messy Symptom Text into Structured Insights"/><published>2025-09-19T00:00:00+00:00</published><updated>2025-09-19T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/map-hpo-symptom</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/map-hpo-symptom/"><![CDATA[<h2 id="the-challenge">The Challenge</h2> <p>In rare disease research, clinical symptoms are often reported in free text, words like “droopy eyelid”, “muscle weakness”, or “difficulty feeding.” While these descriptions are clinically meaningful, their lack of standardization makes it nearly impossible to merge, compare, or analyze across studies.</p> <p>This inconsistency is a major barrier for literature reviews, meta-analyses, and evidence synthesis. Without a common language, symptoms that mean the same thing may be counted differently, and downstream models risk being biased or incomplete.<br/><br/></p> <h2 id="a-structured-solution-with-hpo">A Structured Solution with HPO</h2> <p>The Human Phenotype Ontology (HPO) provides a standardized vocabulary for phenotypic abnormalities, widely used in genomics, clinical genetics, and rare disease studies. If free-text symptoms can be reliably mapped to HPO terms, researchers gain:</p> <ul> <li>Consistency: every symptom linked to a unique ID.</li> <li>Context: ontology hierarchy shows how terms relate.</li> <li>Richness: access to synonyms, definitions, and metadata.</li> </ul> <p>To tackle this, I built a Python pipeline that takes reported symptoms as input and outputs a structured table of standardized HPO terms, IDs, and lineage paths.<br/><br/></p> <h2 id="how-the-workflow-works">How the Workflow Works</h2> <p>The pipeline combines API lookups, fuzzy string matching, and ontology metadata into a step-by-step process:</p> <ol> <li>Search the HPO API with the reported symptom to retrieve candidate terms.</li> <li>Fuzzy match validation: compute similarity between the input text and candidate term names.</li> <li>Synonym and definition lookup from the official HPO ontology file.</li> <li>Ontology lineage extraction: trace each matched term back to the root concept, showing context.</li> <li>Decision rule: accept if the fuzzy score is ≥80, or if the input matches an official synonym.</li> </ol> <p>This hybrid approach ensures both precision and recall, handling common synonyms and spelling variations gracefully.<br/><br/></p> <h2 id="a-peek-under-the-hood">A Peek Under the Hood</h2> <p>The implementation uses a few key Python tools:</p> <ul> <li><code class="language-plaintext highlighter-rouge">rapidfuzz</code> → fast, permissive fuzzy string matching library.</li> <li><code class="language-plaintext highlighter-rouge">obonet</code> → parses ontology files like hp.obo into a traversable network.</li> <li><code class="language-plaintext highlighter-rouge">functools.lru_cache</code> → caches ontology queries for speed.</li> <li><code class="language-plaintext highlighter-rouge">requests</code> → queries the JAX HPO API with retries for robustness.</li> </ul> <p>Together, they form a lightweight but powerful pipeline that scales across hundreds of symptoms.<br/><br/></p> <h2 id="demonstration">Demonstration</h2> <p>Here’s an example with three symptoms:</p> <table> <thead> <tr> <th>reported_symptom</th> <th>hpo_term</th> <th>hpo_id</th> <th>fuzzy_score</th> <th>status</th> </tr> </thead> <tbody> <tr> <td>ptosis</td> <td>Ptosis</td> <td>HP:0000508</td> <td>83.3</td> <td>matched</td> </tr> <tr> <td>weak suck</td> <td>Weak cry</td> <td>HP:0001612</td> <td>58.8</td> <td>not matched</td> </tr> <tr> <td>exercise intolerance</td> <td>Exercise intolerance</td> <td>HP:0003546</td> <td>95.0</td> <td>matched</td> </tr> </tbody> </table> <ul> <li>“ptosis” maps cleanly.</li> <li>“weak suck” is ambiguous (flagged as not matched).</li> <li>“exercise intolerance” matches perfectly.</li> </ul> <p>This table is the end product: a structured, machine-readable mapping that can feed into larger data workflows.<br/><br/></p> <h2 id="applications-in-rare-disease-research">Applications in Rare Disease Research</h2> <p>This pipeline is directly applicable to:</p> <ul> <li>Systematic reviews &amp; evidence synthesis → harmonize symptom data across dozens of studies.</li> <li>Registry curation → align patient-reported outcomes with controlled vocabularies.</li> <li>Epidemiology &amp; modeling → enable symptom-based stratification, subgroup analysis, or disease trajectory modeling.</li> </ul> <p>By reducing noise from free text, it supports more reliable cross-study comparisons and integration.<br/><br/></p> <h2 id="limitations--future-work">Limitations &amp; Future Work</h2> <p>Like any tool, there’s room for improvement:</p> <ul> <li>API availability and response speed.</li> <li>Lineage extraction currently follows only the first parent (simplified).</li> <li>Fuzzy thresholds may need tuning per dataset.</li> </ul> <p>Future directions include:</p> <ul> <li>Integrating semantic embeddings for deeper understanding of symptom phrases.</li> <li>Linking to other vocabularies (MeSH, UMLS) for interoperability.<br/><br/></li> </ul> <h2 id="conclusion">Conclusion</h2> <p>This pipeline addresses a real-world pain point in rare disease research: the challenge of inconsistent symptom reporting. By turning free text into structured HPO terms, it creates a reproducible foundation for evidence synthesis, registry development, and computational modeling.</p> <p>And the best part? Its design is generalizable, the same workflow could be extended to other areas of biomedical text mining, from electronic health records to clinical trial reports.<br/><br/></p> <h2 id="try-it-yourself">Try It Yourself</h2> <p>The full notebook and code are available on <a href="https://github.com/davidzhao1015/map-hpo-symptom/blob/main/HPO-symptom-standardize.ipynb">Github</a>.</p> <p>This article is meant for the rare disease research community. But on my portfolio site, I’ll showcase the technical implementation in detail for potential collaborators.</p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="epi,"/><category term="meta-analysis,"/><category term="ontology,"/><category term="rare-disease"/><summary type="html"><![CDATA[The Challenge]]></summary></entry><entry><title type="html">Visualizing Microbiome Taxonomy with Metacoder in R: A Step-by-Step Guide</title><link href="https://davidzhao1015.github.io/blog/2025/tax-heattree-r/" rel="alternate" type="text/html" title="Visualizing Microbiome Taxonomy with Metacoder in R: A Step-by-Step Guide"/><published>2025-08-23T00:00:00+00:00</published><updated>2025-08-23T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/tax-heattree-r</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/tax-heattree-r/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In microbiome research, stacked bar charts are a go-to method for showing the abundance of different taxa. But there’s a catch — they don’t really show the hierarchical relationships within the taxonomy data.</p> <p>That’s where the metacoder R package comes in. Published in <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5340466/">PLoS ONE in 2017</a> and available on CRAN, metacoder makes it easier to explore and visualize taxonomy in a way that reflects its natural hierarchy, while also mapping data across different taxonomic levels.</p> <p>In this post, I’ll show you how to turn your own data into clean, publication-ready plots using metacoder. The package includes helpful <a href="https://grunwaldlab.github.io/metacoder_documentation/example.html">tutorials</a>, but they don’t always cover every scenario — like the one I’ll walk you through here.</p> <p>I’ll also share tips and lessons learned from my experience as an intermediate R user and microbiome researcher, so you can save time and avoid common pitfalls.<br/><br/></p> <h2 id="example">Example</h2> <p>For this example, I used a dataset curated from the <a href="https://pubmed.ncbi.nlm.nih.gov/35412130/">review</a> by Dr. Micheal Gaenzle on key microbiomes in food fermentation. The input data includes the full taxonomy lineages for more than 30 genera commonly involved in food fermentation.</p> <p>One particularly interesting table in the review maps these 30+ bacterial genera to 115 types of fermented foods from around the world. That inspired me to create a family-focused heat tree to visualize the biodiversity of bacteria involved in fermentation.</p> <p>Gathering detailed metadata for a perfectly accurate tree takes time, so for now, I worked with aggregated data — specifically, the proportion of fermented food types linked to each genus. While metacoder can integrate numeric data into heat trees, I found that it generated a misleading legend in this case. To keep the visualization clear, I’ve chosen not to display numeric values on the plot. Instead, I’ll describe the distribution of key bacterial families in the text alongside the visualization.</p> <p>The example input data can be download <a href="https://github.com/davidzhao1015/taxonomy-lineage-viz/blob/main/tax_abund_data.csv">here</a>.</p> <p>With that context set, let’s walk through the steps to prepare the data and generate the heat tree in R, so you can try it with your own dataset.<br/><br/></p> <h2 id="programmatic-workflow">Programmatic workflow</h2> <p>The overall workflow for this example is straightforward and involves three main steps:</p> <ol> <li>Read the taxonomy input data</li> <li>Parse the data into a taxmap object that is compatible with metacoder</li> <li>Generate and customize the heat tree visualization</li> </ol> <p>In the next sections, I’ll walk through each step, showing the code and explaining how you can adapt it to your own dataset.<br/><br/></p> <h2 id="implementation-in-r">Implementation in R</h2> <p><strong>1. Load and inspect your taxonomy data</strong></p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load required library</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">metacoder</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">

</span><span class="c1"># Read in the taxonomy input file</span><span class="w">
</span><span class="n">tax_abund</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"tax_abund_data.csv"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; head(tax_abund)
  Species_label Fermented.foods1 normalized_prop        Kingdom         Phylum
1         spp_1               16       0.1391304 Pseudomonadati Pseudomonadota
2        spp_10              101       0.8782609      Bacillati      Bacillota
3        spp_11              101       0.8782609      Bacillati      Bacillota
4        spp_12              101       0.8782609      Bacillati      Bacillota
5        spp_13              101       0.8782609      Bacillati      Bacillota
6        spp_14              101       0.8782609      Bacillati      Bacillota
                Class           Order           Family                   Genus
1 Alphaproteobacteria Acetobacterales Acetobacteraceae             Acetobacter
2             Bacilli Lactobacillales Lactobacillaceae    Companilactobacillus
3             Bacilli Lactobacillales Lactobacillaceae Schleiferilactobacillus
4             Bacilli Lactobacillales Lactobacillaceae       Ligilactobacillus
5             Bacilli Lactobacillales Lactobacillaceae     Lactiplantibacillus
6             Bacilli Lactobacillales Lactobacillaceae      Loigolactobacillus
  Species
1      NA
2      NA
3      NA
4      NA
5      NA
6      NA
</code></pre></div></div> <p>This file contains the full taxonomy lineages for approximately 30 genera mentioned in Dr. Gaenzle’s review.</p> <ul> <li><strong>Rows</strong>: Each row represents one genus.</li> <li><strong>Columns</strong>: Include the full taxonomy path (Kingdom → Phylum → Class → Order → Family → Genus), along with aggregated counts and proportions of food types containing that genus.</li> <li><strong>Species column</strong>: Values are set to NA where species-level data is not available.<br/><br/></li> </ul> <p><strong>2. Parse the data into a <code class="language-plaintext highlighter-rouge">taxmap</code> object</strong></p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">obj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">parse_tax_data</span><span class="p">(</span><span class="n">tax_abund</span><span class="p">,</span><span class="w"> </span><span class="n">class_cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="o">:</span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="n">named_by_rank</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; print(obj)
&lt;Taxmap&gt;
  66 taxa: ab. Pseudomonadati, ac. Bacillati ... co. Lactobacillus
  66 edges: NA-&gt;ab, NA-&gt;ac, ab-&gt;ad ... bd-&gt;cm, be-&gt;cn, at-&gt;co
  2 data sets:
    tax_data:
      # A tibble: 36 × 11
        taxon_id Species_label Fermented.foods1 normalized_prop
        &lt;chr&gt;    &lt;chr&gt;                    &lt;int&gt;           &lt;dbl&gt;
      1 bf       spp_1                       16           0.139
      2 bg       spp_10                     101           0.878
      3 bh       spp_11                     101           0.878
      # ℹ 33 more rows
      # ℹ 7 more variables: Kingdom &lt;chr&gt;, Phylum &lt;chr&gt;, Class &lt;chr&gt;,
      #   Order &lt;chr&gt;, Family &lt;chr&gt;, Genus &lt;chr&gt;, Species &lt;lgl&gt;
      # ℹ Use `print(n = ...)` to see more rows
    tax_abund:
      # A tibble: 66 × 2
        taxon_id normalized_prop
        &lt;chr&gt;              &lt;dbl&gt;
      1 ab                 0.835
      2 ac                20.7  
      3 ad                 0.835
      # ℹ 63 more rows
      # ℹ Use `print(n = ...)` to see more rows
  0 functions:
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">parse_tax_data()</code> transforms your table into a taxmap object that powers the heat tree visualization.</p> <ul> <li>The <code class="language-plaintext highlighter-rouge">class_cols</code> argument points to the column that contains the taxonomy path.</li> <li>Setting <code class="language-plaintext highlighter-rouge">named_by_rank</code> = TRUE ensures the function recognizes each taxonomic rank correctly.</li> </ul> <p><strong>3. Generate the heat tree</strong></p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">

</span><span class="n">ht_plot_abund</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">heat_tree</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span><span class="w">
    </span><span class="n">node_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="o">$</span><span class="n">taxon_names</span><span class="p">(),</span><span class="w">
    </span><span class="n">node_color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="o">$</span><span class="n">n_obs</span><span class="p">(),</span><span class="w">
    </span><span class="n">node_color_range</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"purple"</span><span class="p">,</span><span class="w"> </span><span class="s2">"yellow"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w">
    </span><span class="n">initial_layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reingold-tilford"</span><span class="p">,</span><span class="w">
    </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"davidson-harel"</span><span class="p">,</span><span class="w">
    </span><span class="n">node_color_axis_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Number of genera \nwithin the taxa"</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="n">ht_plot_abund</span><span class="w">
</span></code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/heat_tree_plot.png" sizes="95vw"/> <img src="/assets/img/heat_tree_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This initial plot focuses on showing the hierarchical relationships.</p> <ul> <li>Because the numeric proportions generated a misleading legend, no quantitative data are mapped here.</li> <li>This keeps the visualization clean while still clearly showing the relationships between families and genera involved in fermentation.<br/><br/></li> </ul> <h2 id="bonus">Bonus</h2> <h3 id="optional-add-quantitative-data-with-caution">Optional: Add quantitative data (with caution)</h3> <p>After you’ve parsed your taxonomy into a taxmap object, you can compute per-taxon values (e.g., the proportion of fermented food types per genus) and attach them to the object for plotting.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate per-taxon abundance (here using a column called "normalized_prop")</span><span class="w">
</span><span class="c1"># This creates obj$data$tax_abund with one value per taxon</span><span class="w">
</span><span class="n">obj</span><span class="o">$</span><span class="n">data</span><span class="o">$</span><span class="n">tax_abund</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">calc_taxon_abund</span><span class="p">(</span><span class="w">
	</span><span class="n">obj</span><span class="p">,</span><span class="w"> 
	</span><span class="s2">"tax_data"</span><span class="p">,</span><span class="w"> 
	</span><span class="n">cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"normalized_prop"</span><span class="p">)</span><span class="w">

</span><span class="c1"># Draw a heat tree using the computed values</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">

</span><span class="n">ht_plot_abund2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">heat_tree</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span><span class="w">
    </span><span class="n">node_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="o">$</span><span class="n">taxon_names</span><span class="p">(),</span><span class="w"> </span><span class="c1"># Show taxon names</span><span class="w">
    </span><span class="n">node_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="o">$</span><span class="n">data</span><span class="o">$</span><span class="n">tax_abund</span><span class="o">$</span><span class="n">normalized_prop</span><span class="p">,</span><span class="w"> </span><span class="c1"># Size by proportions</span><span class="w">
    </span><span class="n">node_color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="o">$</span><span class="n">data</span><span class="o">$</span><span class="n">tax_abund</span><span class="o">$</span><span class="n">normalized_prop</span><span class="p">,</span><span class="w"> </span><span class="c1"># Color by proportions</span><span class="w">
    </span><span class="n">node_color_range</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"purple"</span><span class="p">,</span><span class="w"> </span><span class="s2">"yellow"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w"> </span><span class="c1"># Color palette</span><span class="w">
    </span><span class="n">initial_layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reingold-tilford"</span><span class="p">,</span><span class="w">
    </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"davidson-harel"</span><span class="p">,</span><span class="w">
    </span><span class="n">node_color_axis_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Prop in Fermented Foods"</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="n">ht_plot_abund2</span><span class="w">
</span></code></pre></div></div> <p><strong>Why the caution?</strong></p> <p>Mapping numbers to node color/size can be powerful, but the legend and scaling can be misleading if your values are tightly clustered, all zeros, or contain many missing values. In my case, the legend confused readers, so I left numeric mappings out of the final figure and explained key patterns in the text instead.</p> <h3 id="optional-focus-at-the-family-level">Optional: Focus at the family level</h3> <p>You can subset the taxonomy to simplify the figure or highlight a specific level (e.g., <strong>family</strong>) and then plot. This often reveals clearer biological patterns.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Keep families (and their super/subtaxa as you prefer) and color by count of child taxa</span><span class="w">
</span><span class="c1"># The example below hides genus nodes to emphasize family-level structure.</span><span class="w">

</span><span class="n">ht_plot_abund3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">obj</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> 
    </span><span class="n">filter_taxa</span><span class="p">(</span><span class="n">taxon_ranks</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s2">"Genus"</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="c1"># Drop genus nodes for a cleaner family-level view</span><span class="w">
    </span><span class="n">heat_tree</span><span class="p">(</span><span class="n">node_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">taxon_names</span><span class="p">,</span><span class="w"> 
    </span><span class="n">node_color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n_obs</span><span class="p">,</span><span class="w">
    </span><span class="n">node_color_range</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"purple"</span><span class="p">,</span><span class="w"> </span><span class="s2">"yellow"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w">
    </span><span class="n">initial_layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reingold-tilford"</span><span class="p">,</span><span class="w">
    </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"davidson-harel"</span><span class="p">,</span><span class="w">
    </span><span class="n">node_color_axis_label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Number of \ngenera within"</span><span class="p">)</span><span class="w">

</span><span class="n">ht_plot_abund3</span><span class="w">
</span></code></pre></div></div> <p><br/></p> <h2 id="data-interpretation">Data Interpretation</h2> <p>The heat tree highlights several key insights about the diversity of bacteria involved in food fermentation:</p> <ul> <li>Core Fermenters – Families like <em>Lactobacillaceae</em>, <em>Leuconostocaceae</em>, <em>Streptococcaceae</em>, <em>Enterococcaceae</em>, and <em>Carnobacteriaceae</em> form the backbone of dairy, cereal, and vegetable fermentations.</li> <li>Initiators – <em>Enterobacteriaceae</em> and <em>Erwiniaceae</em> often kick-start spontaneous fermentations in vegetables, cereals, tubers, coffee, and cocoa.</li> <li>Niche Specialists – <em>Acetobacteraceae</em>, <em>Bacillaceae</em>, and <em>Propionibacteriaceae</em> are critical in vinegar, natto, soy/ fish sauces, and Swiss cheese fermentations.</li> <li>Surface &amp; Meat Fermenters – Families like <em>Staphylococcaceae</em>, <em>Micrococcaceae</em>, and <em>Brevibacteriaceae</em> play key roles in ripening, aroma development, and safety in meat and cheese fermentations.</li> <li>Minor but Emerging Players – <em>Eggerthellaceae</em> show a secondary but notable presence in vegetable fermentations such as sauerkraut and kimchi.</li> </ul> <p>These insights not only showcase the rich biodiversity of fermenting microbes but also highlight their specialized roles in shaping flavors, textures, and safety across different foods. For researchers, educators, or fermentation enthusiasts, such visualizations can guide strain selection, recipe development, and deeper exploration into the microbial ecosystems that make our favorite fermented products possible.<br/><br/></p> <h2 id="personal-tips">Personal tips</h2> <p>Here are a few tips from my experience working with metacoder.</p> <p><strong>Start with the examples</strong>: The package tutorials and help docs include plenty of sample datasets. Taking time to explore these will make it much easier to prepare your own input data correctly.</p> <p><strong>Legend adjustments are limited</strong>: While metacoder offers a lot of flexibility in customizing your plots, the position of the legend doesn’t seem to be adjustable. Plan your layout with that limitation in mind.</p> <p><strong>Interpret node sizes and colors carefully</strong>: These elements are proportional to your quantitative data, such as OTU counts or other biological variables. Always double-check your legend to avoid over- or under-interpreting the results.</p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="taxonomy,"/><category term="microbiome,"/><category term="visualization"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Retrieving Full Taxonomy Lineage from NCBI with Python</title><link href="https://davidzhao1015.github.io/blog/2025/retrieve-tax-lineage-API/" rel="alternate" type="text/html" title="Retrieving Full Taxonomy Lineage from NCBI with Python"/><published>2025-08-09T00:00:00+00:00</published><updated>2025-08-09T00:00:00+00:00</updated><id>https://davidzhao1015.github.io/blog/2025/retrieve-tax-lineage-API</id><content type="html" xml:base="https://davidzhao1015.github.io/blog/2025/retrieve-tax-lineage-API/"><![CDATA[<h2 id="problem">Problem</h2> <p>In microbiome data analysis, taxonomy assignments—derived from marker genes or whole genomes—are essential for understanding microbial ecosystems when paired with abundance data. These assignments are not limited to single taxon names but are structured as hierarchical lineages across multiple taxonomic ranks, offering richer biological insights. However, researchers often only have partial taxonomy data (e.g., names at a single rank), which limits interpretability and analytical depth.<br/><br/></p> <h2 id="solution">Solution</h2> <p>To unlock the full potential of taxonomy-based insights, incomplete taxon names can be mapped to their complete hierarchical lineages by retrieving standardized taxonomy data from the NCBI database. This approach enriches the dataset and supports more robust biological interpretation.<br/><br/></p> <h2 id="background">Background</h2> <p>The <strong>NCBI Taxonomy</strong> is the official nomenclature and classification resource for the <strong>International Nucleotide Sequence Database Collaboration (INSDC)</strong>, which includes GenBank, EMBL, and DDBJ. It provides curated organism names and an approximately phylogenetic classification for all source organisms represented in INSDC records, serving as a framework for linking resources within NCBI and to external taxon-specific information. Names are selected by NCBI curators based on published taxonomic data and expert opinion, ensuring a single current name is assigned to each taxon. The database focuses on nomenclature and systematics rather than detailed taxon descriptions.<br/><br/></p> <h2 id="solution-implementation">Solution implementation</h2> <p>To retrieve full taxonomy lineages, one can use either the NCBI Taxonomy web browser or a programmatic method. The programmatic approach is highly efficient and advantageous, allowing automation of repetitive retrieval for multiple taxa at once. The NCBI E-utilities Python API enables access to the web server directly from Python scripts, integrating seamlessly with other steps in a microbiome data analysis pipeline.</p> <p>This section includes two parts:</p> <ol> <li>A reusable function to download and parse the full taxonomy lineage for a given taxon at a single rank.</li> <li>An example demonstrating how to apply this function to multiple taxa, using a food fermentation microbiome dataset as a case study.<br/><br/></li> </ol> <h3 id="reusable-script">Reusable script</h3> <p>The customizable function below retrieves the full lineage using the Bio.Entrez.esearch(), Bio.Entrez.efetch(), and Bio.Entrez.read() functions from the <strong>Bio.Entrez</strong> submodule in the Biopython package.</p> <p>The input is a taxon string at a given rank, and the output is a list of taxa from higher to lower ranks. You can use this function as is or adapt it for your specific needs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">Bio</span> <span class="kn">import</span> <span class="n">Entrez</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">os</span>


<span class="k">def</span> <span class="nf">get_taxonomy_lineage</span><span class="p">(</span><span class="n">taxon_name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">,</span> <span class="n">email</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Fetches complete taxonomy lineage based on the known taxonomic level (the highest resolution available) from the NCBI taxonomy database.

    Args:
        rank (str): The taxonomic rank to search for (default is </span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="s">). </span><span class="sh">"</span><span class="s">Family</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">Order</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">Class</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">Phylum</span><span class="sh">"</span><span class="s">, or </span><span class="sh">"</span><span class="s">Kingdom</span><span class="sh">"</span><span class="s"> can also be used.
        taxon_name (str): The name of the taxon to search for. If None, the function will not perform a search.
        email (str): Your email address for NCBI Entrez. This is required to track usage and for contact in case of issues.

    Returns:
        list: A list containing the lineage of the taxon, or None if not found.
        The lineage includes domain, kingdom (or clade), phylum, class, order, and family.
        If the taxon is not found, it returns None.
        If the rank is invalid, it returns None with an error message.
        If the taxon_name is None, it returns None with an error message.
    </span><span class="sh">"""</span>
    <span class="c1">#--- Set the email for NCBI Entrez ---
</span>    <span class="c1"># This is required by NCBI to track usage and for contact in case of issues
</span>    <span class="c1"># Replace with your email address
</span>    <span class="n">Entrez</span><span class="p">.</span><span class="n">email</span> <span class="o">=</span> <span class="n">email</span> <span class="ow">or</span> <span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">NCBI_EMAIL</span><span class="sh">"</span><span class="p">)</span> 


    <span class="c1">#--- Input validation ---
</span>    <span class="c1"># Check if taxon_name is provided
</span>    <span class="k">if</span> <span class="n">taxon_name</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Taxon name is None. Please provide a valid taxon name.</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="c1"># Check if rank is valid
</span>    <span class="k">if</span> <span class="n">rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Family</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Order</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Class</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Phylum</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Kingdom</span><span class="sh">"</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Invalid rank: </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">. Please use one of the following ranks: Genus, Family, Order, Class, Phylum, Kingdom.</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>


    <span class="c1">#--- Search for the genus in the NCBI taxonomy database ---
</span>    <span class="c1"># The search term is formatted to include the genus name followed by "[Genus]" to specify the search field
</span>    <span class="c1"># This ensures that the search is limited to the genus level in the taxonomy database
</span>    <span class="n">handle</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">esearch</span><span class="p">(</span><span class="n">db</span><span class="o">=</span><span class="sh">"</span><span class="s">taxonomy</span><span class="sh">"</span><span class="p">,</span> <span class="n">term</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">taxon_name</span><span class="si">}</span><span class="s">[</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">]</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">record</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">read</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span> <span class="c1"># Read the search results
</span>    <span class="n">handle</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span> <span class="c1"># Close the handle to free resources
</span>

    <span class="c1">#--- Check if any IDs were found for the genus ---
</span>    <span class="c1"># If no IDs are found, print a message and return None
</span>    <span class="c1"># This is important to handle cases where the genus does not exist in the database
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">record</span><span class="p">[</span><span class="sh">"</span><span class="s">IdList</span><span class="sh">"</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">No taxonomy ID found for </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">taxon_name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> 
        <span class="k">return</span> <span class="bp">None</span>


    <span class="c1">#--- Fetch the taxonomy record using the first ID found ---
</span>    <span class="c1"># The first ID in the IdList is used to fetch the complete taxonomy record
</span>    <span class="c1"># This is because the search may return multiple IDs, but we are interested in the first one
</span>    <span class="c1"># The efetch function retrieves the record in XML format for easier parsing
</span>    <span class="c1"># The record contains detailed information about the taxonomy, including lineage
</span>    <span class="c1"># The lineage includes domain, kingdom (or clade), phylum, class, order, and family
</span>    <span class="n">taxid</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="sh">"</span><span class="s">IdList</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Get the first taxonomy ID from the search results
</span>    <span class="n">handle</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">efetch</span><span class="p">(</span><span class="n">db</span><span class="o">=</span><span class="sh">"</span><span class="s">taxonomy</span><span class="sh">"</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">taxid</span><span class="p">,</span> <span class="n">retmode</span><span class="o">=</span><span class="sh">"</span><span class="s">xml</span><span class="sh">"</span><span class="p">)</span> 
    <span class="n">records</span> <span class="o">=</span> <span class="n">Entrez</span><span class="p">.</span><span class="nf">read</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span> <span class="c1"># Read the fetched record
</span>    <span class="n">handle</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span> <span class="c1"># Close the handle to free resources
</span>
    <span class="c1">#--- Extract the lineage from the fetched record ---
</span>    <span class="c1"># The lineage is extracted from the first record in the list of records returned by efetch
</span>    <span class="c1"># The lineage is a string that includes the complete taxonomy hierarchy for the genus
</span>    <span class="c1"># It is formatted as "domain; kingdom; phylum; class; order; family"
</span>    <span class="n">lineage</span> <span class="o">=</span> <span class="n">records</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">Lineage</span><span class="sh">"</span><span class="p">]</span> <span class="c1"># Extract the lineage from the record, including domain, kingdom (or clade), phylum, class, order, and family
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">lineage</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">lineage_list</span> <span class="o">=</span> <span class="n">lineage</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">; </span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Split the lineage into a list
</span>    
    <span class="k">return</span> <span class="n">lineage_list</span>
</code></pre></div></div> <p>Here’s a quick example to give you an idea of the output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">genus</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Lactobacillus</span><span class="sh">"</span>
<span class="n">lineage</span> <span class="o">=</span> <span class="nf">get_taxonomy_lineage</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">,</span> <span class="n">taxon_name</span><span class="o">=</span><span class="n">genus</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Lineage for </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">lineage</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Lineage for Lactobacillus: ['cellular organisms', 'Bacteria', 'Bacillati', 'Bacillota', 'Bacilli', 'Lactobacillales', 'Lactobacillaceae']
</code></pre></div></div> <h3 id="example">Example</h3> <p>The example demonstrates applying the function to multiple bacterial taxa using loop iteration in Python, maximizing the efficiency of the Python API. You can adapt the code by replacing it with your own input data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define a list of target genera: Fermenting bacteria
</span><span class="n">genera</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Acetobacter</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Gluconacetobacter</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lentibacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Brevibacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Erwinia</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Enterobacter</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Pantoea</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Kosakonia</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Companilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Schleiferilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Ligilactobacillus</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Lactiplantibacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Loigolactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Paucilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Limosilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Fructilactobacillus</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Acetilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Secundilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lentilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Carnobacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Weissella</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Oenococcus</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Enterococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Tetragenococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Streptococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Lactococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Pediococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Periweissella</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Leuconostoc</span><span class="sh">'</span><span class="p">,</span> 
          <span class="sh">'</span><span class="s">Marinilactobacillus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Alkalibacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Eggerthella</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Propionibacterium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Staphylococcus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Kocuria</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize an empty DataFrame to store all lineages
</span><span class="n">df_all_lineages</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">()</span>

<span class="c1"># Loop through each genus and get the taxonomy lineage
</span><span class="k">for</span> <span class="n">genus</span> <span class="ow">in</span> <span class="n">genera</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Processing genus: </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">lineage</span> <span class="o">=</span> <span class="nf">get_taxonomy_lineage</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">,</span> <span class="n">taxon_name</span><span class="o">=</span><span class="n">genus</span><span class="p">)</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>  <span class="c1"># Sleep to avoid hitting NCBI's rate limits ~3 requests per second without API key
</span>    
    <span class="c1"># Skip if lineage is None or does not belong to Bacteria
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">lineage</span> <span class="ow">or</span> <span class="n">lineage</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">Bacteria</span><span class="sh">"</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Skipping genus: </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="s"> (No lineage or not Bacteria)</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">continue</span>

    <span class="c1"># Create a DataFrame for the current genus
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="n">df_genus_lineage</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">([</span><span class="n">lineage</span><span class="p">[</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">Domain</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Kingdom</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Phylum</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Class</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Order</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Family</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">df_genus_lineage</span><span class="p">[</span><span class="sh">'</span><span class="s">Genus</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">genus</span>
        
        <span class="c1"># Append to the main DataFrame
</span>        <span class="n">df_all_lineages</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df_all_lineages</span><span class="p">,</span> <span class="n">df_genus_lineage</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error processing genus </span><span class="si">{</span><span class="n">genus</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Print summary of successful retrievals
</span><span class="n">genus_successful</span> <span class="o">=</span> <span class="n">df_all_lineages</span><span class="p">[</span><span class="sh">"</span><span class="s">Genus</span><span class="sh">"</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">genus_successful</span><span class="p">)</span><span class="si">}</span><span class="s"> out of </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">genera</span><span class="p">)</span><span class="si">}</span><span class="s"> genera were successfully retrieved.</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Save the DataFrame to a CSV file
</span><span class="n">output_file</span> <span class="o">=</span> <span class="sh">"</span><span class="s">taxonomy_lineages.csv</span><span class="sh">"</span>
<span class="n">df_all_lineages</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Lineages saved to </span><span class="si">{</span><span class="n">output_file</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Display the first few rows of the DataFrame
</span><span class="n">df_all_lineages</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div> <p>The first rows of the output table is as below.</p> <table> <thead> <tr> <th> </th> <th>Domain</th> <th>Kingdom</th> <th>Phylum</th> <th>Class</th> <th>Order</th> <th>Family</th> <th>Genus</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>Bacteria</td> <td>Pseudomonadati</td> <td>Pseudomonadota</td> <td>Alphaproteobacteria</td> <td>Acetobacterales</td> <td>Acetobacteraceae</td> <td>Acetobacter</td> </tr> <tr> <td>1</td> <td>Bacteria</td> <td>Pseudomonadati</td> <td>Pseudomonadota</td> <td>Alphaproteobacteria</td> <td>Acetobacterales</td> <td>Acetobacteraceae</td> <td>Gluconacetobacter</td> </tr> <tr> <td>2</td> <td>Bacteria</td> <td>Bacillati</td> <td>Bacillota</td> <td>Bacilli</td> <td>Bacillales</td> <td>Bacillaceae</td> <td>Lentibacillus</td> </tr> <tr> <td>3</td> <td>Bacteria</td> <td>Bacillati</td> <td>Actinomycetota</td> <td>Actinomycetes</td> <td>Micrococcales</td> <td>Brevibacteriaceae</td> <td>Brevibacterium</td> </tr> <tr> <td>4</td> <td>Bacteria</td> <td>Pseudomonadati</td> <td>Pseudomonadota</td> <td>Gammaproteobacteria</td> <td>Enterobacterales</td> <td>Erwiniaceae</td> <td>Erwinia</td> </tr> </tbody> </table> <p><br/></p> <h2 id="conclusion">Conclusion</h2> <p>Taxonomic lineage information provides far more value than a single taxon name, especially in microbiome data analysis, where biological context is key. By programmatically retrieving complete lineages from NCBI’s Taxonomy database, researchers can enrich partial datasets, standardize taxonomic ranks, and gain deeper ecological insights without manual lookups.</p> <p>This workflow demonstrates how a few lines of Python, combined with the Bio.Entrez API, can turn scattered names into structured, multi-rank taxonomy data. Whether you are refining microbial community analyses or preparing results for publication, automating taxonomy retrieval not only saves time but also ensures reproducibility and accuracy.</p> <p>If you often work with incomplete taxonomy data, integrating such an automated retrieval step into your pipeline is a small effort with a high return in both interpretability and research efficiency.<br/><br/></p> <h3 id="reference">Reference</h3> <p><a href="https://www.ncbi.nlm.nih.gov/books/NBK53758/">NCBI help</a><br/> <a href="https://biopython.org/docs/latest/Tutorial/chapter_entrez.html?utm_source=chatgpt.com">Bio.Entrez</a></p>]]></content><author><name></name></author><category term="bioinformatics"/><category term="taxonomy,"/><category term="microbiome,"/><category term="biopython,"/><category term="ncbi-api"/><summary type="html"><![CDATA[Problem]]></summary></entry></feed>